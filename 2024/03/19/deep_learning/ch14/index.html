<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="超参数调整 14.1 写在前面 ​	关于训练深度学习模型最难的事情之一是你要处理的参数的数量。无论是从网络本身的层宽（宽度）、层数（深度）、连接方式，还是损失函数的超参数设计和调试，亦或者是学习率、批样本数量、优化器参数等等。这些大量的参数都会有网络模型最终的有效容限直接或者间接的影响。面对如此众多的参数，如果我们要一一对其优化调整，所需的无论是时间、资源都是不切实际。结果证实一些超参数比其它的更">
<meta property="og:type" content="article">
<meta property="og:title" content="超参数调整面试题">
<meta property="og:url" content="http://example.com/2024/03/19/deep_learning/ch14/index.html">
<meta property="og:site_name" content="算法工程师的日常">
<meta property="og:description" content="超参数调整 14.1 写在前面 ​	关于训练深度学习模型最难的事情之一是你要处理的参数的数量。无论是从网络本身的层宽（宽度）、层数（深度）、连接方式，还是损失函数的超参数设计和调试，亦或者是学习率、批样本数量、优化器参数等等。这些大量的参数都会有网络模型最终的有效容限直接或者间接的影响。面对如此众多的参数，如果我们要一一对其优化调整，所需的无论是时间、资源都是不切实际。结果证实一些超参数比其它的更">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C%E6%8C%87%E6%95%B0%E8%A1%B0%E5%87%8F.jpeg">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E8%A1%B0%E5%87%8F.jpeg">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5Ccycle%E8%A1%B0%E5%87%8F.jpeg">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C%E9%80%86%E6%97%B6%E8%A1%B0%E5%87%8F.jpeg">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C%E4%BD%99%E5%BC%A6%E8%A1%B0%E5%87%8F.jpeg">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C%E4%BD%99%E5%BC%A6cycle%E8%A1%B0%E5%87%8F.jpeg">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C%E7%BA%BF%E6%80%A7%E4%BD%99%E5%BC%A6%E8%A1%B0%E5%87%8F.jpeg">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C%E5%99%AA%E5%A3%B0%E7%BA%BF%E6%80%A7%E4%BD%99%E5%BC%A6%E8%A1%B0%E5%87%8F.jpeg">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C14.14.png">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5CNAS%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5.png">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5CRNN%E6%8E%A7%E5%88%B6%E5%99%A8.png">
<meta property="og:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5CNASNet%E7%9A%84RNN%E6%8E%A7%E5%88%B6%E5%99%A8.png">
<meta property="article:published_time" content="2024-03-19T15:29:20.000Z">
<meta property="article:modified_time" content="2024-03-24T02:14:07.778Z">
<meta property="article:author" content="Tom">
<meta property="article:tag" content="刷八股">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/03/19/deep_learning/ch14/img%5Cch14%5C%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png">

<link rel="canonical" href="http://example.com/2024/03/19/deep_learning/ch14/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>超参数调整面试题 | 算法工程师的日常</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">算法工程师的日常</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/deep_learning/ch14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          超参数调整面试题
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-24 10:14:07" itemprop="dateModified" datetime="2024-03-24T10:14:07+08:00">2024-03-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">算法面试</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="far fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1>超参数调整</h1>
<h2 id="14-1-写在前面">14.1 写在前面</h2>
<p>​	关于训练深度学习模型最难的事情之一是你要处理的参数的数量。无论是从网络本身的层宽（宽度）、层数（深度）、连接方式，还是损失函数的超参数设计和调试，亦或者是学习率、批样本数量、优化器参数等等。这些大量的参数都会有网络模型最终的有效容限直接或者间接的影响。面对如此众多的参数，如果我们要一一对其优化调整，所需的无论是时间、资源都是不切实际。结果证实一些超参数比其它的更为重要，因此认识各个超参数的作用和其可能会造成的影响是深度学习训练中必不可少的一项重要技能。</p>
<p>​	超参数调整可以说是深度学习中理论和实际联系最重要的一个环节。目前，深度学习仍存在很多不可解释的部分，如何设计优化出好的网络可以为深度学习理论的探索提供重要的支持。超参数调整一般分为手动调整和自动优化超参数两种。读者可先浏览思维导图，本章节不会过多阐述所有超参数的详细原理，如果需要了解这部分，您可以翻阅前面的基础章节或者查阅相关文献资料。当然，下面会讲到的一些超参数优化的建议是根据笔者们的实践以及部分文献资料得到认知建议，并不是非常严格且一定有效的，很多研究者可能会很不同意某些的观点或有着不同的直觉，这都是可保留讨论的，因为这很依赖于数据本身情况。</p>
<p><img src="img%5Cch14%5C%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png" alt></p>
<p>​</p>
<h2 id="14-2-超参数概念">14.2 超参数概念</h2>
<h3 id="14-2-1-什么是超参数，参数和超参数的区别？">14.2.1 什么是超参数，参数和超参数的区别？</h3>
<p>​	区分两者最大的一点就是是否通过数据来进行调整，模型参数通常是有数据来驱动调整，超参数则不需要数据来驱动，而是在训练前或者训练中人为的进行调整的参数。例如卷积核的具体核参数就是指模型参数，这是有数据驱动的。而学习率则是人为来进行调整的超参数。这里需要注意的是，通常情况下卷积核数量、卷积核尺寸这些也是超参数，注意与卷积核的核参数区分。</p>
<h3 id="14-2-2-神经网络中包含哪些超参数？">14.2.2 神经网络中包含哪些超参数？</h3>
<p>通常可以将超参数分为三类：网络参数、优化参数、正则化参数。</p>
<p>​	网络参数：可指网络层与层之间的交互方式（相加、相乘或者串接等）、卷积核数量和卷积核尺寸、网络层数（也称深度）和激活函数等。</p>
<p>​	优化参数：一般指学习率（learning rate）、批样本数量（batch size）、不同优化器的参数以及部分损失函数的可调参数。</p>
<p>​	正则化：权重衰减系数，丢弃比率（dropout）</p>
<h3 id="14-2-3-为什么要进行超参数调优？">14.2.3 为什么要进行超参数调优？</h3>
<p>​	本质上，这是模型优化寻找最优解和正则项之间的关系。网络模型优化调整的目的是为了寻找到全局最优解（或者相比更好的局部最优解），而正则项又希望模型尽量拟合到最优。两者通常情况下，存在一定的对立，但两者的目标是一致的，即最小化期望风险。模型优化希望最小化经验风险，而容易陷入过拟合，正则项用来约束模型复杂度。所以如何平衡两者之间的关系，得到最优或者较优的解就是超参数调整优化的目的。</p>
<h3 id="14-2-4-超参数的重要性顺序">14.2.4 超参数的重要性顺序</h3>
<ul>
<li>
<p>首先， <strong>学习率，损失函数上的可调参数</strong>。在网络参数、优化参数、正则化参数中最重要的超参数可能就是学习率了。学习率直接控制着训练中网络梯度更新的量级，直接影响着模型的<strong>有效容限能力</strong>；损失函数上的可调参数，这些参数通常情况下需要结合实际的损失函数来调整，大部分情况下这些参数也能很直接的影响到模型的的有效容限能力。这些损失一般可分成三类，第一类辅助损失结合常见的损失函数，起到辅助优化特征表达的作用。例如度量学习中的Center loss，通常结合交叉熵损失伴随一个权重完成一些特定的任务。这种情况下一般建议辅助损失值不高于或者不低于交叉熵损失值的两个数量级；第二类，多任务模型的多个损失函数，每个损失函数之间或独立或相关，用于各自任务，这种情况取决于任务之间本身的相关性，目前笔者并没有一个普适的经验由于提供参考；第三类，独立损失函数，这类损失通常会在特定的任务有显著性的效果。例如RetinaNet中的focal loss，其中的参数γ，α，对最终的效果会产生较大的影响。这类损失通常论文中会给出特定的建议值。</p>
</li>
<li>
<p>其次，<strong>批样本数量，动量优化器（Gradient Descent with Momentum）的动量参数<em>β</em></strong>。批样本决定了数量梯度下降的方向。过小的批数量，极端情况下，例如batch size为1，即每个样本都去修正一次梯度方向，样本之间的差异越大越难以收敛。若网络中存在批归一化（batchnorm），batch size过小则更难以收敛，甚至垮掉。这是因为数据样本越少，统计量越不具有代表性，噪声也相应的增加。而过大的batch size，会使得梯度方向基本稳定，容易陷入局部最优解，降低精度。一般参考范围会取在[1:1024]之间，当然这个不是绝对的，需要结合具体场景和样本情况；动量衰减参数<em>β</em>是计算梯度的指数加权平均数，并利用该值来更新参数，设置为 0.9 是一个常见且效果不错的选择；</p>
</li>
<li>
<p>最后，<strong>Adam优化器的超参数、权重衰减系数、丢弃法比率（dropout）和网络参数</strong>。在这里说明下，这些参数重要性放在最后<strong>并不等价于这些参数不重要</strong>。而是表示这些参数在大部分实践中<strong>不建议过多尝试</strong>，例如Adam优化器中的<em>β1，β2，ϵ</em>，常设为 0.9、0.999、10−8就会有不错的表现。权重衰减系数通常会有个建议值，例如0.0005 ，使用建议值即可，不必过多尝试。dropout通常会在全连接层之间使用防止过拟合，建议比率控制在[0.2,0.5]之间。使用dropout时需要特别注意两点：一、在RNN中，如果直接放在memory cell中,循环会放大噪声，扰乱学习。一般会建议放在输入和输出层；二、不建议dropout后直接跟上batchnorm，dropout很可能影响batchnorm计算统计量，导致方差偏移，这种情况下会使得推理阶段出现模型完全垮掉的极端情况；网络参数通常也属于超参数的范围内，通常情况下增加网络层数能增加模型的容限能力，但模型真正有效的容限能力还和样本数量和质量、层之间的关系等有关，所以一般情况下会选择先固定网络层数，调优到一定阶段或者有大量的硬件资源支持可以在网络深度上进行进一步调整。</p>
</li>
</ul>
<h3 id="14-2-5-部分超参数如何影响模型性能？">14.2.5 部分超参数如何影响模型性能？</h3>
<table>
<thead>
<tr>
<th style="text-align:center">超参数</th>
<th style="text-align:center">如何影响模型容量</th>
<th style="text-align:center">原因</th>
<th style="text-align:center">注意事项</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">学习率</td>
<td style="text-align:center">调至最优，提升有效容量</td>
<td style="text-align:center">过高或者过低的学习率，都会由于优化失败而导致降低模型有效容限</td>
<td style="text-align:center">学习率最优点，在训练的不同时间点都可能变化，所以需要一套有效的学习率衰减策略</td>
</tr>
<tr>
<td style="text-align:center">损失函数部分超参数</td>
<td style="text-align:center">调至最优，提升有效容量</td>
<td style="text-align:center">损失函数超参数大部分情况都会可能影响优化，不合适的超参数会使即便是对目标优化非常合适的损失函数同样难以优化模型，降低模型有效容限。</td>
<td style="text-align:center">对于部分损失函数超参数其变化会对结果十分敏感，而有些则并不会太影响。在调整时，建议参考论文的推荐值，并在该推荐值数量级上进行最大最小值调试该参数对结果的影响。</td>
</tr>
<tr>
<td style="text-align:center">批样本数量</td>
<td style="text-align:center">过大过小，容易降低有效容量</td>
<td style="text-align:center">大部分情况下，选择适合自身硬件容量的批样本数量，并不会对模型容限造成。</td>
<td style="text-align:center">在一些特殊的目标函数的设计中，如何选择样本是很可能影响到模型的有效容限的，例如度量学习（metric learning）中的N-pair loss。这类损失因为需要样本的多样性，可能会依赖于批样本数量。</td>
</tr>
<tr>
<td style="text-align:center">丢弃法</td>
<td style="text-align:center">比率降低会提升模型的容量</td>
<td style="text-align:center">较少的丢弃参数意味着模型参数量的提升，参数间适应性提升，模型容量提升，但不一定能提升模型有效容限</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数</td>
<td style="text-align:center">调至最优，提升有效容量</td>
<td style="text-align:center">权重衰减可以有效的起到限制参数变化的幅度，起到一定的正则作用</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">优化器动量</td>
<td style="text-align:center">调至最优，可能提升有效容量</td>
<td style="text-align:center">动量参数通常用来加快训练，同时更容易跳出极值点，避免陷入局部最优解。</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">模型深度</td>
<td style="text-align:center">同条件下，深度增加，模型容量提升</td>
<td style="text-align:center">同条件，下增加深度意味着模型具有更多的参数，更强的拟合能力。</td>
<td style="text-align:center">同条件下，深度越深意味着参数越多，需要的时间和硬件资源也越高。</td>
</tr>
<tr>
<td style="text-align:center">卷积核尺寸</td>
<td style="text-align:center">尺寸增加，模型容量提升</td>
<td style="text-align:center">增加卷积核尺寸意味着参数量的增加，同条件下，模型参数也相应的增加。</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h3 id="14-2-6-部分超参数合适的范围">14.2.6 部分超参数合适的范围</h3>
<table>
<thead>
<tr>
<th style="text-align:center">超参数</th>
<th style="text-align:center">建议范围</th>
<th style="text-align:center">注意事项</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">初始学习率</td>
<td style="text-align:center">SGD: [1e-2, 1e-1]<br>momentum: [1e-3, 1e-2]<br>Adagrad: [1e-3, 1e-2]<br>Adadelta: [1e-2, 1e-1]<br>RMSprop: [1e-3, 1e-2]<br>Adam: [1e-3, 1e-2]<br>Adamax: [1e-3, 1e-2]<br>Nadam: [1e-3, 1e-2]</td>
<td style="text-align:center">这些范围通常是指从头开始训练的情况。若是微调，初始学习率可在降低一到两个数量级。</td>
</tr>
<tr>
<td style="text-align:center">损失函数部分超参数</td>
<td style="text-align:center">多个损失函数之间，损失值之间尽量相近，不建议超过或者低于两个数量级</td>
<td style="text-align:center">这是指多个损失组合的情况，不一定完全正确。单个损失超参数需结合实际情况。</td>
</tr>
<tr>
<td style="text-align:center">批样本数量</td>
<td style="text-align:center">[1:1024]</td>
<td style="text-align:center">当批样本数量过大(大于6000)或者等于1时，需要注意学习策略或者内部归一化方式的调整。</td>
</tr>
<tr>
<td style="text-align:center">丢弃法比率</td>
<td style="text-align:center">[0, 0.5]</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数</td>
<td style="text-align:center">[0, 1e-4]</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">卷积核尺寸</td>
<td style="text-align:center">[7x7],[5x5],[3x3],[1x1], [7x1,1x7]</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h2 id="14-3-网络训练中的超参调整策略">14.3 网络训练中的超参调整策略</h2>
<h3 id="14-3-1-如何调试模型？">14.3.1 如何调试模型？</h3>
<p>在讨论如何调试模型之前，我们先来纠正一个误区。通常理解如何调试模型的时候，我们想到一系列优秀的神经网络模型以及调试技巧。但这里需要指出的是数据才是模型的根本，如果有一批质量优秀的数据，或者说你能将数据质量处理的很好的时候，往往比挑选或者设计模型的收益来的更大。那在这之后才是模型的设计和挑选以及训练技巧上的事情。</p>
<p>1、探索和清洗数据。探索数据集是设计算法之前最为重要的一步，以图像分类为例，我们需要重点知道给定的数据集样本类别和各类别样本数量是否平衡，图像之间是否存在跨域问题（例如网上爬取的图像通常质量各异，存在噪声）。若是类别数远远超过类别样本数（比如类别10000，每个类别却只有10张图像），那通常的方法可能效果并不显著，这时候few-shot learning或者对数据集做进一步增强可能是你比较不错的选择。再如目标检测，待检测目标在数据集中的尺度范围是对检测器的性能有很大影响的部分。因此重点是检测大目标还是小目标、目标是否密集完全取决于数据集本身。所以，探索和进一步清洗数据集一直都是深度学习中最重要的一步。这是很多新手通常会忽略的一点。</p>
<p>2、探索模型结果。探索模型的结果，通常是需要对模型在验证集上的性能进行进一步的分析，这是如何进一步提升模型性能很重要的步骤。将模型在训练集和验证集都进行结果的验证和可视化，可直观的分析出模型是否存在较大偏差以及结果的正确性。以图像分类为例，若类别间样本数量很不平衡时，我们需要重点关注少样本类别在验证集的结果是否和训练集的出入较大，对出错类别可进一步进行模型数值分析以及可视化结果分析，进一步确认模型的行为。</p>
<p>3、监控训练和验证误差。首先很多情况下，我们忽略代码的规范性和算法撰写正确性验证，这点上容易产生致命的影响。在训练和验证都存在问题时，首先请确认自己的代码是否正确。其次，根据训练和验证误差进一步追踪模型的拟合状态。若训练数据集很小，此时监控误差则显得格外重要。确定了模型的拟合状态对进一步调整学习率的策略的选择或者其他有效超参数的选择则会更得心应手。</p>
<p>4、反向传播数值的计算，这种情况通常适合自己设计一个新操作的情况。目前大部分流行框架都已包含自动求导部分，但并不一定是完全符合你的要求的。验证求导是否正确的方式是比较自动求导的结果和有限差分计算结果是否一致。所谓有限差分即导数的定义，使用一个极小的值近似导数。</p>
 $$
f^{'}(x_0) = \lim_{n\rightarrow0}\frac{\Delta y}{\Delta x} = \lim_{n\rightarrow0}\frac{f(x_0+\Delta x -f(x_0))}{\Delta x}
$$ 
<h3 id="14-3-2-为什么要做学习率调整">14.3.2 为什么要做学习率调整?</h3>
<p>​	学习率可以说是模型训练最为重要的超参数。通常情况下，一个或者一组优秀的学习率既能加速模型的训练，又能得到一个较优甚至最优的精度。过大或者过小的学习率会直接影响到模型的收敛。我们知道，当模型训练到一定程度的时候，损失将不再减少，这时候模型的一阶梯度接近零，对应Hessian 矩阵通常是两种情况，一、正定，即所有特征值均为正，此时通常可以得到一个局部极小值，若这个局部极小值接近全局最小则模型已经能得到不错的性能了，但若差距很大，则模型性能还有待于提升，通常情况下后者在训练初最常见。二，特征值有正有负，此时模型很可能陷入了鞍点，若陷入鞍点，模型性能表现就很差。以上两种情况在训练初期以及中期，此时若仍然以固定的学习率，会使模型陷入左右来回的震荡或者鞍点，无法继续优化。所以，学习率衰减或者增大能帮助模型有效的减少震荡或者逃离鞍点。</p>
<h3 id="14-3-3-学习率调整策略有哪些？">14.3.3 学习率调整策略有哪些？</h3>
<p>通常情况下，大部分学习率调整策略都是衰减学习率，但有时若增大学习率也同样起到奇效。这里结合TensorFlow的内置方法来举例。</p>
<p>1、<strong>exponential_decay</strong>和<strong>natural_exp_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">exponential_decay(learning_rate, global_step, decay_steps, decay_rate,</span><br><span class="line">                   staircase=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">natural_exp_decay(learning_rate, global_step, decay_steps, decay_rate,</span><br><span class="line">                   staircase=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>指数衰减是最常用的衰减方式，这种方式简单直接，在训练初期衰减较大利于收敛，在后期衰减较小利于精调。以上两种均为指数衰减，区别在于后者使用以自然指数下降。</p>
<p><img src="img%5Cch14%5C%E6%8C%87%E6%95%B0%E8%A1%B0%E5%87%8F.jpeg" alt="./"></p>
<p>2、<strong>piecewise_constant</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">piecewise_constant(x, boundaries, values, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>分段设置学习率法，跟指数型类似，区别在于每个阶段的衰减并不是按指数调整。可在不同阶段设置手动不同的学习率。这种学习率重点在有利于精调。</p>
<p>3、<strong>polynomial_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">polynomial_decay(learning_rate, global_step, decay_steps,</span><br><span class="line">                  end_learning_rate=<span class="number">0.0001</span>, power=<span class="number">1.0</span>,</span><br><span class="line">                  cycle=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>多项式衰减，计算如下：</p>
 $$
global setp = min(global step, decay steps)
$$ 
 $$
lr_{decayed} = (lr-lr_{end})*(1-{globalstep\over decaysteps})^{power} +lr_{end}
$$ 
<p>有别于上述两种，多项式衰减则是在每一步迭代上都会调整学习率。主要看Power参数，若Power为1，则是下图中的红色直线；若power小于1，则是开1/power次方，为蓝色线；绿色线为指数，power大于1。</p>
<p><img src="img%5Cch14%5C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E8%A1%B0%E5%87%8F.jpeg" alt></p>
<p>此外，需要注意的是参数cycle，cycle对应的是一种周期循环调整的方式。这种cycle策略主要目的在后期防止在一个局部极小值震荡，若跳出该区域或许能得到更有的结果。这里说明cycle的方式不止可以在多项式中应用，可配合类似的周期函数进行衰减，如下图。</p>
<p><img src="img%5Cch14%5Ccycle%E8%A1%B0%E5%87%8F.jpeg" alt></p>
<p>4、<strong>inverse_time_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inverse_time_decay(learning_rate, global_step, decay_steps, decay_rate,</span><br><span class="line">                   staircase=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>逆时衰减，这种方式和指数型类似。如图，<img src="img%5Cch14%5C%E9%80%86%E6%97%B6%E8%A1%B0%E5%87%8F.jpeg" alt></p>
<p>5、<strong>cosine_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cosine_decay(learning_rate, global_step, decay_steps, alpha=<span class="number">0.0</span>,</span><br><span class="line">                 name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>余弦衰减，即按余弦函数的方式衰减学习率，如图</p>
<p><img src="img%5Cch14%5C%E4%BD%99%E5%BC%A6%E8%A1%B0%E5%87%8F.jpeg" alt></p>
<p>6、<strong>cosine_decay_restarts</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cosine_decay_restarts(learning_rate, global_step, first_decay_steps,</span><br><span class="line">                           t_mul=<span class="number">2.0</span>, m_mul=<span class="number">1.0</span>, alpha=<span class="number">0.0</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>余弦衰减，即余弦版本的cycle策略，作用与多项式衰减中的cycle相同。区别在于余弦重启衰减会重新回到初始学习率，拉长周期，而多项式版本则会逐周期衰减。</p>
<p><img src="img%5Cch14%5C%E4%BD%99%E5%BC%A6cycle%E8%A1%B0%E5%87%8F.jpeg" alt></p>
<p>7、<strong>linear_cosine_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear_cosine_decay(learning_rate, global_step, decay_steps,</span><br><span class="line">                        num_periods=<span class="number">0.5</span>, alpha=<span class="number">0.0</span>, beta=<span class="number">0.001</span>,</span><br><span class="line">                        name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>线性余弦衰减，主要应用于增强学习领域。</p>
<p><img src="img%5Cch14%5C%E7%BA%BF%E6%80%A7%E4%BD%99%E5%BC%A6%E8%A1%B0%E5%87%8F.jpeg" alt></p>
<p>8、<strong>noisy_linear_cosine_decay</strong></p>
<p>噪声线性余弦衰减，即在线性余弦衰减中加入随机噪声，增大寻优的随机性。</p>
<p><img src="img%5Cch14%5C%E5%99%AA%E5%A3%B0%E7%BA%BF%E6%80%A7%E4%BD%99%E5%BC%A6%E8%A1%B0%E5%87%8F.jpeg" alt></p>
<h3 id="14-3-4-极端批样本数量下，如何训练网络？">14.3.4 极端批样本数量下，如何训练网络？</h3>
<p>​	极端批样本情况一般是指batch size为1或者batch size在6000以上的情况。这两种情况，在使用不合理的情况下都会导致模型最终性能无法达到最优甚至是崩溃的情况。</p>
<p>​	在目标检测、分割或者3D图像等输入图像尺寸较大的场景，通常batch size 会非常小。而在14.2.4中，我们已经讲到这种情况会导致梯度的不稳定以及batchnorm统计的不准确。针对梯度不稳定的问题，通常不会太致命，若训练中发现梯度不稳定导致性能的严重降低时可采用累计梯度的策略，即每次计算完不反向更新，而是累计多次的误差后进行一次更新，这是一种在内存有限情况下实现有效梯度更新的一个策略。batch size过小通常对batchnorm的影响是最大的，若网络模型中存在batchnorm，batch size若只为1或者2时会对训练结果产生非常大的影响。这时通常有两种策略，一、若模型使用了预训练网络，可冻结预训练网络中batchnorm的模型参数，有效降低batch size引起的统计量变化的影响。二、在网络不是过深或者过于复杂时可直接移除batchnorm或者使用groupnorm代替batchnorm，前者不多阐释，后者是有FAIR提出的一种用于减少batch对batchnorm影响，其主要策略是先将特征在通道上进行分组，然后在组内进行归一化。即归一化操作上完全与batch size无关。这种groupnorm的策略被证实在极小批量网络训练上能达到较优秀的性能。当然这里也引入里group这个超参数，一般情况下建议不宜取group为1或者各通道单独为组的group数量，可结合实际网络稍加调试。</p>
<p>​	为了降低训练时间的成本，多机多卡的分布式系统通常会使用超大的batch size进行网络训练。同样的在14.2.4中，我们提到了超大batch size会带来梯度方向过于一致而导致的精度大幅度降低的问题。这时通常可采用层自适应速率缩放（LARS）算法。从理论认知上将，batch size增大会减少反向传播的梯度更新次数，但为了达到相同的模型效果，需要增大学习率。但学习率一旦增大，又会引起模型的不收敛。为了解决这一矛盾，LARS算法就在各层上自适应的计算一个本地学习率用于更新本层的参数，这样能有效的提升训练的稳定性。目前利用LARS算法，腾讯公司使用65536的超大batch size能将ResNet50在ImageNet在4分钟完成训练，而谷歌使用32768的batch size使用TPU能将该时间缩短至2分钟。</p>
<h2 id="14-4-合理使用预训练网络">14.4 合理使用预训练网络</h2>
<h3 id="14-4-1-什么是微调（fine-tune）">14.4.1 什么是微调（fine-tune）</h3>
<p>​	微调（fine-tune），顾名思义指稍微调整参数即可得到优秀的性能，是迁移学习的一种实现方式。微调和从头训练（train from scratch）的本质区别在于模型参数的初始化，train from scratch通常指对网络各类参数进行随机初始化（当然随机初始化也存在一定技巧），随机初始化模型通常不具有任何预测能力，通常需要大量的数据或者特定域的数据进行从零开始的训练，这样需要训练到优秀的模型通常是稍困难的。而微调的网络，网络各类参数已经在其他数据集（例如ImageNet数据集）完成较好调整的，具备了较优秀的表达能力。因此，我们只需要以较小的学习速率在自己所需的数据集领域进行学习即可得到较为优秀的模型。微调通常情况下，无须再重新设计网络结构，预训练模型提供了优秀的结构，只需稍微修改部分层即可。在小数据集上，通常微调的效果比从头训练要好很多，原因在于数据量较小的前提下，训练更多参数容易导致过度拟合。</p>
<h3 id="14-4-2-微调有哪些不同方法？">14.4.2 微调有哪些不同方法？</h3>
<p>​	以图像分类为例，通常情况下由于不同数据集需要的类别数不同，我们需要修改网络的输出顶层。这种情况下有两种微调方式：</p>
<ul>
<li>
<p>不冻结网络模型的任何层，对最后的改动层使用较大的学习率，对未改动层以较小的学习率进行训练全模型训练，进行多轮训练即可。即一步完成训练。</p>
</li>
<li>
<p>冻结除了顶部改动层以外的所有层参数，即不对冻结部分的层进行参数训练更新，进行若干轮的微调训练后，放开顶部层以下的若干层或者全部放开所有层的参数，再次进行若干轮训练即可。即分多步训练。</p>
<p>以上两种都属于微调。目前由于存在大量优秀的预训练模型，如何确定哪个模型适合自己的任务并能得到最佳性能需要花大量的时间探索。此时，上述的前者是种不错训练方式，你无须进行过多分步的操作。而当探索到一个比较适合的模型时，你不妨可以再次重新尝试下以第二种方式进行训练，或许能得到相比于前者稍高些的性能，因为小数据集上调整过多的参数过拟合的机率也会增大，当然这并不是绝对的。</p>
</li>
</ul>
<h3 id="14-4-3-微调先冻结底层，训练顶层的原因？">14.4.3 微调先冻结底层，训练顶层的原因？</h3>
<p>​	14.12中第二种冻结多步训练的方式。首先冻结除了顶部改动层以外的所有层参数，对顶层进行训练，这个过程可以理解为顶层的域适应训练，主要用来训练适应模型的现有特征空间，防止顶层糟糕的初始化，对已经具备一定表达能力的层的干扰和破坏，影响最终的性能。之后，在很多深度学习框架教程中会使用放开顶层往下一半的层数，继续进行微调。这样的好处在于越底层的特征通常是越通用的特征，越往上其整体的高层次语义越完备，这通过感受野很容易理解。所以，若预训练模型的数据和微调训练的数据语义差异越大（例如ImageNet的预模型用于医学图像的训练），那越往顶层的特征语义差异就越大，因此通常也需要进行相应的调整。</p>
<h3 id="14-4-4-不同的数据集特性下如何微调？">14.4.4 不同的数据集特性下如何微调？</h3>
<ul>
<li>数据集数据量少，数据和原数据集类似。这是通常做法只需修改最后的输出层，训练即可，训练过多参数容易过拟合。</li>
<li>数据集数据量少，数据和原数据集差异较大。由于数据差异较大，可以在完成输出顶层的微调后，微调顶层往下一半的层数，进行微调。</li>
<li>数据集数据量大，数据与原数据集差异较大。这种情况下，通常已经不需要用预训练模型进行微调，通常直接重新训练即可。</li>
<li>数据集数据量大，数据与原数据类似。这时预训练模型的参数是个很好的初始化，可利用预训练模型放开所有层以较小的学习率微调即可。</li>
</ul>
<h3 id="14-4-4-目标检测中使用预训练模型的优劣？">14.4.4 目标检测中使用预训练模型的优劣？</h3>
<p>​	目标检测中无论是一阶段的YOLO、SSD或者RetinaNet 还是二阶段的Faster R-CNN、R-FCN 和 FPN都是基于ImageNet上预训练好的分类模型。</p>
<p>​	优势在于：</p>
<p>​	1、正如大部分微调的情况一样，使用预训练网络已拥有优秀的语义特征，能有效的加快训练速度；</p>
<p>​	2、其次，对于大部分二阶段的模型来说，并未实现严格意义上的完全端对端的训练，所以使用预训练模型能直接提取到语义特征，能使两个阶段的网络更容易实现模型的优化。</p>
<p>​	劣势在于，分类模型和检测模型之间仍然存在一定任务上的差异：</p>
<p>​	1、分类模型大部分训练于单目标数据，对同时进行多目标的捕捉能力稍弱，且不关注目标的位置，在一定程度上让模型损失部分空间信息，这对检测模型通常是不利的；</p>
<p>​	2、域适应问题，若预训练模型（ImageNet）和实际检测器的使用场景（医学图像，卫星图像）差异较大时，性能会受到影响；</p>
<p>​	3、使用预训练模型就意味着难以自由改变网络结构和参数限制了应用场合。</p>
<h3 id="14-4-5-目标检测中如何从零开始训练-train-from-scratch-？">14.4.5 目标检测中如何从零开始训练(train from scratch)？</h3>
<p>​	结合FAIR相关的研究，我们可以了解目标检测和其他任务从零训练模型一样，只要拥有足够的数据以及充分而有效的训练，同样能训练出不亚于利用预训练模型的检测器。这里我们提供如下几点建议：</p>
<p>​	1、数据集不大时，同样需要进行数据集增强。</p>
<p>​	2、预训练模型拥有更好的初始化，train from scratch需要更多的迭代次数以及时间训练和优化检测器。而二阶段模型由于并不是严格的端对端训练，此时可能需要更多的迭代次数以及时间，而一阶段检测模型训练会相对更容易些（例如DSOD以ScratchDet及）。</p>
<p>​	3、目标检测中train from scratch最大的问题还是batch size过小。所以可采取的策略是增加GPU使用异步batchnorm增大batch size，若条件限制无法使用更多GPU时，可使用groupnorm代替batchnorm</p>
<p>​	4、由于分类模型存在对多目标的捕捉能力弱以及对物体空间位置信息不敏感等问题，可借鉴DetNet训练一个专属于目标检测的模型网络，增强对多目标、尺度和位置拥有更强的适应性。</p>
<h2 id="14-5-如何改善-GAN-的性能">14.5 如何改善 GAN 的性能</h2>
<p>优化GAN性能通常需要在如下几个方面进行</p>
<ul>
<li>设计或选择更适合目的代价函数。</li>
<li>添加额外的惩罚。</li>
<li>避免判别器过度自信和生成器过度拟合。</li>
<li>更好的优化模型的方法。</li>
<li>添加标签明确优化目标。</li>
</ul>
<p>GAN常用训练技巧</p>
<ul>
<li>
<p>输入规范化到（-1，1）之间，最后一层的激活函数使用tanh（BEGAN除外）</p>
</li>
<li>
<p>使用wassertein GAN的损失函数，</p>
</li>
<li>
<p>如果有标签数据的话，尽量使用标签，也有人提出使用反转标签效果很好，另外使用标签平滑，单边标签平滑或者双边标签平滑</p>
</li>
<li>
<p>使用mini-batch norm， 如果不用batch norm 可以使用instance norm 或者weight norm</p>
</li>
<li>
<p>避免使用RELU和pooling层，减少稀疏梯度的可能性，可以使用leakrelu激活函数</p>
</li>
<li>
<p>优化器尽量选择ADAM，学习率不要设置太大，初始1e-4可以参考，另外可以随着训练进行不断缩小学习率，</p>
</li>
<li>
<p>给D的网络层增加高斯噪声，相当于是一种正则</p>
</li>
</ul>
<h2 id="14-6-AutoML">14.6 AutoML</h2>
<h3 id="14-6-1-什么是AutoML？">14.6.1 什么是AutoML？</h3>
<p>​	目前一个优秀的机器学习和深度学习模型，离不开这几个方面：</p>
<p>​	一、优秀的数据预处理；</p>
<p>​	二、合适的模型结构和功能；</p>
<p>​	三、优秀的训练策略和超参数；</p>
<p>​	四、合适的后处理操作；</p>
<p>​	五、严格的结果分析。</p>
<p>​	这几方面都对最终的结果有着举足轻重的影响，这也是目前的数据工程师和学者们的主要工作。但由于这每一方面都十分繁琐，尤其是在构建模型和训练模型上。而大部分情况下，这些工作有无须过深专业知识就能使用起来。所以AutoML主要的作用就是来帮助实现高效的模型构建和超参数调整。例如深度学习网络的架构搜索、超参数的重要性分析等等。当然AutoML并不简单的进行暴力或者随机的搜索，其仍然需要机器学习方面的知识，例如贝叶斯优化、强化学习、元学习以及迁移学习等等。目前也有些不错的AutoML工具包，例如Alex Honchar的Hyperopt、微软的NNI、Autokeras等。</p>
<p>目前AutoML已经成为最新的研究热点，有兴趣的可以参考<a target="_blank" rel="noopener" href="https://www.automl.org/automl/literature-on-neural-architecture-search/">AutoML literature</a>。</p>
<h3 id="14-6-2-自动化超参数搜索方法有哪些？">14.6.2 自动化超参数搜索方法有哪些？</h3>
<p>​	目前自动化搜索主要包含网格搜索，随机搜索，基于模型的超参优化</p>
<p>​	网格搜索：</p>
<p>​		通常当超参数量较少的时候，可以使用网格搜索法。即列出每个超参数的大致候选集合。利用这些集合		进行逐项组合优化。在条件允许的情况下，重复进行网格搜索会当优秀，当然每次重复需要根据上一步得到的最优参数组合，进行进一步的细粒度的调整。网格搜索最大的问题就在于计算时间会随着超参数的数量指数级的增长。</p>
<p>​	随机搜索：</p>
<p>​		随机搜索，是一种用来替代网格搜索的搜索方式。随机搜索有别于网格搜索的一点在于，我们不需要设定一个离散的超参数集合，而是对每个超参数定义一个分布函数来生成随机超参数。随机搜索相比于网格搜索在一些不敏感超参上拥有明显优势。例如网格搜索对于批样本数量（batch size），在[16,32,64]这些范围内进行逐项调试，这样的调试显然收益更低下。当然随机搜索也可以进行细粒度范围内的重复的搜索优化。</p>
<p><img src="img%5Cch14%5C14.14.png" alt></p>
<p>​	基于模型的超参优化：</p>
<p>​		有别于上述两种的搜索策略，基于模型的超参调优问题转化为了优化问题。直觉上会考虑是否进行一个可导建模，然后利用梯度下降进行优化。但不幸的是我们的超参数通常情况下是离散的，而且其计算代价依旧很高。</p>
<p>​		基于模型的搜索算法，最常见的就是贝叶斯超参优化。有别于的网格搜索和随机搜索独立于前几次搜索结果的搜索，贝叶斯则是利用历史的搜索结果进行优化搜索。其主要有四部分组成，1.目标函数，大部分情况下就是模型验证集上的损失。2、搜索空间，即各类待搜索的超参数。3、优化策略，建立的概率模型和选择超参数的方式。4、历史的搜索结果。首先对搜索空间进行一个先验性的假设猜想，即假设一种选择超参的方式，然后不断的优化更新概率模型，最终的目标是找到验证集上误差最小的一组超参数。</p>
<h3 id="14-6-3-什么是神经网络架构搜索（NAS）">14.6.3 什么是神经网络架构搜索（NAS）</h3>
<p>2015至2017年间，是CNN网络设计最兴盛的阶段，大多都是由学者人工设计的网络结构。这个过程通常会很繁琐。其主要原因在于对不同模块组件的组成通常是个黑盒优化的问题，此外，在不同结构超参数以及训练超参数的选择优化上非凸优化问题，或者是个混合优化问题，既有离散空间又有连续空间。NAS（Neural Architecture Search）的出现就是为了解决如何通过机器策略和自动化的方式设计出优秀高效的网络。而这种策略通常不是统一的标准，不同的网络结合实际的需求通常会有不同的设计，比如移动端的模型会在效率和精度之间做平衡。目前，NAS也是AUTOML中最重要的部分。NAS通常会分为三个方面，搜索空间（在哪搜索），搜索策略（如何搜索）及评价预估。</p>
<ul>
<li>
<p>搜索空间，即在哪搜索，定义了优化问题所需变量。不同规模的搜索空间的变量其对于的难度也是不一样的。早期由于网络结构以及层数相对比较简单，参数量较少，因此会更多的使用遗传算法等进化算法对网络的超参数和权重进行优化。深度学习发展到目前，模型网络结构越来越复杂，参数量级越来越庞大，这些进化算法已经无法继续使用。但若我们先验给定一些网络结构和超参数，模型的性能已经被限制在给定的空间，此时搜索的空间已变得有限，所以只需对复杂模型的架构参数和对应的超参数进行优化即可。</p>
</li>
<li>
<p>搜索策略， 即如何搜索，定义了如何快速、准确找到最优的网络结构参数配置的策略。常见的搜索方法包括：随机搜索、贝叶斯优化、强化学习、进化算法以及基于模型的搜索算法。其中主要代表为2017年谷歌大脑的使用强化学习的搜索方法。</p>
</li>
<li>
<p>评价预估，定义了如何高效对搜索的评估策略。深度学习中，数据规模往往是庞大的，模型要在如此庞大的数据规模上进行搜索，这无疑是非常耗时的，对优化也会造成非常大的困难，所以需要一些高效的策略做近似的评估。 这里一般会有如下三种思路：</p>
<p>一、使用些低保真的训练集来训练模型。低保真在实际中可以用不同的理解，比如较少的迭代次数，用一小部分数据集或者保证结构的同时减少通道数等。这些方法都可以在测试优化结构时大大降低计算时间，当然也会存在一定的偏差。但架构搜索从来并不是要一组固定的参数，而是一种优秀的模型结构。最终选取时，只需在较优秀的几组结构中进行全集训练，进行择优选取即可。</p>
<p>二、使用代理模型。除了低保真的训练方式外，学者们提出了一种叫做代理模型的回归模型，采用例如插值等策略对已知的一些参数范围进行预测，目的是为了用尽可能少的点预测到最佳的结果。</p>
<p>三、参数级别的迁移。例如知识蒸馏等。用已训练好的模型权重参数对目标问题搜索，通常会让搜索拥有一个优秀的起点。由于积累了大量的历史寻优数据，对新问题的寻优将会起到很大的帮助。</p>
</li>
</ul>
<h3 id="14-6-4-NASNet的设计策略">14.6.4 NASNet的设计策略</h3>
<p>NASNet是最早由google brain 通过网络架构搜索策略搜索并成功训练ImageNet的网络，其性能超越所有手动设计的网络模型。关于NASNet的搜索策略，首先需要参考google brain发表在ICLR2017的论文《Neural Architecture Search with Reinforcement Learning》。该论文是最早成功通过架构搜索策略在cifar-10数据集上取得比较不错效果的工作。NASNet很大程度上是沿用该搜索框架的设计思想。</p>
<p>NASNet的核心思想是利用强化学习对搜索空间内的结构进行反馈探索。架构搜索图如下，定义了一个以RNN为核心的搜索控制器。在搜索空间以概率p对模型进行搜索采样。得到网络模型A后，对该模型进行训练，待模型收敛得到设定的准确率R后，将梯度传递给控制器RNN进行梯度更新。</p>
<p><img src="img%5Cch14%5CNAS%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5.png" alt></p>
<p>​									架构搜索策略流程</p>
<p>RNN控制器会对卷积层的滤波器的尺寸、数量以及滑动间隔进行预测。每次预测的结果都会作为下一级的输入，档层数达到设定的阈值时，会停止预测。而这个阈值也会随着训练的进行而增加。这里的控制器之预测了卷积，并没有对例如inception系列的分支结构或者ResNet的跳级结构等进行搜索。所以，控制器需要进一步扩展到预测这些跳级结构上，这样搜索空间相应的也会增大。为了预测这些结构，RNN控制器内每一层都增加了一个预测跳级结构的神经元，文中称为锚点，稍有不同的是该锚点的预测会由前面所有层的锚点状态决定。</p>
<p><img src="img%5Cch14%5CRNN%E6%8E%A7%E5%88%B6%E5%99%A8.png" alt></p>
<p>​									RNN控制器</p>
<p>NASNet大体沿用了上述生成网络结构的机器，并在此基础上做了如下两点改进：</p>
<p>1、先验行地加入inception系列和ResNet的堆叠模块的思想。其定义了两种卷积模块，Normal Cell和Reduction Cell，前者不进行降采样，而后者是个降采样的模块。而由这两种模块组成的结构可以很方便的通过不同数量的模块堆叠将其从小数据集搜索到的架构迁移到大数据集上，大大提高了搜索效率。</p>
<p><img src="img%5Cch14%5CNASNet%E7%9A%84RNN%E6%8E%A7%E5%88%B6%E5%99%A8.png" alt></p>
<p>​									NASNet的RNN控制器</p>
<p>2、对RNN控制进行优化，先验性地将各种尺寸和类型的卷积和池化层加入到搜索空间内，用预测一个卷积模块代替原先预测一层卷积。如图，控制器RNN不在预测单个卷积内的超参数组成，而是对一个模块内的每一个部分进行搜索预测，搜索的空间则限定在如下这些操作中：</p>
<p>​						• identity					  • 1x3 then 3x1 convolution<br>
​						• 1x7 then 7x1 convolution	  • 3x3 dilated convolution<br>
​						• 3x3 average pooling 			  • 3x3 max pooling<br>
​						• 5x5 max pooling			  • 7x7 max pooling<br>
​						• 1x1 convolution				  • 3x3 convolution<br>
​						• 3x3 depthwise-separable conv • 5x5 depthwise-seperable conv<br>
​						• 7x7 depthwise-separable conv</p>
<p>在模块内的连接方式上也提供了element-wise addition和concatenate两种方式。NASNet的搜索方式和过程对NAS的一些后续工作都具有非常好的参考借鉴意义。</p>
<h3 id="14-6-5-网络设计中，为什么卷积核设计尺寸都是奇数">14.6.5 网络设计中，为什么卷积核设计尺寸都是奇数</h3>
<p>我们发现在很多大部分网络设计时都会使用例如3x3/5x5/7x7等奇数尺寸卷积核，主要原因有两点：</p>
<ul>
<li>保证像素点中心位置，避免位置信息偏移</li>
<li>填充边缘时能保证两边都能填充，原矩阵依然对称</li>
</ul>
<h3 id="14-6-6-网络设计中，权重共享的形式有哪些，为什么要权重共享">14.6.6 网络设计中，权重共享的形式有哪些，为什么要权重共享</h3>
<p>权重共享的形式：</p>
<ul>
<li>深度学习中，权重共享最具代表性的就是卷积网络的卷积操作。卷积相比于全连接神经网络参数大大减少；</li>
<li>多任务网络中，通常为了降低每个任务的计算量，会共享一个骨干网络。</li>
<li>一些相同尺度下的结构化递归网络</li>
</ul>
<p>权重共享的好处：</p>
<p>​	权重共享一定程度上能增强参数之间的联系，获得更好的共性特征。同时很大程度上降低了网络的参数，节省计算量和计算所需内存（当然，结构化递归并不节省计算量）。此外权重共享能起到很好正则的作用。正则化的目的是为了降低模型复杂度，防止过拟合，而权重共享则正好降低了模型的参数和复杂度。</p>
<p>​	因此一个设计优秀的权重共享方式，在降低计算量的同时，通常会较独享网络有更好的效果。</p>
<p>权重共享不仅在人工设计（human-invented）的网络结构中有简化参数，降低模型复杂度的作用，在神经网络搜索（NAS）的网络结构中可以使得child model的计算效率提升，使得搜索过程可以在单卡GPU上复现，见Efficient NAS(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.03268">ENAS</a>)。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%88%B7%E5%85%AB%E8%82%A1/" rel="tag"># 刷八股</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/03/19/deep_learning/ch13/" rel="prev" title="优化算法面试题">
      <i class="fa fa-chevron-left"></i> 优化算法面试题
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/03/19/deep_learning/ch12/" rel="next" title="网络搭建及训练面试题">
      网络搭建及训练面试题 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">超参数调整</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#14-1-%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="nav-number">1.1.</span> <span class="nav-text">14.1 写在前面</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-2-%E8%B6%85%E5%8F%82%E6%95%B0%E6%A6%82%E5%BF%B5"><span class="nav-number">1.2.</span> <span class="nav-text">14.2 超参数概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-1-%E4%BB%80%E4%B9%88%E6%98%AF%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%8C%E5%8F%82%E6%95%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">1.2.1.</span> <span class="nav-text">14.2.1 什么是超参数，参数和超参数的区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%8C%85%E5%90%AB%E5%93%AA%E4%BA%9B%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="nav-number">1.2.2.</span> <span class="nav-text">14.2.2 神经网络中包含哪些超参数？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-3-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%9F"><span class="nav-number">1.2.3.</span> <span class="nav-text">14.2.3 为什么要进行超参数调优？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-4-%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E9%A1%BA%E5%BA%8F"><span class="nav-number">1.2.4.</span> <span class="nav-text">14.2.4 超参数的重要性顺序</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-5-%E9%83%A8%E5%88%86%E8%B6%85%E5%8F%82%E6%95%B0%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="nav-number">1.2.5.</span> <span class="nav-text">14.2.5 部分超参数如何影响模型性能？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-2-6-%E9%83%A8%E5%88%86%E8%B6%85%E5%8F%82%E6%95%B0%E5%90%88%E9%80%82%E7%9A%84%E8%8C%83%E5%9B%B4"><span class="nav-number">1.2.6.</span> <span class="nav-text">14.2.6 部分超参数合适的范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-3-%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E4%B8%AD%E7%9A%84%E8%B6%85%E5%8F%82%E8%B0%83%E6%95%B4%E7%AD%96%E7%95%A5"><span class="nav-number">1.3.</span> <span class="nav-text">14.3 网络训练中的超参调整策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#14-3-1-%E5%A6%82%E4%BD%95%E8%B0%83%E8%AF%95%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="nav-number">1.3.1.</span> <span class="nav-text">14.3.1 如何调试模型？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-3-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%81%9A%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4"><span class="nav-number">1.3.2.</span> <span class="nav-text">14.3.2 为什么要做学习率调整?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-3-3-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E7%AD%96%E7%95%A5%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="nav-number">1.3.3.</span> <span class="nav-text">14.3.3 学习率调整策略有哪些？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-3-4-%E6%9E%81%E7%AB%AF%E6%89%B9%E6%A0%B7%E6%9C%AC%E6%95%B0%E9%87%8F%E4%B8%8B%EF%BC%8C%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C%EF%BC%9F"><span class="nav-number">1.3.4.</span> <span class="nav-text">14.3.4 极端批样本数量下，如何训练网络？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-4-%E5%90%88%E7%90%86%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C"><span class="nav-number">1.4.</span> <span class="nav-text">14.4 合理使用预训练网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#14-4-1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%BE%AE%E8%B0%83%EF%BC%88fine-tune%EF%BC%89"><span class="nav-number">1.4.1.</span> <span class="nav-text">14.4.1 什么是微调（fine-tune）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-4-2-%E5%BE%AE%E8%B0%83%E6%9C%89%E5%93%AA%E4%BA%9B%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="nav-number">1.4.2.</span> <span class="nav-text">14.4.2 微调有哪些不同方法？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-4-3-%E5%BE%AE%E8%B0%83%E5%85%88%E5%86%BB%E7%BB%93%E5%BA%95%E5%B1%82%EF%BC%8C%E8%AE%AD%E7%BB%83%E9%A1%B6%E5%B1%82%E7%9A%84%E5%8E%9F%E5%9B%A0%EF%BC%9F"><span class="nav-number">1.4.3.</span> <span class="nav-text">14.4.3 微调先冻结底层，训练顶层的原因？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-4-4-%E4%B8%8D%E5%90%8C%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E7%89%B9%E6%80%A7%E4%B8%8B%E5%A6%82%E4%BD%95%E5%BE%AE%E8%B0%83%EF%BC%9F"><span class="nav-number">1.4.4.</span> <span class="nav-text">14.4.4 不同的数据集特性下如何微调？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-4-4-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E5%8A%A3%EF%BC%9F"><span class="nav-number">1.4.5.</span> <span class="nav-text">14.4.4 目标检测中使用预训练模型的优劣？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-4-5-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83-train-from-scratch-%EF%BC%9F"><span class="nav-number">1.4.6.</span> <span class="nav-text">14.4.5 目标检测中如何从零开始训练(train from scratch)？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-5-%E5%A6%82%E4%BD%95%E6%94%B9%E5%96%84-GAN-%E7%9A%84%E6%80%A7%E8%83%BD"><span class="nav-number">1.5.</span> <span class="nav-text">14.5 如何改善 GAN 的性能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-6-AutoML"><span class="nav-number">1.6.</span> <span class="nav-text">14.6 AutoML</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#14-6-1-%E4%BB%80%E4%B9%88%E6%98%AFAutoML%EF%BC%9F"><span class="nav-number">1.6.1.</span> <span class="nav-text">14.6.1 什么是AutoML？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-6-2-%E8%87%AA%E5%8A%A8%E5%8C%96%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="nav-number">1.6.2.</span> <span class="nav-text">14.6.2 自动化超参数搜索方法有哪些？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-6-3-%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%EF%BC%88NAS%EF%BC%89"><span class="nav-number">1.6.3.</span> <span class="nav-text">14.6.3 什么是神经网络架构搜索（NAS）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-6-4-NASNet%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%AD%96%E7%95%A5"><span class="nav-number">1.6.4.</span> <span class="nav-text">14.6.4 NASNet的设计策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-6-5-%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1%E4%B8%AD%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8D%B7%E7%A7%AF%E6%A0%B8%E8%AE%BE%E8%AE%A1%E5%B0%BA%E5%AF%B8%E9%83%BD%E6%98%AF%E5%A5%87%E6%95%B0"><span class="nav-number">1.6.5.</span> <span class="nav-text">14.6.5 网络设计中，为什么卷积核设计尺寸都是奇数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-6-6-%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1%E4%B8%AD%EF%BC%8C%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB%E7%9A%84%E5%BD%A2%E5%BC%8F%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB"><span class="nav-number">1.6.6.</span> <span class="nav-text">14.6.6 网络设计中，权重共享的形式有哪些，为什么要权重共享</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Tom</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tom</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
