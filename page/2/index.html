<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="算法工程师的日常">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="算法工程师的日常">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Tom">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>算法工程师的日常</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">算法工程师的日常</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/deep_learning/ch3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/deep_learning/ch3/" class="post-title-link" itemprop="url">深度学习基础面试题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-24 10:13:39" itemprop="dateModified" datetime="2024-03-24T10:13:39+08:00">2024-03-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">算法面试</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>深度学习基础</h1>
<h2 id="3-1-基本概念">3.1 基本概念</h2>
<h3 id="3-1-1-神经网络组成？">3.1.1 神经网络组成？</h3>
<p>神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，我们先从最简单的神经网络说起。</p>
<p><strong>感知机</strong></p>
<p>多层感知机中的特征神经元模型称为感知机，由<em>Frank Rosenblatt</em>于1957年发明。</p>
<p>简单的感知机如下图所示：</p>
<p><img src="3-1.png" alt></p>
<p>其中 $x_1$ ， $x_2$ ， $x_3$ 为感知机的输入，其输出为：</p>
 $$
output = \left\{
\begin{aligned}
0, \quad if \ \ \sum_i w_i x_i \leqslant threshold \\
1, \quad if \ \ \sum_i w_i x_i > threshold
\end{aligned}
\right.
$$ 
<p>假如把感知机想象成一个加权投票机制，比如 3 位评委给一个歌手打分，打分分别为 $ 4 $ 分、 $1$  分、 $-3 $ 分，这 $ 3$  位评分的权重分别是  $1、3、2$ ，则该歌手最终得分为  $4 \times 1 + 1 \times 3 + (-3) \times 2 = 1$  。按照比赛规则，选取的  $threshold$  为  $3$ ，说明只有歌手的综合评分大于 $ 3$  时，才可顺利晋级。对照感知机，该选手被淘汰，因为：</p>
 $$
\sum_i w_i x_i < threshold=3, output = 0
$$ 
<p>用  $-b$   代替  $threshold$ ，输出变为：</p>
 $$
output = \left\{
\begin{aligned}
0, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b \leqslant 0 \\
1, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b > 0
\end{aligned}
\right.
$$ 
<p>设置合适的   $\boldsymbol{x}$   和   $b$  ，一个简单的感知机单元的与非门表示如下：</p>
<p><img src="3-2.png" alt></p>
<p>当输入为  $0$ ， $1$  时，感知机输出为  $ 0 \times (-2) + 1 \times (-2) + 3 = 1$ 。</p>
<p>复杂一些的感知机由简单的感知机单元组合而成：</p>
<p><img src="3-3.png" alt></p>
<p><strong>多层感知机</strong></p>
<p>多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第  $ i $  层的每个神经元和第  $ i-1 $  层的每个神经元都有连接。</p>
<p><img src="3.1.1.5.png" alt></p>
<p>输出层可以不止有 $ 1$  个神经元。隐藏层可以只有 $ 1$  层，也可以有多层。输出层为多个神经元的神经网络例如下图所示：</p>
<p><img src="3.1.1.6.png" alt></p>
<h3 id="3-1-2-神经网络有哪些常用模型结构？">3.1.2 神经网络有哪些常用模型结构？</h3>
<p>下图包含了大部分常用的模型：</p>
<p><img src="3-7.jpg" alt></p>
<h3 id="3-1-3-如何选择深度学习开发平台？">3.1.3 如何选择深度学习开发平台？</h3>
<p>​	现有的深度学习开源平台主要有 Caffe, PyTorch, MXNet, CNTK, Theano, TensorFlow, Keras, fastai等。那如何选择一个适合自己的平台呢，下面列出一些衡量做参考。</p>
<p><strong>参考1：与现有编程平台、技能整合的难易程度</strong></p>
<p>​	主要是前期积累的开发经验和资源，比如编程语言，前期数据集存储格式等。</p>
<p><strong>参考2: 与相关机器学习、数据处理生态整合的紧密程度</strong></p>
<p>​	深度学习研究离不开各种数据处理、可视化、统计推断等软件包。考虑建模之前，是否具有方便的数据预处理工具？建模之后，是否具有方便的工具进行可视化、统计推断、数据分析。</p>
<p><strong>参考3：对数据量及硬件的要求和支持</strong></p>
<p>​	深度学习在不同应用场景的数据量是不一样的，这也就导致我们可能需要考虑分布式计算、多GPU计算的问题。例如，对计算机图像处理研究的人员往往需要将图像文件和计算任务分部到多台计算机节点上进行执行。当下每个深度学习平台都在快速发展，每个平台对分布式计算等场景的支持也在不断演进。</p>
<p><strong>参考4：深度学习平台的成熟程度</strong></p>
<p>​	成熟程度的考量是一个比较主观的考量因素，这些因素可包括：社区的活跃程度；是否容易和开发人员进行交流；当前应用的势头。</p>
<p><strong>参考5：平台利用是否多样性？</strong></p>
<p>​	有些平台是专门为深度学习研究和应用进行开发的，有些平台对分布式计算、GPU 等构架都有强大的优化，能否用这些平台/软件做其他事情？比如有些深度学习软件是可以用来求解二次型优化；有些深度学习平台很容易被扩展，被运用在强化学习的应用中。</p>
<h3 id="3-1-4-为什么使用深层表示">3.1.4 为什么使用深层表示?</h3>
<ol>
<li>深度神经网络是一种特征递进式的学习算法，浅层的神经元直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而深层的特征则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。</li>
<li>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</li>
</ol>
<h3 id="3-1-5-为什么深层神经网络难以训练？">3.1.5 为什么深层神经网络难以训练？</h3>
<ol>
<li>
<p>梯度消失<br>
梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。</p>
<p>​	梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率：</p>
</li>
</ol>
<p><img src="3-8.png" alt></p>
<ol start="2">
<li>
<p>梯度爆炸<br>
在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为 $NaN$ 值，再也无法更新。</p>
</li>
<li>
<p>权重矩阵的退化导致模型的有效自由度减少。</p>
<p>​	参数空间中学习的退化速度减慢，导致减少了模型的有效维数，网络的可用自由度对学习中梯度范数的贡献不均衡，随着相乘矩阵的数量（即网络深度）的增加，矩阵的乘积变得越来越退化。在有硬饱和边界的非线性网络中（例如 ReLU 网络），随着深度增加，退化过程会变得越来越快。Duvenaud等人2014年的论文里展示了关于该退化过程的可视化：</p>
</li>
</ol>
<p><img src="3-9.jpg" alt></p>
<p>随着深度的增加，输入空间（左上角所示）会在输入空间中的每个点处被扭曲成越来越细的单丝，只有一个与细丝正交的方向影响网络的响应。沿着这个方向，网络实际上对变化变得非常敏感。</p>
<h3 id="3-1-6-深度学习和机器学习有什么不同？">3.1.6 深度学习和机器学习有什么不同？</h3>
<p>​	<strong>机器学习</strong>：利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。机器学习的过程，就是训练数据去优化目标函数。</p>
<p>​	<strong>深度学习</strong>：是一种特殊的机器学习，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。</p>
<p>​	传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。而深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程，这也是深度学习在大数据时代受欢迎的一大原因。</p>
<p><img src="3.1.6.1.png" alt></p>
<p><img src="3-11.jpg" alt></p>
<h2 id="3-2-网络操作与计算">3.2 网络操作与计算</h2>
<h3 id="3-2-1-前向传播与反向传播？">3.2.1 前向传播与反向传播？</h3>
<p>神经网络的计算主要有两种：前向传播（foward propagation, FP）作用于每一层的输入，通过逐层计算得到输出结果；反向传播（backward propagation, BP）作用于网络的输出，通过计算梯度由深到浅更新网络参数。</p>
<p><strong>前向传播</strong></p>
<p><img src="3.2.1.1.png" alt></p>
<p>假设上一层结点  $ i,j,k,... $  等一些结点与本层的结点  $ w $  有连接，那么结点  $ w $  的值怎么算呢？就是通过上一层的  $ i,j,k,... $  等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如  $ReLu$ ， $sigmoid$  等函数，最后得到的结果就是本层结点  $ w $  的输出。</p>
<p>最终不断的通过这种方法一层层的运算，得到输出层结果。</p>
<p><strong>反向传播</strong></p>
<p><img src="3.2.1.2.png" alt></p>
<p>由于我们前向传播最终得到的结果，以分类为例，最终总是有误差的，那么怎么减少误差呢，当前应用广泛的一个算法就是梯度下降算法，但是求梯度就要求偏导数，下面以图中字母为例讲解一下：</p>
<p>设最终误差为  $ E $ 且输出层的激活函数为线性激活函数，对于输出那么  $ E $  对于输出节点  $ y_l $  的偏导数是  $ y_l - t_l $ ，其中  $ t_l $  是真实值， $ \frac{\partial y_l}{\partial z_l} $  是指上面提到的激活函数， $ z_l $  是上面提到的加权和，那么这一层的  $ E $  对于  $ z_l $  的偏导数为  $ \frac{\partial E}{\partial z_l} = \frac{\partial E}{\partial y_l} \frac{\partial y_l}{\partial z_l} $ 。同理，下一层也是这么计算，只不过  $ \frac{\partial E}{\partial y_k} $  计算方法变了，一直反向传播到输入层，最后有  $ \frac{\partial E}{\partial x_i} = \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial z_j} $ ，且  $ \frac{\partial z_j}{\partial x_i} = w_i j $ 。然后调整这些过程中的权值，再不断进行前向传播和反向传播的过程，最终得到一个比较好的结果。</p>
<h3 id="3-2-2-如何计算神经网络的输出？">3.2.2 如何计算神经网络的输出？</h3>
<p><img src="3.2.2.1.png" alt></p>
<p>如上图，输入层有三个节点，我们将其依次编号为 1、2、3；隐藏层的 4 个节点，编号依次为 4、5、6、7；最后输出层的两个节点编号为 8、9。比如，隐藏层的节点 4，它和输入层的三个节点 1、2、3 之间都有连接，其连接上的权重分别为是  $ w_{41}, w_{42}, w_{43} $ 。</p>
<p>为了计算节点 4 的输出值，我们必须先得到其所有上游节点（也就是节点 1、2、3）的输出值。节点 1、2、3 是输入层的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点 1、2、3 的输出值分别是  $ x_1, x_2, x_3 $ 。</p>
 $$
a_4 = \sigma(w^T \cdot a) = \sigma(w_{41}x_4 + w_{42}x_2 + w_{43}a_3 + w_{4b})
$$ 
<p>其中  $ w_{4b} $  是节点 4 的偏置项。</p>
<p>同样，我们可以继续计算出节点 5、6、7 的输出值  $ a_5, a_6, a_7 $ 。</p>
<p>计算输出层的节点 8 的输出值  $ y_1 $ ：</p>
 $$
y_1 = \sigma(w^T \cdot a) = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b})
$$ 
<p>其中  $ w_{8b} $  是节点 8 的偏置项。</p>
<p>同理，我们还可以计算出  $ y_2 $ 。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量  $ x_1, x_2, x_3, x_4 $  时，神经网络的输出向量  $ y_1, y_2 $  。这里我们也看到，输出向量的维度和输出层神经元个数相同。</p>
<h3 id="3-2-3-如何计算卷积神经网络输出值？">3.2.3 如何计算卷积神经网络输出值？</h3>
<p>假设有一个 5*5 的图像，使用一个 3*3 的 filter 进行卷积，想得到一个 3*3 的 Feature Map，如下所示：</p>
<p><img src="3.2.3.1.png" alt></p>
 $ x_{i,j} $  表示图像第   $ i $  行第  $ j $  列元素。 $ w_{m,n} $  表示 filter​ 第  $ m $  行第  $ n $  列权重。  $ w_b $  表示  $filter$  的偏置项。 表 $a_i,_j$ 示 feature map 第  $ i$  行第  $ j $  列元素。  $f$  表示激活函数，这里以 $ ReLU$  函数为例。
<p>卷积计算公式如下：</p>
 $$
a_{i,j} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b )
$$ 
<p>当步长为  $1$  时，计算 feature map 元素  $ a_{0,0} $  如下：</p>
 $$
a_{0,0} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{0+m, 0+n} + w_b )

= relu(w_{0,0} x_{0,0} + w_{0,1} x_{0,1} + w_{0,2} x_{0,2} + w_{1,0} x_{1,0} + \\w_{1,1} x_{1,1} + w_{1,2} x_{1,2} + w_{2,0} x_{2,0} + w_{2,1} x_{2,1} + w_{2,2} x_{2,2}) \\

= 1 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 1 \\

= 4
$$ 
<p>其计算过程图示如下：</p>
<p><img src="3.2.3.2.png" alt></p>
<p>以此类推，计算出全部的Feature Map。</p>
<p><img src="3.2.3.4.png" alt></p>
<p>当步幅为 2 时，Feature Map计算如下</p>
<p><img src="3.2.3.5.png" alt></p>
<p>注：图像大小、步幅和卷积后的Feature Map大小是有关系的。它们满足下面的关系：</p>
 $$
W_2 = (W_1 - F + 2P)/S + 1\\
H_2 = (H_1 - F + 2P)/S + 1
$$ 
<p>​	其中  $ W_2 $ ， 是卷积后 Feature Map 的宽度； $ W_1 $  是卷积前图像的宽度； $ F $  是 filter 的宽度； $ P $  是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈  $0$ ，如果  $P$  的值是  $1$ ，那么就补  $1$  圈  $0$ ； $S$  是步幅； $ H_2 $  卷积后 Feature Map 的高度； $ H_1 $  是卷积前图像的宽度。</p>
<p>​	举例：假设图像宽度  $ W_1 = 5 $ ，filter 宽度  $ F=3 $ ，Zero Padding  $ P=0 $ ，步幅  $ S=2 $ ， $ Z $  则</p>
 $$
W_2 = (W_1 - F + 2P)/S + 1

= (5-3+0)/2 + 1

= 2
$$ 
<p>​	说明 Feature Map 宽度是2。同样，我们也可以计算出 Feature Map 高度也是 2。</p>
<p>如果卷积前的图像深度为  $ D $ ，那么相应的 filter 的深度也必须为  $ D $ 。深度大于 1 的卷积计算公式：</p>
 $$
a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)
$$ 
<p>​	其中， $ D $  是深度； $ F $  是 filter 的大小； $ w_{d,m,n} $  表示 filter 的第  $ d $  层第  $ m $  行第  $ n $  列权重； $ a_{d,i,j} $  表示 feature map 的第  $ d $  层第  $ i $  行第  $ j $  列像素；其它的符号含义前面相同，不再赘述。</p>
<p>​	每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。下面的图示显示了包含两个 filter 的卷积层的计算。 $7*7*3$  输入，经过两个  $3*3*3$  filter 的卷积(步幅为  $2$ )，得到了  $3*3*2$  的输出。图中的 Zero padding 是  $1$ ，也就是在输入元素的周围补了一圈  $0$ 。</p>
<p><img src="3.2.3.6.png" alt></p>
<p>​	以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个  $ 3 * 3 * 3 $  的 fitler 的卷积层来说，其参数数量仅有  $ (3 * 3 * 3+1) * 2 = 56 $  个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p>
<h3 id="3-2-4-如何计算-Pooling-层输出值输出值？">3.2.4 如何计算 Pooling 层输出值输出值？</h3>
<p>​	Pooling 层主要的作用是下采样，通过去掉 Feature Map 中不重要的样本，进一步减少参数数量。Pooling 的方法很多，最常用的是 Max Pooling。Max Pooling 实际上就是在 n*n 的样本中取最大值，作为采样后的样本值。下图是 2*2 max pooling：</p>
<p><img src="3.2.4.1.png" alt></p>
<p>​	除了 Max Pooing 之外，常用的还有 Average Pooling ——取各样本的平均值。<br>
​	对于深度为  $ D $  的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为  $ D $ 。</p>
<h3 id="3-2-5-实例理解反向传播">3.2.5 实例理解反向传播</h3>
<p>​	一个典型的三层神经网络如下所示：</p>
<p><img src="3.2.5.1.png" alt></p>
<p>​	其中 Layer  $ L_1 $  是输入层，Layer  $ L_2 $  是隐含层，Layer  $ L_3 $  是输出层。</p>
<p>​	假设输入数据集为  $ D={x_1, x_2, ..., x_n} $ ，输出数据集为  $ y_1, y_2, ..., y_n $ 。</p>
<p>​	如果输入和输出是一样，即为自编码模型。如果原始数据经过映射，会得到不同于输入的输出。</p>
<p>假设有如下的网络层：</p>
<p><img src="3.2.5.2.png" alt></p>
<p>​	输入层包含神经元  $ i_1, i_2 $ ，偏置  $ b_1 $ ；隐含层包含神经元  $ h_1, h_2 $ ，偏置  $ b_2 $ ，输出层为   $ o_1, o_2 $ ， $ w_i $  为层与层之间连接的权重，激活函数为  $sigmoid$  函数。对以上参数取初始值，如下图所示：</p>
<p><img src="3.2.5.3.png" alt></p>
<p>其中：</p>
<ul>
<li>输入数据  $ i1=0.05, i2 = 0.10 $</li>
<li>输出数据  $ o1=0.01, o2=0.99 $ ;</li>
<li>初始权重  $ w1=0.15, w2=0.20, w3=0.25,w4=0.30, w5=0.40, w6=0.45, w7=0.50, w8=0.55 $</li>
<li>目标：给出输入数据  $ i1,i2 $  (  $0.05$ 和 $0.10$  )，使输出尽可能与原始输出  $ o1,o2 $ ，(  $0.01$ 和 $0.99$ )接近。</li>
</ul>
<p><strong>前向传播</strong></p>
<ol>
<li>输入层 --&gt; 输出层</li>
</ol>
<p>计算神经元  $ h1 $  的输入加权和：</p>
 $$
net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1\\

net_{h1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775
$$ 
<p>神经元  $ h1 $  的输出  $ o1 $  ：（此处用到激活函数为 sigmoid 函数）：</p>
 $$
out_{h1} = \frac{1}{1 + e^{-net_{h1}}} = \frac{1}{1 + e^{-0.3775}} = 0.593269992
$$ 
<p>同理，可计算出神经元  $ h2 $  的输出  $ o1 $ ：</p>
 $$
out_{h2} = 0.596884378
$$ 
<ol start="2">
<li>隐含层–&gt;输出层：</li>
</ol>
<p>计算输出层神经元  $ o1 $  和  $ o2 $  的值：</p>
 $$
net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1
$$ 
 $$
net_{o1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967
$$ 
 $$
out_{o1} = \frac{1}{1 + e^{-net_{o1}}} = \frac{1}{1 + e^{1.105905967}} = 0.75136079
$$ 
<p>这样前向传播的过程就结束了，我们得到输出值为  $ [0.75136079 ,  0.772928465] $ ，与实际值  $ [0.01 , 0.99] $  相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。</p>
<p>**反向传播 **</p>
<p>​	1.计算总误差</p>
<p>总误差：(这里使用Square Error)</p>
 $$
E_{total} = \sum \frac{1}{2}(target - output)^2
$$ 
<p>但是有两个输出，所以分别计算  $ o1 $  和  $ o2 $  的误差，总误差为两者之和：</p>
 $E_{o1} = \frac{1}{2}(target_{o1} - out_{o1})^2 
= \frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083$ .
 $E_{o2} = 0.023560026$ .
 $E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109$ .
<p>​	2.隐含层 --&gt; 输出层的权值更新：</p>
<p>以权重参数  $ w5 $  为例，如果我们想知道  $ w5 $  对整体误差产生了多少影响，可以用整体误差对  $ w5 $  求偏导求出：（链式法则）</p>
 $$
\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial w5}
$$ 
<p>下面的图可以更直观的看清楚误差是怎样反向传播的：</p>
<p><img src="3.2.5.4.png" alt></p>
<h3 id="3-2-6-神经网络更“深”有什么意义？">3.2.6 神经网络更“深”有什么意义？</h3>
<p>前提：在一定范围内。</p>
<ul>
<li>在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。</li>
<li>隐藏层增加则意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。</li>
</ul>
<h2 id="3-3-超参数">3.3 超参数</h2>
<h3 id="3-3-1-什么是超参数？">3.3.1 什么是超参数？</h3>
<p>​	<strong>超参数</strong> : 在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</p>
<p>​	超参数通常存在于：</p>
<pre><code>1.  定义关于模型的更高层次的概念，如复杂性或学习能力。
2.  不能直接从标准模型培训过程中的数据中学习，需要预先定义。
3.  可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定
</code></pre>
<p>​	超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。</p>
<h3 id="3-3-2-如何寻找超参数的最优值？">3.3.2 如何寻找超参数的最优值？</h3>
<p>​	在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：</p>
<ol>
<li>
<p>猜测和检查：根据经验或直觉，选择参数，一直迭代。</p>
</li>
<li>
<p>网格搜索：让计算机尝试在一定范围内均匀分布的一组值。</p>
</li>
<li>
<p>随机搜索：让计算机随机挑选一组值。</p>
</li>
<li>
<p>贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。</p>
</li>
<li>
<p>MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。</p>
</li>
<li>
<p>最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。</p>
</li>
</ol>
<h3 id="3-3-3-超参数搜索一般过程？">3.3.3 超参数搜索一般过程？</h3>
<p>超参数搜索一般过程：</p>
<ol>
<li>将数据集划分成训练集、验证集及测试集。</li>
<li>在训练集上根据模型的性能指标对模型参数进行优化。</li>
<li>在验证集上根据模型的性能指标对模型的超参数进行搜索。</li>
<li>步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。</li>
</ol>
<p>其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索。</p>
<h2 id="3-4-激活函数">3.4 激活函数</h2>
<h3 id="3-4-1-为什么需要非线性激活函数？">3.4.1 为什么需要非线性激活函数？</h3>
<p><strong>为什么需要激活函数？</strong></p>
<ol>
<li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li>
<li>激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</li>
<li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li>
</ol>
<p><strong>为什么激活函数需要非线性函数？</strong></p>
<ol>
<li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。</li>
<li>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。</li>
</ol>
<h3 id="3-4-2-常见的激活函数及图像">3.4.2 常见的激活函数及图像</h3>
<ol>
<li>
<p>sigmoid 激活函数</p>
<p>函数的定义为： $ f(x) = \frac{1}{1 + e^{-x}} $ ，其值域为  $ (0,1) $ 。</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="3-26.png" alt></p>
<ol start="2">
<li>
<p>tanh激活函数</p>
<p>函数的定义为： $ f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $ ，值域为  $ (-1,1) $ 。</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="3-27.png" alt></p>
<ol start="3">
<li>
<p>Relu激活函数</p>
<p>函数的定义为： $ f(x) = max(0, x) $   ，值域为  $ [0,+∞) $ ；</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="3-28.png" alt></p>
<ol start="4">
<li>
<p>Leak Relu 激活函数</p>
<p>函数定义为：  $ f(x) =  \left\{
   \begin{aligned}
   ax, \quad x<0 \\ x, \quad x>0
   \end{aligned}
   \right. $ ，值域为  $ (-∞,+∞) $ 。</0></p>
<p>图像如下（ $ a = 0.5 $ ）：</p>
</li>
</ol>
<p><img src="3-29.png" alt></p>
<ol start="5">
<li>
<p>SoftPlus 激活函数</p>
<p>函数的定义为： $ f(x) = ln( 1 + e^x) $ ，值域为  $ (0,+∞) $ 。</p>
<p>函数图像如下:</p>
</li>
</ol>
<p><img src="3-30.png" alt></p>
<ol start="6">
<li>
<p>softmax 函数</p>
<p>函数定义为：  $ \sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} $ 。</p>
<p>Softmax 多用于多分类神经网络输出。</p>
</li>
</ol>
<h3 id="3-4-3-常见激活函数的导数计算？">3.4.3 常见激活函数的导数计算？</h3>
<p>对常见激活函数，导数计算如下：</p>
<table>
<thead>
<tr>
<th>原函数</th>
<th>函数表达式</th>
<th>导数</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid激活函数</td>
<td>$f(x)=\frac{1}{1+e^{-x}}$</td>
<td>$f^{'}(x)=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))$</td>
<td>当 $x=10$ ,或 $x=-10​$ ， $f^{'}(x) \approx0​$ ,当 $x=0​ {% raw%}$$f^{'}(x) =0.25​${% endraw %}</td>
</tr>
<tr>
<td>Tanh激活函数</td>
<td>{% raw%}$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}${% endraw %}</td>
<td>{% raw%}$f^{'}(x)=-(tanh(x))^2${% endraw %}</td>
<td>当 {% raw%}$x=10${% endraw %} ,或 {% raw%}$x=-10${% endraw %} ， {% raw%}$f^{'}(x) \approx0${% endraw %} ,当 {% raw%}$x=0$${% endraw %} f^{`}(x) =1$</td>
</tr>
<tr>
<td>Relu激活函数</td>
<td>$f(x)=max(0,x)$</td>
<td>$c(u)=\begin{cases} 0,x<0 \\ 1,x>0 \\ undefined,x=0\end{cases}$</0></td>
<td>通常 $x=0$ 时，给定其导数为1和0</td>
</tr>
</tbody>
</table>
<h3 id="3-4-4-激活函数有哪些性质？">3.4.4 激活函数有哪些性质？</h3>
<ol>
<li>非线性： 当激活函数是非线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即  $ f(x)=x $ ，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；</li>
<li>可微性： 当优化方法是基于梯度的时候，就体现了该性质；</li>
<li>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；</li>
<li>
$ f(x)≈x $ ： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；
</li>
<li>输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。</li>
</ol>
<h3 id="3-4-5-如何选择激活函数？">3.4.5 如何选择激活函数？</h3>
<p>​	选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<p>以下是常见的选择情况：</p>
<ol>
<li>如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。</li>
<li>如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。</li>
<li>sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</li>
<li>tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。</li>
<li>ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。</li>
<li>如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。</li>
</ol>
<h3 id="3-4-6-使用-ReLu-激活函数的优点？">3.4.6 使用 ReLu 激活函数的优点？</h3>
<ol>
<li>在区间变动很大的情况下，ReLu 激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。</li>
<li>sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。</li>
<li>需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。</li>
</ol>
<h3 id="3-4-7-什么时候可以用线性激活函数？">3.4.7 什么时候可以用线性激活函数？</h3>
<ol>
<li>输出层，大多使用线性激活函数。</li>
<li>在隐含层可能会使用一些线性激活函数。</li>
<li>一般用到的线性激活函数很少。</li>
</ol>
<h3 id="3-4-8-怎样理解-Relu（-0-时）是非线性激活函数？">3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？</h3>
<p>Relu 激活函数图像如下：</p>
<p><img src="3-32.png" alt></p>
<p>根据图像可看出具有如下特点：</p>
<ol>
<li>
<p>单侧抑制；</p>
</li>
<li>
<p>相对宽阔的兴奋边界；</p>
</li>
<li>
<p>稀疏激活性；</p>
<p>ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。</p>
<p>因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p>
<p><strong>稀疏激活性</strong>：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当  $ x<0 $ 时，relu 硬饱和，而当 x>0 $  时，则不存在饱和问题。ReLU 能够在  $ x>0 $  时保持梯度不衰减，从而缓解梯度消失问题。</0></p>
</li>
</ol>
<h3 id="3-4-9-Softmax-定义及作用">3.4.9 Softmax 定义及作用</h3>
<p>Softmax 是一种形如下式的函数：</p>
 $$
P(i) = \frac{exp(\theta_i^T x)}{\sum_{k=1}^{K} exp(\theta_i^T x)}
$$ 
<p>​	其中， $ \theta_i $  和  $ x $  是列向量， $ \theta_i^T x $  可能被换成函数关于  $ x $  的函数  $ f_i(x) $</p>
<p>​	通过 softmax 函数，可以使得  $ P(i) $  的范围在  $ [0,1] $  之间。在回归和分类问题中，通常  $ \theta $  是待求参数，通过寻找使得  $ P(i) $  最大的  $ \theta_i $  作为最佳参数。</p>
<p>​	但是，使得范围在  $ [0,1] $   之间的方法有很多，为啥要在前面加上以  $ e $  的幂函数的形式呢？参考 logistic 函数：</p>
 $$
P(i) = \frac{1}{1+exp(-\theta_i^T x)}
$$ 
<p>​	这个函数的作用就是使得  $ P(i) $  在负无穷到 0 的区间趋向于 0， 在 0 到正无穷的区间趋向 1,。同样 softmax 函数加入了  $ e $  的幂函数正是为了两极化：正样本的结果将趋近于 1，而负样本的结果趋近于 0。这样为多类别提供了方便（可以把  $ P(i) $  看做是样本属于类别的概率）。可以说，Softmax 函数是 logistic 函数的一种泛化。</p>
<p>​	softmax 函数可以把它的输入，通常被称为 logits 或者 logit scores，处理成 0 到 1 之间，并且能够把输出归一化到和为 1。这意味着 softmax 函数与分类的概率分布等价。它是一个网络预测多酚类问题的最佳输出激活函数。</p>
<h3 id="3-4-10-Softmax-函数如何应用于多分类？">3.4.10 Softmax 函数如何应用于多分类？</h3>
<p>​	softmax 用于多分类过程中，它将多个神经元的输出，映射到  $ (0,1) $  区间内，可以看成概率来理解，从而来进行多分类！</p>
<p>​	假设我们有一个数组， $ V_i $  表示  $ V $   中的第  $ i $  个元素，那么这个元素的 softmax 值就是</p>
 $$
S_i = \frac{e^{V_i}}{\sum_j e^{V_j}}
$$ 
<p>​	从下图看，神经网络中包含了输入层，然后通过两个特征层处理，最后通过 softmax 分析器就能得到不同条件下的概率，这里需要分成三个类别，最终会得到  $ y=0, y=1, y=2 $  的概率值。</p>
<p><img src="3.4.9.1.png" alt></p>
<p>继续看下面的图，三个输入通过 softmax 后得到一个数组  $ [0.05 , 0.10 , 0.85] $ ，这就是 soft 的功能。</p>
<p><img src="3.4.9.2.png" alt></p>
<p>更形象的映射过程如下图所示：</p>
<p><img src="3.4.9.3.png" alt="****"></p>
<p>​	softmax 直白来说就是将原来输出是  $ 3,1,-3 $  通过 softmax 函数一作用，就映射成为  $ (0,1) $  的值，而这些值的累和为  $ 1 $ （满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！</p>
<h3 id="3-4-11-交叉熵代价函数定义及其求导推导">3.4.11 交叉熵代价函数定义及其求导推导</h3>
<p>(<strong>贡献者：黄钦建－华南理工大学</strong>)</p>
<p>​	神经元的输出就是 a = σ(z)，其中 $z=\sum w_{j}i_{j}+b$ 是输⼊的带权和。</p>
 $C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)]$ 
<p>​	其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的， y 是对应的⽬标输出。</p>
<p>​	表达式是否解决学习缓慢的问题并不明显。实际上，甚⾄将这个定义看做是代价函数也不是显⽽易⻅的！在解决学习缓慢前，我们来看看交叉熵为何能够解释成⼀个代价函数。</p>
<p>​	将交叉熵看做是代价函数有两点原因。</p>
<p>​	第⼀，它是⾮负的， C &gt; 0。可以看出：式子中的求和中的所有独⽴的项都是负数的，因为对数函数的定义域是 (0，1)，并且求和前⾯有⼀个负号，所以结果是非负。</p>
<p>​	第⼆，如果对于所有的训练输⼊ x，神经元实际的输出接近⽬标值，那么交叉熵将接近 0。</p>
<p>​	假设在这个例⼦中， y = 0 ⽽ a ≈ 0。这是我们想到得到的结果。我们看到公式中第⼀个项就消去了，因为 y = 0，⽽第⼆项实际上就是 − ln(1 − a) ≈ 0。反之， y = 1 ⽽ a ≈ 1。所以在实际输出和⽬标输出之间的差距越⼩，最终的交叉熵的值就越低了。（这里假设输出结果不是0，就是1，实际分类也是这样的）</p>
<p>​	综上所述，交叉熵是⾮负的，在神经元达到很好的正确率的时候会接近 0。这些其实就是我们想要的代价函数的特性。其实这些特性也是⼆次代价函数具备的。所以，交叉熵就是很好的选择了。但是交叉熵代价函数有⼀个⽐⼆次代价函数更好的特性就是它避免了学习速度下降的问题。为了弄清楚这个情况，我们来算算交叉熵函数关于权重的偏导数。我们将 $a={\varsigma}(z)$ 代⼊到 公式中应⽤两次链式法则，得到：</p>
 $\begin{eqnarray}\frac{\partial C}{\partial w_{j}}&=&-\frac{1}{n}\sum \frac{\partial }{\partial w_{j}}[ylna+(1-y)ln(1-a)]\\&=&-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]*\frac{\partial a}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})*\frac{\partial a}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)})\frac{\partial \varsigma(z)}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)}){\varsigma}'(z)x_{j}\end{eqnarray}$ 
<p>​	根据 $\varsigma(z)=\frac{1}{1+e^{-z}}$  的定义，和⼀些运算，我们可以得到  ${\varsigma}'(z)=\varsigma(z)(1-\varsigma(z))$ 。化简后可得：</p>
 $\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum x_{j}({\varsigma}(z)-y)$ 
<p>​	这是⼀个优美的公式。它告诉我们权重学习的速度受到 $\varsigma(z)-y$ ，也就是输出中的误差的控制。更⼤的误差，更快的学习速度。这是我们直觉上期待的结果。特别地，这个代价函数还避免了像在⼆次代价函数中类似⽅程中 ${\varsigma}'(z)$ 导致的学习缓慢。当我们使⽤交叉熵的时候， ${\varsigma}'(z)$ 被约掉了，所以我们不再需要关⼼它是不是变得很⼩。这种约除就是交叉熵带来的特效。实际上，这也并不是⾮常奇迹的事情。我们在后⾯可以看到，交叉熵其实只是满⾜这种特性的⼀种选择罢了。</p>
<p>​	根据类似的⽅法，我们可以计算出关于偏置的偏导数。我这⾥不再给出详细的过程，你可以轻易验证得到：</p>
 $\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\varsigma}(z)-y)$ 
<p>​	再⼀次, 这避免了⼆次代价函数中类似 ${\varsigma}'(z)$ 项导致的学习缓慢。</p>
<h3 id="3-4-12-为什么Tanh收敛速度比Sigmoid快？">3.4.12 为什么Tanh收敛速度比Sigmoid快？</h3>
<p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>首先看如下两个函数的求导：</p>
 $tanh^{,}(x)=1-tanh(x)^{2}\in (0,1)$ 
 $s^{,}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}]$ 
<p>由上面两个公式可知tanh(x)梯度消失的问题比sigmoid轻，所以Tanh收敛速度比Sigmoid快。</p>
<p><strong>（贡献者：郜泉凯 - 华南理工大学）</strong></p>
<p>注：梯度消失（gradient vanishing）或者爆炸（gradient explosion）是激活函数<strong>以及当前权重</strong>耦合产生的综合结果：<br>
​   设任意激活函数为 $\sigma(\cdot)$ ，k+1层网络输出为 $f_{k+1}=\sigma(Wf_k)$ ，求导得到 $\frac {\partial h_{t+1}}{\partial h_t}=diag(\sigma'(Wh_t))W$ 。可见求导结果同时会受到权重 $W$ 和激活函数的导数 $\sigma'(\cdot)$ 的影响，以sigmoid函数 $\sigma(X)=\frac {1}{1+e^{-x}}$ 为例，其导数为 $\sigma'(x)=\frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}})$ ，其值恒大于零小于1，用链式法则求梯度回传时连续相乘使得结果趋于0，但是如果权重 $W$ 是较大的数值，使得 $\frac {\partial f_{t+1}}{\partial f_t}$ 相乘结果大于1，则梯度回传时连续相乘则不会发生梯度消失。<br>
综上，在讨论激活函数收敛速度或与梯度消失或者爆炸相关时，应同时考虑当前权重 $W$ 数值的影响。</p>
<p>3.4.13</p>
<h3 id="3-4-12-内聚外斥-Center-Loss">3.4.12 内聚外斥 - Center Loss</h3>
<p><strong>（贡献者：李世轩－加州大学伯克利分校）</strong></p>
<p>在计算机视觉任务中, 由于其简易性, 良好的表现, 与对分类任务的概率性理解, Cross Entropy Loss (交叉熵代价) + Softmax 组合被广泛应用于以分类任务为代表的任务中. 在此应用下, 我们可将其学习过程进一步理解为: 更相似(同类/同物体)的图像在特征域中拥有“更近的距离”, 相反则”距离更远“. 换而言之, 我们可以进一步理解为其学习了一种低类内距离(Intra-class Distance)与高类间距离(Inter-class Distance)的特征判别模型. 在此Center Loss则可以高效的计算出这种具判别性的特征. 不同于传统的Softmax Loss, Center Loss通过学习“特征中心”从而最小化其类内距离. 其表达形式如下:</p>
 $L_{C} = \frac{1}{2}\sum^{m}_{i=1}||x_{i}-c_{y_{i}}||^{2}_{2}$ 
<p>其中 $x_{i}$ 表示FCN(全连接层)之前的特征,  $c_{y_{i}}$ 表示第 $y_{i} $ 个类别的特征中心,  $m$ 表示mini-batch的大小. 我们很清楚的看到 $L_{C}$ 的终极目标为最小化每个特征与其特征中心的方差, 即最小化类内距离. 其迭代公式为:</p>
 $\frac{\partial L_{C}}{\partial x_{i}}=x_{i}-c_{y_{i}}$ 
 $\Delta{c_{j}} = \frac{\sum^{m}_{i=1}\delta(y_{i}=j)\cdot(c_{j}-x_{i})}{1+\sum^{m}_{i=1}\delta(y_{i}=j)}$ 
<p>其中 $ \delta(condition)=\left\{
\begin{array}{rcl}
1       &      & {condition\ is\ True}\\
0     &      & {otherwise}\\ \end{array} \right.$</p>
<p>结合Softmax, 我们可以搭配二者使用, 适当平衡这两种监督信号. 在Softmax拉开类间距离的同时, 利用Center Loss最小化类内距离. 例如:</p>
 $\begin{eqnarray}L & = & L_{S} + \lambda L_{C} \\ &=& -\sum^{m}_{i=1}log\frac{e^{W_{y}^{T}x_{i}+b_{y_{i}}}}{\sum^{m}_{i=1}e^{W^{T}_{j}x_{i}+b_{j}}} + \frac{\lambda}{2}\sum^{m}_{i=1}||x_{i}-c_{y_{i}}||^{2}_{2}\\ \end{eqnarray}$ 
<p>即便如此, Center Loss仍有它的不足之处: 其特征中心为存储在网络模型之外的额外参数, 不能与模型参数一同优化. 这些额外参数将与记录每一步特征变化的自动回归均值估计(autoregressive mean estimator)进行更迭. 当需要学习的类别数量较大时, mini-batch可能无力提供足够的样本进行均值估计. 若此Center Loss将需要平衡两种监督损失来以确定更迭, 其过程需要一个对平衡超参数的搜索过程, 使得其择值消耗昂贵.</p>
<h2 id="3-5-Batch-Size">3.5 Batch_Size</h2>
<h3 id="3-5-1-为什么需要-Batch-Size？">3.5.1 为什么需要 Batch_Size？</h3>
<p>Batch的选择，首先决定的是下降的方向。</p>
<p>如果数据集比较小，可采用全数据集的形式，好处是：</p>
<ol>
<li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。</li>
<li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。</li>
</ol>
<p>对于更大的数据集，假如采用全数据集的形式，坏处是：</p>
<ol>
<li>随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。</li>
<li>以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。</li>
</ol>
<h3 id="3-5-2-Batch-Size-值的选择">3.5.2 Batch_Size 值的选择</h3>
<p>​	假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。</p>
<p>​	既然 Batch_Size 为全数据集或者Batch_Size = 1都有各自缺点，可不可以选择一个适中的Batch_Size值呢？</p>
<p>​	此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。</p>
<h3 id="3-5-3-在合理范围内，增大Batch-Size有何好处？">3.5.3 在合理范围内，增大Batch_Size有何好处？</h3>
<ol>
<li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li>
<li>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。</li>
</ol>
<h3 id="3-5-4-盲目增大-Batch-Size-有何坏处？">3.5.4 盲目增大 Batch_Size 有何坏处？</h3>
<ol>
<li>内存利用率提高了，但是内存容量可能撑不住了。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<h3 id="3-5-5-调节-Batch-Size-对训练效果影响到底如何？">3.5.5 调节 Batch_Size 对训练效果影响到底如何？</h3>
<ol>
<li>Batch_Size 太小，模型表现效果极其糟糕(error飙升)。</li>
<li>随着 Batch_Size 增大，处理相同数据量的速度越快。</li>
<li>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</li>
<li>由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。</li>
<li>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。</li>
</ol>
<h2 id="3-6-归一化">3.6 归一化</h2>
<h3 id="3-6-1-归一化含义？">3.6.1 归一化含义？</h3>
<ol>
<li>
<p>归纳统一样本的统计分布性。归一化在  $ 0-1$  之间是统计的概率分布，归一化在 $ -1--+1$  之间是统计的坐标分布。</p>
</li>
<li>
<p>无论是为了建模还是为了计算，首先基本度量单位要同一，神经网络是以样本在事件中的统计分别几率来进行训练（概率计算）和预测，且 sigmoid 函数的取值是 0 到 1 之间的，网络最后一个节点的输出也是如此，所以经常要对样本的输出归一化处理。</p>
</li>
<li>
<p>归一化是统一在  $ 0-1 $  之间的统计概率分布，当所有样本的输入信号都为正值时，与第一隐含层神经元相连的权值只能同时增加或减小，从而导致学习速度很慢。</p>
</li>
<li>
<p>另外在数据中常存在奇异样本数据，奇异样本数据存在所引起的网络训练时间增加，并可能引起网络无法收敛。为了避免出现这种情况及后面数据处理的方便，加快网络学习速度，可以对输入信号进行归一化，使得所有样本的输入信号其均值接近于 0 或与其均方差相比很小。</p>
</li>
</ol>
<h3 id="3-6-2-为什么要归一化？">3.6.2 为什么要归一化？</h3>
<ol>
<li>为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。</li>
<li>为了程序运行时收敛加快。</li>
<li>同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。</li>
<li>避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。</li>
<li>保证输出数据中数值小的不被吞食。</li>
</ol>
<h3 id="3-6-3-为什么归一化能提高求解最优解速度？">3.6.3 为什么归一化能提高求解最优解速度？</h3>
<p><img src="3.6.3.1.png" alt></p>
<p>​	上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。</p>
<p>​	当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。</p>
<p>​	因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。</p>
<h3 id="3-6-4-3D-图解未归一化">3.6.4 3D 图解未归一化</h3>
<p>例子：</p>
<p>​	假设  $ w1 $  的范围在  $ [-10, 10] $ ，而  $ w2 $  的范围在  $ [-100, 100] $ ，梯度每次都前进 1 单位，那么在  $ w1 $  方向上每次相当于前进了  $ 1/20 $ ，而在  $ w2 $  上只相当于  $ 1/200 $ ！某种意义上来说，在  $ w2 $  上前进的步长更小一些,而  $ w1 $  在搜索过程中会比  $ w2 $  “走”得更快。</p>
<p>​	这样会导致，在搜索过程中更偏向于  $ w1 $  的方向。走出了“L”形状，或者成为“之”字形。</p>
<p><img src="3-37.png" alt></p>
<h3 id="3-6-5-归一化有哪些类型？">3.6.5 归一化有哪些类型？</h3>
<ol>
<li>线性归一化</li>
</ol>
 $$
x^{\prime} = \frac{x-min(x)}{max(x) - min(x)}
$$ 
<p>​	适用范围：比较适用在数值比较集中的情况。</p>
<p>​	缺点：如果 max 和 min 不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。</p>
<ol start="2">
<li>标准差标准化</li>
</ol>
 $$
x^{\prime} = \frac{x-\mu}{\sigma}
$$ 
<p>​	含义：经过处理的数据符合标准正态分布，即均值为 0，标准差为 1 其中  $ \mu $  为所有样本数据的均值， $ \sigma $  为所有样本数据的标准差。</p>
<ol start="3">
<li>
<p>非线性归一化</p>
<p>适用范围：经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括  $ log $ 、指数，正切等。</p>
</li>
</ol>
<h3 id="3-6-6-局部响应归一化作用">3.6.6 局部响应归一化作用</h3>
<p>​	LRN 是一种提高深度学习准确度的技术方法。LRN 一般是在激活、池化函数后的一种方法。</p>
<p>​	在 ALexNet 中，提出了 LRN 层，对局部神经元的活动创建竞争机制，使其中响应比较大对值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p>
<h3 id="3-6-7-理解局部响应归一化">3.6.7 理解局部响应归一化</h3>
<p>​	局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象（侧抑制），其公式如下：</p>
 $$
b_{x,y}^i = a_{x,y}^i / (k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2 )^\beta
$$ 
<p>其中，</p>
<ol>
<li>
$ a $ ：表示卷积层（包括卷积操作和池化操作）后的输出结果，是一个四维数组[batch,height,width,channel]。
</li>
</ol>
<ul>
<li>batch：批次数(每一批为一张图片)。</li>
<li>height：图片高度。</li>
<li>width：图片宽度。</li>
<li>channel：通道数。可以理解成一批图片中的某一个图片经过卷积操作后输出的神经元个数，或理解为处理后的图片深度。</li>
</ul>
<ol start="2">
<li>
$ a_{x,y}^i $  表示在这个输出结构中的一个位置  $ [a,b,c,d] $ ，可以理解成在某一张图中的某一个通道下的某个高度和某个宽度位置的点，即第  $ a $  张图的第  $ d $  个通道下的高度为b宽度为c的点。
</li>
<li>
$ N $ ：论文公式中的  $ N $  表示通道数 (channel)。
</li>
<li>
$ a $ ， $ n/2 $ ，  $ k $  分别表示函数中的 input,depth_radius,bias。参数  $ k, n, \alpha, \beta $  都是超参数，一般设置  $ k=2, n=5, \alpha=1*e-4, \beta=0.75 $ 
</li>
<li>
$ \sum $ ： $ \sum $  叠加的方向是沿着通道方向的，即每个点值的平方和是沿着  $ a $  中的第 3 维 channel 方向的，也就是一个点同方向的前面  $ n/2 $  个通道（最小为第  $ 0 $  个通道）和后  $ n/2 $  个通道（最大为第  $ d-1 $  个通道）的点的平方和(共  $ n+1 $  个点)。而函数的英文注解中也说明了把 input 当成是  $ d $  个 3 维的矩阵，说白了就是把 input 的通道数当作 3 维矩阵的个数，叠加的方向也是在通道方向。 
</li>
</ol>
<p>简单的示意图如下：</p>
<p><img src="3.6.7.1.png" alt></p>
<h3 id="3-6-8-什么是批归一化（Batch-Normalization）">3.6.8 什么是批归一化（Batch Normalization）</h3>
<p>​	以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过  $ \sigma(WX+b) $  这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。</p>
<p>​	这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。</p>
<h3 id="3-6-9-批归一化（BN）算法的优点">3.6.9 批归一化（BN）算法的优点</h3>
<p>下面我们来说一下BN算法的优点：</p>
<ol>
<li>减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数；</li>
<li>减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛；</li>
<li>可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在)</li>
<li>破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。</li>
<li>减少梯度消失，加快收敛速度，提高训练精度。</li>
</ol>
<h3 id="3-6-10-批归一化（BN）算法流程">3.6.10 批归一化（BN）算法流程</h3>
<p>下面给出 BN 算法在训练时的过程</p>
<p>输入：上一层输出结果  $ X = {x_1, x_2, ..., x_m} $ ，学习参数  $ \gamma, \beta $</p>
<p>算法流程：</p>
<ol>
<li>计算上一层输出数据的均值</li>
</ol>
 $$
\mu_{\beta} = \frac{1}{m} \sum_{i=1}^m(x_i)
$$ 
<p>其中， $ m $  是此次训练样本 batch 的大小。</p>
<ol start="2">
<li>计算上一层输出数据的标准差</li>
</ol>
 $$
\sigma_{\beta}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\beta})^2
$$ 
<ol start="3">
<li>归一化处理，得到</li>
</ol>
 $$
\hat x_i = \frac{x_i + \mu_{\beta}}{\sqrt{\sigma_{\beta}^2} + \epsilon}
$$ 
<p>其中  $ \epsilon $  是为了避免分母为 0 而加进去的接近于 0 的很小值</p>
<ol start="4">
<li>重构，对经过上面归一化处理得到的数据进行重构，得到</li>
</ol>
 $$
y_i = \gamma \hat x_i + \beta
$$ 
<p>其中， $ \gamma, \beta $  为可学习参数。</p>
<p>注：上述是 BN 训练时的过程，但是当在投入使用时，往往只是输入一个样本，没有所谓的均值  $ \mu_{\beta} $  和标准差  $ \sigma_{\beta}^2 $ 。此时，均值  $ \mu_{\beta} $  是计算所有 batch  $ \mu_{\beta} $  值的平均值得到，标准差  $ \sigma_{\beta}^2 $  采用每个batch  $ \sigma_{\beta}^2 $   的无偏估计得到。</p>
<h3 id="3-6-11-批归一化和群组归一化比较">3.6.11 批归一化和群组归一化比较</h3>
<table>
<thead>
<tr>
<th>名称</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>批量归一化（Batch Normalization，以下简称 BN）</td>
<td style="text-align:left">可让各种网络并行训练。但是，批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的 BN。</td>
</tr>
<tr>
<td>群组归一化 Group Normalization (简称 GN)</td>
<td style="text-align:left">GN 将通道分成组，并在每组内计算归一化的均值和方差。GN 的计算与批量大小无关，并且其准确度在各种批量大小下都很稳定。</td>
</tr>
<tr>
<td>比较</td>
<td style="text-align:left">在 ImageNet 上训练的 ResNet-50上，GN 使用批量大小为 2 时的错误率比 BN 的错误率低 10.6％ ;当使用典型的批量时，GN 与 BN 相当，并且优于其他标归一化变体。而且，GN 可以自然地从预训练迁移到微调。在进行 COCO 中的目标检测和分割以及 Kinetics 中的视频分类比赛中，GN 可以胜过其竞争对手，表明 GN 可以在各种任务中有效地取代强大的 BN。</td>
</tr>
</tbody>
</table>
<h3 id="3-6-12-Weight-Normalization和Batch-Normalization比较">3.6.12 Weight Normalization和Batch Normalization比较</h3>
<p>​	Weight Normalization 和 Batch Normalization 都属于参数重写（Reparameterization）的方法，只是采用的方式不同。</p>
<p>​	Weight Normalization 是对网络权值 $  W $  进行 normalization，因此也称为 Weight Normalization；</p>
<p>​	Batch Normalization 是对网络某一层输入数据进行 normalization。</p>
<p>​	Weight Normalization相比Batch Normalization有以下三点优势：</p>
<ol>
<li>
<p>Weight Normalization 通过重写深度学习网络的权重W的方式来加速深度学习网络参数收敛，没有引入 minbatch 的依赖，适用于 RNN（LSTM）网络（Batch Normalization 不能直接用于RNN，进行 normalization 操作，原因在于：1) RNN 处理的 Sequence 是变长的；2) RNN 是基于 time step 计算，如果直接使用 Batch Normalization 处理，需要保存每个 time step 下，mini btach 的均值和方差，效率低且占内存）。</p>
</li>
<li>
<p>Batch Normalization 基于一个 mini batch 的数据计算均值和方差，而不是基于整个 Training set 来做，相当于进行梯度计算式引入噪声。因此，Batch Normalization 不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用。相反，Weight Normalization 对通过标量  $ g $  和向量  $ v $  对权重  $ W $  进行重写，重写向量  $ v $  是固定的，因此，基于 Weight Normalization 的 Normalization 可以看做比 Batch Normalization 引入更少的噪声。</p>
</li>
<li>
<p>不需要额外的存储空间来保存 mini batch 的均值和方差，同时实现 Weight Normalization 时，对深度学习网络进行正向信号传播和反向梯度计算带来的额外计算开销也很小。因此，要比采用 Batch Normalization 进行 normalization 操作时，速度快。  但是 Weight Normalization 不具备 Batch Normalization 把网络每一层的输出 Y 固定在一个变化范围的作用。因此，采用 Weight Normalization 进行 Normalization 时需要特别注意参数初始值的选择。</p>
</li>
</ol>
<h3 id="3-6-13-Batch-Normalization在什么时候用比较合适？">3.6.13 Batch Normalization在什么时候用比较合适？</h3>
<p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>​	在CNN中，BN应作用在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。</p>
<p>​	BN比较适用的场景是：每个mini-batch比较大，数据分布比较接近。在进行训练之前，要做好充分的shuffle，否则效果会差很多。另外，由于BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量，因此不适用于动态的网络结构和RNN网络。</p>
<h2 id="3-7-预训练与微调-fine-tuning">3.7 预训练与微调(fine tuning)</h2>
<h3 id="3-7-1-为什么无监督预训练可以帮助深度学习？">3.7.1 为什么无监督预训练可以帮助深度学习？</h3>
<p>深度网络存在问题:</p>
<ol>
<li>
<p>网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。</p>
</li>
<li>
<p>多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解；</p>
</li>
<li>
<p>梯度扩散问题，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。</p>
</li>
</ol>
<p><strong>解决方法：</strong></p>
<p>​	逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。</p>
<p>经过预训练最终能得到比较好的局部最优解。</p>
<h3 id="3-7-2-什么是模型微调fine-tuning">3.7.2 什么是模型微调fine tuning</h3>
<p>​	用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning).</p>
<p><strong>模型的微调举例说明：</strong></p>
<p>​	我们知道，CNN 在图像识别这一领域取得了巨大的进步。如果想将 CNN 应用到我们自己的数据集上，这时通常就会面临一个问题：通常我们的 dataset 都不会特别大，一般不会超过 1 万张，甚至更少，每一类图片只有几十或者十几张。这时候，直接应用这些数据训练一个网络的想法就不可行了，因为深度学习成功的一个关键性因素就是大量带标签数据组成的训练集。如果只利用手头上这点数据，即使我们利用非常好的网络结构，也达不到很高的 performance。这时候，fine-tuning 的思想就可以很好解决我们的问题：我们通过对 ImageNet 上训练出来的模型（如CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。</p>
<h3 id="3-7-3-微调时候网络参数是否更新？">3.7.3 微调时候网络参数是否更新？</h3>
<p>答案：会更新。</p>
<ol>
<li>finetune 的过程相当于继续训练，跟直接训练的区别是初始化的时候。</li>
<li>直接训练是按照网络定义指定的方式初始化。</li>
<li>finetune是用你已经有的参数文件来初始化。</li>
</ol>
<h3 id="3-7-4-fine-tuning-模型的三种状态">3.7.4 fine-tuning 模型的三种状态</h3>
<ol>
<li>
<p>状态一：只预测，不训练。<br>
特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；</p>
</li>
<li>
<p>状态二：训练，但只训练最后分类层。<br>
特点：fine-tuning的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。</p>
</li>
<li>
<p>状态三：完全训练，分类层+之前卷积层都训练<br>
特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。</p>
</li>
</ol>
<h2 id="3-8-权重偏差初始化">3.8 权重偏差初始化</h2>
<h3 id="3-8-1-全都初始化为-0">3.8.1 全都初始化为 0</h3>
<p><strong>偏差初始化陷阱</strong>： 都初始化为 0。</p>
<p><strong>产生陷阱原因</strong>：因为并不知道在训练神经网络中每一个权重最后的值，但是如果进行了恰当的数据归一化后，我们可以有理由认为有一半的权重是正的，另一半是负的。令所有权重都初始化为 0，如果神经网络计算出来的输出值是一样的，神经网络在进行反向传播算法计算出来的梯度值也一样，并且参数更新值也一样。更一般地说，如果权重初始化为同一个值，网络就是对称的。</p>
<p><strong>形象化理解</strong>：在神经网络中考虑梯度下降的时候，设想你在爬山，但身处直线形的山谷中，两边是对称的山峰。由于对称性，你所在之处的梯度只能沿着山谷的方向，不会指向山峰；你走了一步之后，情况依然不变。结果就是你只能收敛到山谷中的一个极大值，而走不到山峰上去。</p>
<h3 id="3-8-2-全都初始化为同样的值">3.8.2 全都初始化为同样的值</h3>
<p>​	偏差初始化陷阱： 都初始化为一样的值。<br>
​	以一个三层网络为例：<br>
首先看下结构</p>
<p><img src="3.8.2.1.png" alt></p>
<p>它的表达式为：</p>
 $$
a_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})
$$ 
 $$
a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)})
$$ 
 $$
a_3^{(2)} = f(W_{31}^{(1)} x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)})
$$ 
 $$
h_{W,b}(x) = a_1^{(3)} = f(W_{11}^{(2)} a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)})
$$ 
 $$
xa_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + 
$$ 
<p>如果每个权重都一样，那么在多层网络中，从第二层开始，每一层的输入值都是相同的了也就是 $ a1=a2=a3=.... $ ，既然都一样，就相当于一个输入了，为啥呢？？</p>
<p>如果是反向传递算法（如果这里不明白请看上面的连接），其中的偏置项和权重项的迭代的偏导数计算公式如下</p>
 $$
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b;x,y) = a_j^{(l)} \delta_i^{(l+1)}

\frac{\partial}{\partial b_{i}^{(l)}} J(W,b;x,y) = \delta_i^{(l+1)}
$$ 
 $ \delta $  的计算公式
 $$
\delta_i^{(l)} = (\sum_{j=1}^{s_{t+1}} W_{ji}^{(l)} \delta_j^{(l+1)} ) f^{\prime}(z_i^{(l)})
$$ 
<p>如果用的是 sigmoid 函数</p>
 $$
f^{\prime}(z_i^{(l)}) = a_i^{(l)}(1-a_i^{(l)})
$$ 
<p>把后两个公式代入，可以看出所得到的梯度下降法的偏导相同，不停的迭代，不停的相同，不停的迭代，不停的相同…，最后就得到了相同的值（权重和截距）。</p>
<h3 id="3-8-3-初始化为小的随机数">3.8.3 初始化为小的随机数</h3>
<p>​	将权重初始化为很小的数字是一个普遍的打破网络对称性的解决办法。这个想法是，神经元在一开始都是随机的、独一无二的，所以它们会计算出不同的更新，并将自己整合到整个网络的各个部分。一个权重矩阵的实现可能看起来像  $ W=0.01∗np.random.randn(D,H) $ ，其中 randn 是从均值为 0 的单位标准高斯分布进行取样。通过这个公式(函数)，每个神经元的权重向量初始化为一个从多维高斯分布取样的随机向量，所以神经元在输入空间中指向随机的方向(so the neurons point in random direction in the input space). 应该是指输入空间对于随机方向有影响)。其实也可以从均匀分布中来随机选取小数，但是在实际操作中看起来似乎对最后的表现并没有太大的影响。</p>
<p>​	备注：并不是数字越小就会表现的越好。比如，如果一个神经网络层的权重非常小，那么在反向传播算法就会计算出很小的梯度(因为梯度 gradient 是与权重成正比的)。在网络不断的反向传播过程中将极大地减少“梯度信号”，并可能成为深层网络的一个需要注意的问题。</p>
<h3 id="3-8-4-用-swig￼331-校准方差">3.8.4 用  $ 1/\sqrt n $  校准方差</h3>
<p>​	上述建议的一个问题是，随机初始化神经元的输出的分布有一个随输入量增加而变化的方差。结果证明，我们可以通过将其权重向量按其输入的平方根(即输入的数量)进行缩放，从而将每个神经元的输出的方差标准化到 1。也就是说推荐的启发式方法 (heuristic) 是将每个神经元的权重向量按下面的方法进行初始化:  $ w=np.random.randn(n)/\sqrt n $ ，其中 n 表示输入的数量。这保证了网络中所有的神经元最初的输出分布大致相同，并在经验上提高了收敛速度。</p>
<h3 id="3-8-5-稀疏初始化-Sparse-Initialazation">3.8.5 稀疏初始化(Sparse Initialazation)</h3>
<p>​	另一种解决未校准方差问题的方法是把所有的权重矩阵都设为零，但是为了打破对称性，每个神经元都是随机连接地(从如上面所介绍的一个小的高斯分布中抽取权重)到它下面的一个固定数量的神经元。一个典型的神经元连接的数目可能是小到 10 个。</p>
<h3 id="3-8-6-初始化偏差">3.8.6 初始化偏差</h3>
<p>​	将偏差初始化为零是可能的，也是很常见的，因为非对称性破坏是由权重的小随机数导致的。因为 ReLU 具有非线性特点，所以有些人喜欢使用将所有的偏差设定为小的常数值如 0.01，因为这样可以确保所有的 ReLU 单元在最开始就激活触发(fire)并因此能够获得和传播一些梯度值。然而，这是否能够提供持续的改善还不太清楚(实际上一些结果表明这样做反而使得性能更加糟糕)，所以更通常的做法是简单地将偏差初始化为 0.</p>
<h2 id="3-9-学习率">3.9 学习率</h2>
<h3 id="3-9-1-学习率的作用">3.9.1 学习率的作用</h3>
<p>​	在机器学习中，监督式学习通过定义一个模型，并根据训练集上的数据估计最优参数。梯度下降法是一个广泛被用来最小化模型误差的参数优化算法。梯度下降法通过多次迭代，并在每一步中最小化成本函数（cost 来估计模型的参数。学习率 (learning rate)，在迭代过程中会控制模型的学习进度。</p>
<p>​	在梯度下降法中，都是给定的统一的学习率，整个优化过程中都以确定的步长进行更新， 在迭代优化的前期中，学习率较大，则前进的步长就会较长，这时便能以较快的速度进行梯度下降，而在迭代优化的后期，逐步减小学习率的值，减小步长，这样将有助于算法的收敛，更容易接近最优解。故而如何对学习率的更新成为了研究者的关注点。<br>
​	在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减</p>
<h3 id="3-9-2-学习率衰减常用参数有哪些">3.9.2 学习率衰减常用参数有哪些</h3>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>learning_rate</td>
<td>初始学习率</td>
</tr>
<tr>
<td>global_step</td>
<td>用于衰减计算的全局步数，非负，用于逐步计算衰减指数</td>
</tr>
<tr>
<td>decay_steps</td>
<td>衰减步数，必须是正值，决定衰减周期</td>
</tr>
<tr>
<td>decay_rate</td>
<td>衰减率</td>
</tr>
<tr>
<td>end_learning_rate</td>
<td>最低的最终学习率</td>
</tr>
<tr>
<td>cycle</td>
<td>学习率下降后是否重新上升</td>
</tr>
<tr>
<td>alpha</td>
<td>最小学习率</td>
</tr>
<tr>
<td>num_periods</td>
<td>衰减余弦部分的周期数</td>
</tr>
<tr>
<td>initial_variance</td>
<td>噪声的初始方差</td>
</tr>
<tr>
<td>variance_decay</td>
<td>衰减噪声的方差</td>
</tr>
</tbody>
</table>
<h3 id="3-9-3-分段常数衰减">3.9.3 分段常数衰减</h3>
<p>​	分段常数衰减需要事先定义好的训练次数区间，在对应区间置不同的学习率的常数值，一般情况刚开始的学习率要大一些，之后要越来越小，要根据样本量的大小设置区间的间隔大小，样本量越大，区间间隔要小一点。下图即为分段常数衰减的学习率变化图，横坐标代表训练次数，纵坐标代表学习率。</p>
<p><img src="learnrate1.png" alt></p>
<h3 id="3-9-4-指数衰减">3.9.4 指数衰减</h3>
<p>​	以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关，其更新规则为：</p>
 $$
decayed{\_}learning{\_}rate =learning{\_}rate*decay{\_}rate^{\frac{global{\_step}}{decay{\_}steps}}
$$ 
<p>​	这种衰减方式简单直接，收敛速度快，是最常用的学习率衰减方式，如下图所示，绿色的为学习率随<br>
训练次数的指数衰减方式，红色的即为分段常数衰减，它在一定的训练区间内保持学习率不变。</p>
<p><img src="learnrate2.png" alt></p>
<h3 id="3-9-5-自然指数衰减">3.9.5 自然指数衰减</h3>
<p>​	它与指数衰减方式相似，不同的在于它的衰减底数是 $e$ ，故而其收敛的速度更快，一般用于相对比较<br>
容易训练的网络，便于较快的收敛，其更新规则如下</p>
 $$
decayed{\_}learning{\_}rate =learning{\_}rate*e^{\frac{-decay{\_rate}}{global{\_}step}}
$$ 
<p>​	下图为为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，很明可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。</p>
<p><img src="learnrate3.png" alt></p>
<h3 id="3-9-6-多项式衰减">3.9.6 多项式衰减</h3>
<p>​	应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照<br>
给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示。</p>
 $$
global{\_}step=min(global{\_}step,decay{\_}steps)
$$ 
 $$
decayed{\_}learning{\_}rate =(learning{\_}rate-end{\_}learning{\_}rate)* \left( 1-\frac{global{\_step}}{decay{\_}steps}\right)^{power} \\
 +end{\_}learning{\_}rate
$$ 
<p>​	需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，如下式所示.它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。</p>
 $$
decay{\_}steps = decay{\_}steps*ceil \left( \frac{global{\_}step}{decay{\_}steps}\right)
$$ 
<p>​	如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。</p>
<p><img src="learnrate4.png" alt></p>
<h3 id="3-9-7-余弦衰减">3.9.7 余弦衰减</h3>
<p>​	余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示：</p>
 $$
global{\_}step=min(global{\_}step,decay{\_}steps)
$$ 
 $$
cosine{\_}decay=0.5*\left( 1+cos\left( \pi* \frac{global{\_}step}{decay{\_}steps}\right)\right)
$$ 
 $$
decayed=(1-\alpha)*cosine{\_}decay+\alpha
$$ 
 $$
decayed{\_}learning{\_}rate=learning{\_}rate*decayed
$$ 
<p>​	如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是学习率从初始学习率以线性的方式下降到最低学习率值。绿色噪声线性余弦衰减方式。</p>
<p><img src="learnrate5.png" alt></p>
<h2 id="3-12-Dropout-系列问题">3.12 Dropout 系列问题</h2>
<h3 id="3-12-1-为什么要正则化？">3.12.1 为什么要正则化？</h3>
<ol>
<li>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。</li>
<li>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差。</li>
</ol>
<h3 id="3-12-2-为什么正则化有利于预防过拟合？">3.12.2 为什么正则化有利于预防过拟合？</h3>
<p><img src="3.12.2.1.png" alt><br>
<img src="3.12.2.2.png" alt></p>
<p>左图是高偏差，右图是高方差，中间是Just Right，这几张图我们在前面课程中看到过。</p>
<h3 id="3-12-3-理解dropout正则化">3.12.3 理解dropout正则化</h3>
<p>​	Dropout可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？</p>
<p>​	直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。</p>
<h3 id="3-12-4-dropout率的选择">3.12.4 dropout率的选择</h3>
<ol>
<li>经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。</li>
<li>dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8）</li>
<li>对参数  $ w $  的训练进行球形限制 (max-normalization)，对 dropout 的训练非常有用。</li>
<li>球形半径  $ c $  是一个需要调整的参数，可以使用验证集进行参数调优。</li>
<li>dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的learning rate 导致的参数 blow up。</li>
<li>使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以  $ 1/p $ 。</li>
</ol>
<h3 id="3-12-5-dropout有什么缺点？">3.12.5 dropout有什么缺点？</h3>
<p>​	dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。</p>
<h2 id="3-13-深度学习中常用的数据增强方法？">3.13 深度学习中常用的数据增强方法？</h2>
<p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<ul>
<li>
<p>Color Jittering：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）；</p>
</li>
<li>
<p>PCA  Jittering：首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering；</p>
</li>
<li>
<p>Random Scale：尺度变换；</p>
</li>
<li>
<p>Random Crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括Scale Jittering方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换；</p>
</li>
<li>
<p>Horizontal/Vertical Flip：水平/垂直翻转；</p>
</li>
<li>
<p>Shift：平移变换；</p>
</li>
<li>
<p>Rotation/Reflection：旋转/仿射变换；</p>
</li>
<li>
<p>Noise：高斯噪声、模糊处理；</p>
</li>
<li>
<p>Label Shuffle：类别不平衡数据的增广；</p>
</li>
</ul>
<h2 id="3-14-如何理解-Internal-Covariate-Shift？">3.14 如何理解 Internal Covariate Shift？</h2>
<p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>​	深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。</p>
<p>​	Google 将这一现象总结为 Internal Covariate Shift，简称 ICS。 什么是 ICS 呢？</p>
<p>​	大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。</p>
<p>​	大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p>
<p><strong>那么ICS会导致什么问题？</strong></p>
<p>简而言之，每个神经元的输入数据不再是“独立同分布”。</p>
<p>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</p>
<p>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</p>
<p>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</p>
<h2 id="参考文献">参考文献</h2>
<p>[1] Rosenblatt, F. The perceptron: A probabilistic model for information storage and organization in the brain.[J]. Psychological Review, 1958, 65(6):386-408.</p>
<p>[2] Duvenaud D , Rippel O , Adams R P , et al. Avoiding pathologies in very deep networks[J]. Eprint Arxiv, 2014:202-210.</p>
<p>[3] Rumelhart D E, Hinton G E, Williams R J. Learning representations by back-propagating errors[J]. Cognitive modeling, 1988, 5(3): 1.</p>
<p>[4] Hecht-Nielsen R. Theory of the backpropagation neural network[M]//Neural networks for perception. Academic Press, 1992: 65-93.</p>
<p>[5] Felice M. Which deep learning network is best for you?| CIO[J]. 2017.</p>
<p>[6] Conneau A, Schwenk H, Barrault L, et al. Very deep convolutional networks for natural language processing[J]. arXiv preprint arXiv:1606.01781, 2016, 2.</p>
<p>[7] Ba J, Caruana R. Do deep nets really need to be deep?[C]//Advances in neural information processing systems. 2014: 2654-2662.</p>
<p>[8] Nielsen M A. Neural networks and deep learning[M]. USA: Determination press, 2015.</p>
<p>[9] Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.</p>
<p>[10] 周志华. 机器学习[M].清华大学出版社, 2016.</p>
<p>[11] Kim J, Kwon Lee J, Mu Lee K. Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 1646-1654.</p>
<p>[12] Chen Y, Lin Z, Zhao X, et al. Deep learning-based classification of hyperspectral data[J]. IEEE Journal of Selected topics in applied earth observations and remote sensing, 2014, 7(6): 2094-2107.</p>
<p>[13] Domhan T, Springenberg J T, Hutter F. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves[C]//Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.</p>
<p>[14] Maclaurin D, Duvenaud D, Adams R. Gradient-based hyperparameter optimization through reversible learning[C]//International Conference on Machine Learning. 2015: 2113-2122.</p>
<p>[15] Srivastava R K, Greff K, Schmidhuber J. Training very deep networks[C]//Advances in neural information processing systems. 2015: 2377-2385.</p>
<p>[16] Bergstra J, Bengio Y. Random search for hyper-parameter optimization[J]. Journal of Machine Learning Research, 2012, 13(Feb): 281-305.</p>
<p>[17] Ngiam J, Khosla A, Kim M, et al. Multimodal deep learning[C]//Proceedings of the 28th international conference on machine learning (ICML-11). 2011: 689-696.</p>
<p>[18] Deng L, Yu D. Deep learning: methods and applications[J]. Foundations and Trends® in Signal Processing, 2014, 7(3–4): 197-387.</p>
<p>[19] Erhan D, Bengio Y, Courville A, et al. Why does unsupervised pre-training help deep learning?[J]. Journal of Machine Learning Research, 2010, 11(Feb): 625-660.</p>
<p>[20] Dong C, Loy C C, He K, et al. Learning a deep convolutional network for image super resolution[C]//European conference on computer vision. Springer, Cham, 2014: 184-199.</p>
<p>[21] 郑泽宇，梁博文，顾思宇.TensorFlow：实战Google深度学习框架（第2版）[M].电子工业出版社,2018.</p>
<p>[22] 焦李成. 深度学习优化与识别[M].清华大学出版社,2017.</p>
<p>[23] 吴岸城. 神经网络与深度学习[M].电子工业出版社,2016.</p>
<p>[24] Wei, W.G.H., Liu, T., Song, A., et al. (2018) An Adaptive Natural Gradient Method with Adaptive Step Size in Multilayer Perceptrons. Chinese Automation Congress, 1593-1597.</p>
<p>[25] Y Feng, Y <a target="_blank" rel="noopener" href="http://Li.An">Li.An</a> Overview of Deep Learning Optimization Methods and Learning Rate Attenuation Methods[J].Hans Journal of Data Mining,2018,8(4),186-200.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/deep_learning/ch6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/deep_learning/ch6/" class="post-title-link" itemprop="url">循环神经网络面试题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-24 10:13:51" itemprop="dateModified" datetime="2024-03-24T10:13:51+08:00">2024-03-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">算法面试</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>循环神经网络(RNN)</h1>
<h2 id="6-1-为什么需要RNN？">6.1 为什么需要RNN？</h2>
<p>​	时间序列数据是指在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随时间的变化状态或程度。一般的神经网络，在训练数据足够、算法模型优越的情况下，给定特定的x，就能得到期望y。其一般处理单个的输入，前一个输入和后一个输入完全无关，但实际应用中，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如：</p>
<p>​	当我们在理解一句话意思时，孤立的理解这句话的每个词不足以理解整体意思，我们通常需要处理这些词连接起来的整个序列； 当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就由此诞生了。</p>
<h2 id="6-2-图解RNN基本结构">6.2 图解RNN基本结构</h2>
<h3 id="6-2-1-基本的单层网络结构">6.2.1 基本的单层网络结构</h3>
<p>​	在进一步了解RNN之前，先给出最基本的单层网络结构，输入是<code>$x$</code>，经过变换<code>Wx+b</code>和激活函数<code>f</code>得到输出<code>y</code>：</p>
<p><img src="6.1.jpg" alt></p>
<h3 id="6-2-2-图解经典RNN结构">6.2.2 图解经典RNN结构</h3>
<p>​	在实际应用中，我们还会遇到很多序列形的数据，如：</p>
<ul>
<li>
<p>自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。</p>
</li>
<li>
<p>语音处理。此时，x1、x2、x3……是每帧的声音信号。</p>
</li>
<li>
<p>时间序列问题。例如每天的股票价格等等。</p>
<p>其单个序列如下图所示：</p>
<p><img src="6.2.jpg" alt></p>
<p>前面介绍了诸如此类的序列数据用原始的神经网络难以建模，基于此，RNN引入了隐状态 $h$ （hidden state）， $h​$ 可对序列数据提取特征，接着再转换为输出。</p>
<p>为了便于理解，先计算 $h_1​$ ：</p>
<p><img src="6.3.jpg" alt></p>
<p>注：图中的圆圈表示向量，箭头表示对向量做变换。</p>
<p>RNN中，每个步骤使用的参数<code>$U,W,b$</code>​相同，<code>$h_2$</code>的计算方式和<code>$h_1​$</code>类似，其计算结果如下：</p>
<p><img src="6.4.jpg" alt></p>
<p>计算 $h_3$ , $h_4​$ 也相似，可得：</p>
<p><img src="6.5.jpg" alt></p>
<p>接下来，计算RNN的输出 $y_1$ ，采用 $Softmax$ 作为激活函数，根据 $y_n=f(Wx+b)$ ，得 $y_1​$ :</p>
<p><img src="6.6.jpg" alt></p>
<p>使用和 $y_1​$ 相同的参数 $V,c​$ ，得到 $y_1,y_2,y_3,y_4​$ 的输出结构：</p>
<p><img src="6.7.jpg" alt></p>
<p>以上即为最经典的RNN结构，其输入为 $x_1,x_2,x_3,x_4$ ，输出为 $y_1,y_2,y_3,y_4$ ，当然实际中最大值为 $y_n$ ，这里为了便于理解和展示，只计算4个输入和输出。从以上结构可看出，RNN结构的输入和输出等长。</p>
</li>
</ul>
<h3 id="6-2-3-vector-to-sequence结构">6.2.3 vector-to-sequence结构</h3>
<p>​	有时我们要处理的问题输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p>
<p>​	方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算，其建模方式如下：</p>
<p><img src="6.9.jpg" alt></p>
<p>​	方式二：把输入信息X作为每个阶段的输入，其建模方式如下：</p>
<p><img src="6.10.jpg" alt></p>
<h3 id="6-2-4-sequence-to-vector结构">6.2.4 sequence-to-vector结构</h3>
<p>​	有时我们要处理的问题输入是一个序列，输出是一个单独的值，此时通常在最后的一个序列上进行输出变换，其建模如下所示：</p>
<p><img src="6.8.jpg" alt></p>
<h3 id="6-2-5-Encoder-Decoder结构">6.2.5 Encoder-Decoder结构</h3>
<p>​	原始的sequence-to-sequence结构的RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。</p>
<p>​	其建模步骤如下：</p>
<p>​	<strong>步骤一</strong>：将输入数据编码成一个上下文向量 $c$ ，这部分称为Encoder，得到 $c$ 有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给 $c$ ，还可以对最后的隐状态做一个变换得到 $c$ ，也可以对所有的隐状态做变换。其示意如下所示：</p>
<p><img src="6.12.jpg" alt></p>
<p>​	<strong>步骤二</strong>：用另一个RNN网络（我们将其称为Decoder）对其进行编码，方法一是将步骤一中的 $c​$ 作为初始状态输入到Decoder，示意图如下所示：</p>
<p><img src="6.13.jpg" alt></p>
<p>方法二是将 $c$ 作为Decoder的每一步输入，示意图如下所示：</p>
<p><img src="6.14.jpg" alt></p>
<h3 id="6-2-6-以上三种结构各有怎样的应用场景">6.2.6  以上三种结构各有怎样的应用场景</h3>
<table>
<thead>
<tr>
<th>网络结构</th>
<th style="text-align:center">结构图示</th>
<th>应用场景举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 vs N</td>
<td style="text-align:center"><img src="6.9.jpg" alt></td>
<td>1、从图像生成文字，输入为图像的特征，输出为一段句子<br>2、根据图像生成语音或音乐，输入为图像特征，输出为一段语音或音乐</td>
</tr>
<tr>
<td>N vs 1</td>
<td style="text-align:center"><img src="6.8.jpg" alt></td>
<td>1、输出一段文字，判断其所属类别<br>2、输入一个句子，判断其情感倾向<br>3、输入一段视频，判断其所属类别</td>
</tr>
<tr>
<td>N vs M</td>
<td style="text-align:center"><img src="6.13.jpg" alt></td>
<td>1、机器翻译，输入一种语言文本序列，输出另外一种语言的文本序列<br>2、文本摘要，输入文本序列，输出这段文本序列摘要<br>3、阅读理解，输入文章，输出问题答案<br>4、语音识别，输入语音序列信息，输出文字序列</td>
</tr>
</tbody>
</table>
<h3 id="6-2-7-图解RNN中的Attention机制">6.2.7 图解RNN中的Attention机制</h3>
<p>​	在上述通用的Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征 $c​$ 再解码，因此， $c​$ 中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个 $c​$ 可能存不下那么多信息，就会造成翻译精度的下降。Attention机制通过在每个时间输入不同的 $c​$ 来解决此问题。</p>
<p>​	引入了Attention机制的Decoder中，有不同的 $c$ ，每个 $c​$ 会自动选择与当前输出最匹配的上下文信息，其示意图如下所示：</p>
<p><img src="6.15.jpg" alt></p>
<p>​	<strong>举例</strong>，比如输入序列是“我爱中国”，要将此输入翻译成英文：</p>
<p>​	假如用 $a_{ij}$ 衡量Encoder中第 $j$ 阶段的 $h_j$ 和解码时第 $i$ 阶段的相关性， $a_{ij}$ 从模型中学习得到，和Decoder的第 $i-1$ 阶段的隐状态、Encoder 第 $j$ 个阶段的隐状态有关，比如 $a_{3j}​$ 的计算示意如下所示：</p>
<p><img src="6.19.jpg" alt></p>
<p>最终Decoder中第 $i$ 阶段的输入的上下文信息  $c_i$ 来自于所有 $h_j$ 对 $a_{ij}$ 的加权和。</p>
<p>其示意图如下图所示：</p>
<p><img src="6.16.jpg" alt></p>
<p>​	在Encoder中， $h_1,h_2,h_3,h_4$ 分别代表“我”，“爱”，“中”，“国”所代表信息。翻译的过程中， $c_1$ 会选择和“我”最相关的上下午信息，如上图所示，会优先选择 $a_{11}$ ，以此类推， $c_2$ 会优先选择相关性较大的 $a_{22}$ ， $c_3$ 会优先选择相关性较大的 $a_{33}，a_{34}$ ，这就是attention机制。</p>
<h2 id="6-3-RNNs典型特点？">6.3 RNNs典型特点？</h2>
<ol>
<li>RNNs主要用于处理序列数据。对于传统神经网络模型，从输入层到隐含层再到输出层，层与层之间一般为全连接，每层之间神经元是无连接的。但是传统神经网络无法处理数据间的前后关联问题。例如，为了预测句子的下一个单词，一般需要该词之前的语义信息。这是因为一个句子中前后单词是存在语义联系的。</li>
<li>RNNs中当前单元的输出与之前步骤输出也有关，因此称之为循环神经网络。具体的表现形式为当前单元会对之前步骤信息进行储存并应用于当前输出的计算中。隐藏层之间的节点连接起来，隐藏层当前输出由当前时刻输入向量和之前时刻隐藏层状态共同决定。</li>
<li>标准的RNNs结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值。</li>
<li>在标准的RNN结构中，隐层的神经元之间也是带有权值的，且权值共享。</li>
<li>理论上，RNNs能够对任何长度序列数据进行处理。但是在实践中，为了降低复杂度往往假设当前的状态只与之前某几个时刻状态相关，<strong>下图便是一个典型的RNNs</strong>：</li>
</ol>
<p><img src="figure_6.2_1.png" alt></p>
<p><img src="figure_6.2_2.jpg" alt></p>
<p>输入单元(Input units)：输入集 $\bigr\{x_0,x_1,...,x_t,x_{t+1},...\bigr\}$ ，</p>
<p>输出单元(Output units)：输出集 $\bigr\{y_0,y_1,...,y_t,y_{y+1},...\bigr\}$ ，</p>
<p>隐藏单元(Hidden units)：输出集 $\bigr\{s_0,s_1,...,s_t,s_{t+1},...\bigr\}$ 。</p>
<p><strong>图中信息传递特点：</strong></p>
<ol>
<li>一条单向流动的信息流是从输入单元到隐藏单元。</li>
<li>一条单向流动的信息流从隐藏单元到输出单元。</li>
<li>在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”。</li>
<li>在某些情况下，隐藏层的输入还包括上一时刻隐藏层的状态，即隐藏层内的节点可以自连也可以互连。</li>
<li>当前单元（cell）输出是由当前时刻输入和上一时刻隐藏层状态共同决定。</li>
</ol>
<h2 id="6-4-CNN和RNN的区别-？">6.4 CNN和RNN的区别 ？</h2>
<table>
<thead>
<tr>
<th>类别</th>
<th>特点描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>相同点</td>
<td>1、传统神经网络的扩展。<br>2、前向计算产生结果，反向计算模型更新。<br>3、每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。</td>
</tr>
<tr>
<td>不同点</td>
<td>1、CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算<br>2、RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出</td>
</tr>
</tbody>
</table>
<h2 id="6-5-RNNs和FNNs有什么区别？">6.5 RNNs和FNNs有什么区别？</h2>
<ol>
<li>不同于传统的前馈神经网络(FNNs)，RNNs引入了定向循环，能够处理输入之间前后关联问题。</li>
<li>RNNs可以记忆之前步骤的训练信息。<br>
<strong>定向循环结构如下图所示</strong>：</li>
</ol>
<p><img src="figure_6.1_1.jpg" alt></p>
<h2 id="6-6-RNNs训练和传统ANN训练异同点？">6.6 RNNs训练和传统ANN训练异同点？</h2>
<p><strong>相同点</strong>：</p>
<ol>
<li>RNNs与传统ANN都使用BP（Back Propagation）误差反向传播算法。</li>
</ol>
<p><strong>不同点</strong>：</p>
<ol>
<li>RNNs网络参数W,U,V是共享的(具体在本章6.2节中已介绍)，而传统神经网络各层参数间没有直接联系。</li>
<li>对于RNNs，在使用梯度下降算法中，每一步的输出不仅依赖当前步的网络，还依赖于之前若干步的网络状态。</li>
</ol>
<h2 id="6-7-为什么RNN-训练的时候Loss波动很大">6.7 为什么RNN 训练的时候Loss波动很大</h2>
<p>​	由于RNN特有的memory会影响后期其他的RNN的特点，梯度时大时小，learning rate没法个性化的调整，导致RNN在train的过程中，Loss会震荡起伏，为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。</p>
<h2 id="6-8-标准RNN前向输出流程">6.8 标准RNN前向输出流程</h2>
<p>​	以 $x$ 表示输入， $h$ 是隐层单元， $o$ 是输出， $L$ 为损失函数， $y$ 为训练集标签。 $t$ 表示 $t$ 时刻的状态， $V,U,W$ 是权值，同一类型的连接权值相同。以下图为例进行说明标准RNN的前向传播算法：</p>
<p>​	<img src="rnnbp.png" alt></p>
<p>对于 $t$ 时刻：</p>
 $$
h^{(t)}=\phi(Ux^{(t)}+Wh^{(t-1)}+b)
$$ 
<p>其中 $\phi()$ 为激活函数，一般会选择tanh函数， $b$ 为偏置。</p>
 $t$ 时刻的输出为：
 $$
o^{(t)}=Vh^{(t)}+c
$$ 
<p>模型的预测输出为：</p>
 $$
\widehat{y}^{(t)}=\sigma(o^{(t)})
$$ 
<p>其中 $\sigma​$ 为激活函数，通常RNN用于分类，故这里一般用softmax函数。</p>
<h2 id="6-9-BPTT算法推导">6.9 BPTT算法推导</h2>
<p>​	BPTT（back-propagation through time）算法是常用的训练RNN的方法，其本质还是BP算法，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。需要寻优的参数有三个，分别是U、V、W。与BP算法不同的是，其中W和U两个参数的寻优过程需要追溯之前的历史数据，参数V相对简单只需关注目前，那么我们就来先求解参数V的偏导数。</p>
 $$
\frac{\partial L^{(t)}}{\partial V}=\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}
$$ 
<p>RNN的损失也是会随着时间累加的，所以不能只求t时刻的偏导。</p>
 $$
L=\sum_{t=1}^{n}L^{(t)}
$$ 
 $$
\frac{\partial L}{\partial V}=\sum_{t=1}^{n}\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}
$$ 
<p>​	W和U的偏导的求解由于需要涉及到历史数据，其偏导求起来相对复杂。为了简化推导过程，我们假设只有三个时刻，那么在第三个时刻 L对W，L对U的偏导数分别为：</p>
 $$
\frac{\partial L^{(3)}}{\partial W}=\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial W}
$$ 
 $$
\frac{\partial L^{(3)}}{\partial U}=\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial U}
$$ 
<p>可以观察到，在某个时刻的对W或是U的偏导数，需要追溯这个时刻之前所有时刻的信息。根据上面两个式子得出L在t时刻对W和U偏导数的通式：</p>
 $$
\frac{\partial L^{(t)}}{\partial W}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial W}
$$ 
 $$
\frac{\partial L^{(t)}}{\partial U}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial U}
$$ 
<p>整体的偏导公式就是将其按时刻再一一加起来。</p>
<h2 id="6-9-RNN中为什么会出现梯度消失？">6.9 RNN中为什么会出现梯度消失？</h2>
<p>首先来看tanh函数的函数及导数图如下所示：</p>
<p><img src="tanh.jpg" alt></p>
<p>sigmoid函数的函数及导数图如下所示：</p>
<p><img src="sigmoid.jpg" alt></p>
<p>从上图观察可知，sigmoid函数的导数范围是(0,0.25]，tanh函数的导数范围是(0,1]，他们的导数最大都不大于1。</p>
<p>​	基于6.8中式（9-10）中的推导，RNN的激活函数是嵌套在里面的，如果选择激活函数为 $tanh$ 或 $sigmoid$ ，把激活函数放进去，拿出中间累乘的那部分可得：</p>
 $$
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{tanh^{'}}\cdot W_{s}
$$ 
 $$
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{sigmoid^{'}}\cdot W_{s}
$$ 
<p>​	<strong>梯度消失现象</strong>：基于上式，会发现累乘会导致激活函数导数的累乘，如果取tanh或sigmoid函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。</p>
<p>​	实际使用中，会优先选择tanh函数，原因是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p>
<h2 id="6-10-如何解决RNN中的梯度消失问题？">6.10 如何解决RNN中的梯度消失问题？</h2>
<p>​	上节描述的梯度消失是在无限的利用历史数据而造成，但是RNN的特点本来就是能利用历史数据获取更多的可利用信息，解决RNN中的梯度消失方法主要有：</p>
<p>​	1、选取更好的激活函数，如Relu激活函数。ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。</p>
<p>​	2、加入BN层，其优点包括可加速收敛、控制过拟合，可以少用或不用Dropout和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。</p>
<p>​	2、改变传播结构，LSTM结构可以有效解决这个问题。下面将介绍LSTM相关内容。</p>
<h2 id="6-11-LSTM">6.11 LSTM</h2>
<h3 id="6-11-1-LSTM的产生原因">6.11.1 LSTM的产生原因</h3>
<p>​	RNN在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成梯度消失或者梯度膨胀的现象。为了解决该问题，研究人员提出了许多解决办法，例如ESN（Echo State Network），增加有漏单元（Leaky Units）等等。其中最成功应用最广泛的就是门限RNN（Gated RNN），而LSTM就是门限RNN中最著名的一种。有漏单元通过设计连接间的权重系数，从而允许RNN累积距离较远节点间的长期联系；而门限RNN则泛化了这样的思想，允许在不同时刻改变该系数，且允许网络忘记当前已经累积的信息。</p>
<h3 id="6-11-2-图解标准RNN和LSTM的区别">6.11.2 图解标准RNN和LSTM的区别</h3>
<p>​	所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层，如下图所示：</p>
<p><img src="LSTM1.png" alt></p>
<p>​	LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p>
<p><img src="LSTM2.png" alt></p>
<p>注：上图图标具体含义如下所示：</p>
<p><img src="LSTM3.png" alt></p>
<p>​	上图中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p>
<h3 id="6-11-3-LSTM核心思想图解">6.11.3 LSTM核心思想图解</h3>
<p>​	LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。示意图如下所示：</p>
<p><img src="LSTM4.png" alt></p>
<p>LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。示意图如下：</p>
<p><img src="LSTM5.png" alt></p>
<p>LSTM 拥有三个门，分别是忘记层门，输入层门和输出层门，来保护和控制细胞状态。</p>
<p><strong>忘记层门</strong></p>
<p>​	作用对象：细胞状态 。</p>
<p>​	作用：将细胞状态中的信息选择性的遗忘。</p>
<p>​	操作步骤：该门会读取 $h_{t-1}$ 和 $x_t$ ，输出一个在 0 到 1 之间的数值给每个在细胞状态 $C_{t-1}​$ 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。示意图如下：</p>
<p><img src="LSTM6.png" alt></p>
<p><strong>输入层门</strong></p>
<p>​	作用对象：细胞状态</p>
<p>​	作用：将新的信息选择性的记录到细胞状态中。</p>
<p>​	操作步骤：</p>
<p>​	步骤一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。</p>
<p>​	步骤二，tanh 层创建一个新的候选值向量 $\tilde{C}_t$ 加入到状态中。其示意图如下：</p>
<p><img src="LSTM7.png" alt></p>
<p>​	步骤三：将 $c_{t-1}$ 更新为 $c_{t}$ 。将旧状态与 $f_t$ 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 $i_t * \tilde{C}_t$ 得到新的候选值，根据我们决定更新每个状态的程度进行变化。其示意图如下：</p>
<p><img src="LSTM8.png" alt></p>
<p><strong>输出层门</strong><br>
作用对象：隐层 $h_t$</p>
<p>​	作用：确定输出什么值。</p>
<p>​	操作步骤：</p>
<p>​	步骤一：通过sigmoid 层来确定细胞状态的哪个部分将输出。</p>
<p>​	步骤二：把细胞状态通过 tanh 进行处理，并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p>其示意图如下所示：</p>
<p><img src="LSTM9.png" alt></p>
<h3 id="6-11-4-LSTM流行的变体">6.11.4 LSTM流行的变体</h3>
<p><strong>增加peephole 连接</strong></p>
<p>​	在正常的LSTM结构中，Gers F A 等人提出增加peephole 连接，可以门层接受细胞状态的输入。示意图如下所示：</p>
<p><img src="LSTM10.png" alt></p>
<p><strong>对忘记门和输入门进行同时确定</strong></p>
<p>​	不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。示意图如下所示：</p>
<p><img src="LSTM11.png" alt></p>
<p><strong>Gated Recurrent Unit</strong></p>
<p>​	 由Kyunghyun Cho等人提出的Gated Recurrent Unit (GRU)，其将忘记门和输入门合成了一个单一的更新门，同样还混合了细胞状态和隐藏状态，和其他一些改动。其示意图如下：</p>
<p><img src="LSTM12.png" alt></p>
<p>最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p>
<h2 id="6-12-LSTMs与GRUs的区别">6.12 LSTMs与GRUs的区别</h2>
<p>LSTMs与GRUs的区别如图所示：</p>
<p><img src="figure_6.6.6_2.png" alt></p>
<p>从上图可以看出，二者结构十分相似，<strong>不同在于</strong>：</p>
<ol>
<li>new memory都是根据之前state及input进行计算，但是GRUs中有一个reset gate控制之前state的进入量，而在LSTMs里没有类似gate；</li>
<li>产生新的state的方式不同，LSTMs有两个不同的gate，分别是forget gate (f gate)和input gate(i gate)，而GRUs只有一种update gate(z gate)；</li>
<li>LSTMs对新产生的state可以通过output gate(o gate)进行调节，而GRUs对输出无任何调节。</li>
</ol>
<h2 id="6-13-RNNs在NLP中典型应用？">6.13 RNNs在NLP中典型应用？</h2>
<p><strong>（1）语言模型与文本生成(Language Modeling and Generating Text)</strong></p>
<p>​	给定一组单词序列，需要根据前面单词预测每个单词出现的可能性。语言模型能够评估某个语句正确的可能性，可能性越大，语句越正确。另一种应用便是使用生成模型预测下一个单词的出现概率，从而利用输出概率的采样生成新的文本。</p>
<p><strong>（2）机器翻译(Machine Translation)</strong></p>
<p>​	机器翻译是将一种源语言语句变成意思相同的另一种源语言语句，如将英语语句变成同样意思的中文语句。与语言模型关键的区别在于，需要将源语言语句序列输入后，才进行输出，即输出第一个单词时，便需要从完整的输入序列中进行获取。</p>
<p><strong>（3）语音识别(Speech Recognition)</strong></p>
<p>​	语音识别是指给定一段声波的声音信号，预测该声波对应的某种指定源语言语句以及计算该语句的概率值。</p>
<p><strong>（4）图像描述生成 (Generating Image Descriptions)</strong></p>
<p>​	同卷积神经网络一样，RNNs已经在对无标图像描述自动生成中得到应用。CNNs与RNNs结合也被应用于图像描述自动生成。<br>
<img src="figure_6.4_1.png" alt></p>
<h2 id="6-13-常见的RNNs扩展和改进模型">6.13 常见的RNNs扩展和改进模型</h2>
<h3 id="6-13-1-Simple-RNNs-SRNs">6.13.1 Simple RNNs(SRNs)</h3>
<ol>
<li>SRNs是一个三层网络，其在隐藏层增加了上下文单元。下图中的y是隐藏层，u是上下文单元。上下文单元节点与隐藏层中节点的连接是固定的，并且权值也是固定的。上下文节点与隐藏层节点一一对应，并且值是确定的。</li>
<li>在每一步中，使用标准的前向反馈进行传播，然后使用学习算法进行学习。上下文每一个节点保存其连接隐藏层节点上一步输出，即保存上文，并作用于当前步对应的隐藏层节点状态，即隐藏层的输入由输入层的输出与上一步的自身状态所决定。因此SRNs能够解决标准多层感知机(MLP)无法解决的对序列数据进行预测的问题。<br>
SRNs网络结构如下图所示：</li>
</ol>
<p><img src="figure_6.6.1_1.png" alt></p>
<h3 id="6-13-2-Bidirectional-RNNs">6.13.2 Bidirectional RNNs</h3>
<p>​	Bidirectional RNNs(双向网络)将两层RNNs叠加在一起，当前时刻输出(第t步的输出)不仅仅与之前序列有关，还与之后序列有关。例如：为了预测一个语句中的缺失词语，就需要该词汇的上下文信息。Bidirectional RNNs是一个相对较简单的RNNs，是由两个RNNs上下叠加在一起组成的。输出由前向RNNs和后向RNNs共同决定。如下图所示：</p>
<p><img src="figure_6.6.2_1.png" alt></p>
<h3 id="6-13-3-Deep-RNNs">6.13.3 Deep RNNs</h3>
<p>​	Deep RNNs与Bidirectional RNNs相似，其也是又多层RNNs叠加，因此每一步的输入有了多层网络。该网络具有更强大的表达与学习能力，但是复杂性也随之提高，同时需要更多的训练数据。Deep RNNs的结构如下图所示：<br>
<img src="figure_6.6.3_1.png" alt></p>
<h3 id="6-13-4-Echo-State-Networks（ESNs）">6.13.4 Echo State Networks（ESNs）</h3>
<p><strong>ESNs特点</strong>：</p>
<ol>
<li>它的核心结构为一个随机生成、且保持不变的储备池(Reservoir)。储备池是大规模随机生成稀疏连接(SD通常保持1%～5%，SD表示储备池中互相连接的神经元占总神经元个数N的比例)的循环结构；</li>
<li>从储备池到输出层的权值矩阵是唯一需要调整的部分；</li>
<li>简单的线性回归便能够完成网络训练；</li>
</ol>
<p><strong>ESNs基本思想</strong>：</p>
<p>​	使用大规模随机连接的循环网络取代经典神经网络中的中间层，从而简化网络的训练过程。<br>
网络中的参数包括：<br>
（1）W - 储备池中节点间连接权值矩阵；<br>
（2）Win - 输入层到储备池之间连接权值矩阵，表明储备池中的神经元之间是相互连接；<br>
（3）Wback - 输出层到储备池之间的反馈连接权值矩阵，表明储备池会有输出层来的反馈；<br>
（4）Wout - 输入层、储备池、输出层到输出层的连接权值矩阵，表明输出层不仅与储备池连接，还与输入层和自己连接。<br>
（5）Woutbias - 输出层的偏置项。</p>
<p>​	ESNs的结构如下图所示：</p>
<p><img src="figure_6.6.4_2.png" alt></p>
<h3 id="6-13-4-Gated-Recurrent-Unit-Recurrent-Neural-Networks">6.13.4 Gated Recurrent Unit Recurrent Neural Networks</h3>
<p>GRUs是一般的RNNs的变型版本，其主要是从以下两个方面进行改进。</p>
<ol>
<li>
<p>以语句为例，序列中不同单词处的数据对当前隐藏层状态的影响不同，越前面的影响越小，即每个之前状态对当前的影响进行了距离加权，距离越远，权值越小。</p>
</li>
<li>
<p>在产生误差error时，其可能是由之前某一个或者几个单词共同造成，所以应当对对应的单词weight进行更新。GRUs的结构如下图所示。GRUs首先根据当前输入单词向量word vector以及前一个隐藏层状态hidden state计算出update gate和reset gate。再根据reset gate、当前word vector以及前一个hidden state计算新的记忆单元内容(new memory content)。当reset gate为1的时候，new memory content忽略之前所有memory content，最终的memory是由之前的hidden state与new memory content一起决定。</p>
</li>
</ol>
<p><img src="figure_6.6.5_1.png" alt></p>
<h3 id="6-13-5-Bidirectional-LSTMs">6.13.5 Bidirectional LSTMs</h3>
<ol>
<li>与bidirectional RNNs 类似，bidirectional LSTMs有两层LSTMs。一层处理过去的训练信息，另一层处理将来的训练信息。</li>
<li>在bidirectional LSTMs中，通过前向LSTMs获得前向隐藏状态，后向LSTMs获得后向隐藏状态，当前隐藏状态是前向隐藏状态与后向隐藏状态的组合。</li>
</ol>
<h3 id="6-13-6-Stacked-LSTMs">6.13.6 Stacked LSTMs</h3>
<ol>
<li>与deep rnns 类似，stacked LSTMs 通过将多层LSTMs叠加起来得到一个更加复杂的模型。</li>
<li>不同于bidirectional LSTMs，stacked LSTMs只利用之前步骤的训练信息。</li>
</ol>
<h3 id="6-13-7-Clockwork-RNNs-CW-RNNs">6.13.7 Clockwork RNNs(CW-RNNs)</h3>
<p>​	CW-RNNs是RNNs的改良版本，其使用时钟频率来驱动。它将隐藏层分为几个块(组，Group/Module)，每一组按照自己规定的时钟频率对输入进行处理。为了降低RNNs的复杂度，CW-RNNs减少了参数数量，并且提高了网络性能，加速网络训练。CW-RNNs通过不同隐藏层模块在不同时钟频率下工作来解决长时依赖问题。将时钟时间进行离散化，不同的隐藏层组将在不同时刻进行工作。因此，所有的隐藏层组在每一步不会全部同时工作，这样便会加快网络的训练。并且，时钟周期小组的神经元不会连接到时钟周期大组的神经元，只允许周期大的神经元连接到周期小的(组与组之间的连接以及信息传递是有向的)。周期大的速度慢，周期小的速度快，因此是速度慢的神经元连速度快的神经元，反之则不成立。</p>
<p>​	CW-RNNs与SRNs网络结构类似，也包括输入层(Input)、隐藏层(Hidden)、输出层(Output)，它们之间存在前向连接，输入层到隐藏层连接，隐藏层到输出层连接。但是与SRN不同的是，隐藏层中的神经元会被划分为若干个组，设为 $g​$ ，每一组中的神经元个数相同，设为 $k​$ ，并为每一个组分配一个时钟周期 $T_i\epsilon\{T_1,T_2,...,T_g\}​$ ，每一组中的所有神经元都是全连接，但是组 $j​$ 到组 $i​$ 的循环连接则需要满足 $T_j​$ 大于 $T_i​$ 。如下图所示，将这些组按照时钟周期递增从左到右进行排序，即 $T_1<T_2<...<T_g​$ 4="64个节点，第一组隐藏层与隐藏层的连接矩阵为64" ，那么连接便是从右到左。例如：隐藏层共有256个节点，分为四组，周期分别是[1,2,4,8]，那么每个隐藏层组256 $\times​$ 64的矩阵，第二层的矩阵则为64 128矩阵，第三组为64 (3 64)="64" 192矩阵，第四组为64 (4 256矩阵。这就解释了上一段中速度慢的组连接到速度快的组，反之则不成立。< p>
<p><strong>CW-RNNs的网络结构如下图所示</strong>：</p>
<p><img src="figure_6.6.7_1.png" alt></p>
<h3 id="6-13-8-CNN-LSTMs">6.13.8 CNN-LSTMs</h3>
<ol>
<li>为了同时利用CNN以及LSTMs的优点，CNN-LSTMs被提出。在该模型中，CNN用于提取对象特征，LSTMs用于预测。CNN由于卷积特性，其能够快速而且准确地捕捉对象特征。LSTMs的优点在于能够捕捉数据间的长时依赖性。</li>
</ol>
<h2 id="参考文献">参考文献</h2>
<p>[1] 何之源.<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28054589">https://zhuanlan.zhihu.com/p/28054589</a>.</p>
<p>[2] <a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://blog.csdn.net/zhaojc1995/article/details/80572098">https://blog.csdn.net/zhaojc1995/article/details/80572098</a></p>
<p>[4] Graves A. Supervised Sequence Labelling with Recurrent Neural Networks[J]. Studies in Computational Intelligence, 2008, 385.</p>
<p>[5] Graves A. Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.</p>
<p>[6]  Greff K ,  Srivastava R K , Koutník, Jan, et al. LSTM: A Search Space Odyssey[J]. IEEE Transactions on Neural Networks &amp; Learning Systems, 2015, 28(10):2222-2232.</p>
<p>[7] Lanchantin J, Singh R, Wang B, et al. DEEP MOTIF DASHBOARD: VISUALIZING AND UNDERSTANDING GENOMIC SEQUENCES USING DEEP NEURAL NETWORKS.[J]. Pacific Symposium on Biocomputing Pacific Symposium on Biocomputing, 2016, 22:254.</p>
<p>[8]  Pascanu R ,  Mikolov T ,  Bengio Y . On the difficulty of training Recurrent Neural Networks[J].  2012.</p>
<p>[9]  Hochreiter S. The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions[J]. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 1998, 06(02):-.</p>
<p>[10] Dyer C, Kuncoro A, Ballesteros M, et al. Recurrent Neural Network Grammars[C]// Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.</p>
<p>[11]  Mulder W D ,  Bethard S ,  Moens M F . A survey on the application of recurrent neural networks to statistical language modeling.[M]. Academic Press Ltd.  2015.</p>
<p>[12] Graves A. Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.</p>
<p>[13] Zhang B, Xiong D, Su J. Neural Machine Translation with Deep Attention[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, PP(99):1-1.</p>
<p>[14] <a target="_blank" rel="noopener" href="https://github.com/xuanyuansen/scalaLSTM">https://github.com/xuanyuansen/scalaLSTM</a></p>
<p>[15] Deep Learning，Ian Goodfellow Yoshua Bengio and Aaron Courville，Book in preparation for MIT Press，2016；</p>
<p>[16] <a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>[17] Greff K, Srivastava R K, Koutník J, et al. LSTM: A Search Space Odyssey[J]. IEEE Transactions on Neural Networks &amp; Learning Systems, 2016, 28(10):2222-2232.</p>
<p>[18] Yao K ,  Cohn T ,  Vylomova K , et al. Depth-Gated Recurrent Neural Networks[J].  2015.</p>
<p>[19] Koutník J, Greff K, Gomez F, et al. A Clockwork RNN[J]. Computer Science, 2014:1863-1871.</p>
<p>[20]  Gers F A ,  Schmidhuber J . Recurrent nets that time and count[C]// Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on. IEEE, 2000.</p>
<p>[21] Li S, Wu C, Hai L, et al. FPGA Acceleration of Recurrent Neural Network Based Language Model[C]// IEEE International Symposium on Field-programmable Custom Computing Machines. 2015.</p>
<p>[22]  Mikolov T ,  Kombrink S ,  Burget L , et al. Extensions of recurrent neural network language model[C]// Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011.</p>
<p>[23]  Graves A . Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.</p>
<p>[24]  Sutskever I ,  Vinyals O ,  Le Q V . Sequence to Sequence Learning with Neural Networks[J].  2014.</p>
<p>[25] Liu B, Lane I. Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks[J].  2016.</p>
<p>[26] Graves A, Mohamed A R, Hinton G. Speech recognition with deep recurrent neural networks[C]// IEEE International Conference on Acoustics. 2013.</p>
<p>[27] <a target="_blank" rel="noopener" href="https://cs.stanford.edu/people/karpathy/deepimagesent/">https://cs.stanford.edu/people/karpathy/deepimagesent/</a></p>
<p>[28] Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078, 2014.</p>
</T_2<...<T_g​$></p>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/deep_learning/ch7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/deep_learning/ch7/" class="post-title-link" itemprop="url">生成对抗网络面试题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-24 10:13:53" itemprop="dateModified" datetime="2024-03-24T10:13:53+08:00">2024-03-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">算法面试</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>生成对抗网络</h1>
<h2 id="7-1-GAN基本概念">7.1 GAN基本概念</h2>
<h3 id="7-1-1-如何通俗理解GAN？">7.1.1 如何通俗理解GAN？</h3>
<p>​	生成对抗网络(GAN, Generative adversarial network)自从2014年被Ian Goodfellow提出以来，掀起来了一股研究热潮。GAN由生成器和判别器组成，生成器负责生成样本，判别器负责判断生成器生成的样本是否为真。生成器要尽可能迷惑判别器，而判别器要尽可能区分生成器生成的样本和真实样本。</p>
<p>​	在GAN的原作[1]中，作者将生成器比喻为印假钞票的犯罪分子，判别器则类比为警察。犯罪分子努力让钞票看起来逼真，警察则不断提升对于假钞的辨识能力。二者互相博弈，随着时间的进行，都会越来越强。那么类比于图像生成任务，生成器不断生成尽可能逼真的假图像。判别器则判断图像是否是真实的图像，还是生成的图像，二者不断博弈优化。最终生成器生成的图像使得判别器完全无法判别真假。</p>
<h3 id="7-1-2-GAN的形式化表达">7.1.2 GAN的形式化表达</h3>
<p>​	上述例子只是简要介绍了一下GAN的思想，下面对于GAN做一个形式化的，更加具体的定义。通常情况下，无论是生成器还是判别器，我们都可以用神经网络来实现。那么，我们可以把通俗化的定义用下面这个模型来表示：<br>
<img src="7.1.png" alt="GAN网络结构"></p>
<p>​	上述模型左边是生成器G，其输入是 $z$ ，对于原始的GAN， $z$ 是由高斯分布随机采样得到的噪声。噪声 $z$ 通过生成器得到了生成的假样本。</p>
<p>​	生成的假样本与真实样本放到一起，被随机抽取送入到判别器D，由判别器去区分输入的样本是生成的假样本还是真实的样本。整个过程简单明了，生成对抗网络中的“生成对抗”主要体现在生成器和判别器之间的对抗。</p>
<h3 id="7-1-3-GAN的目标函数是什么？">7.1.3 GAN的目标函数是什么？</h3>
<p>​	对于上述神经网络模型，如果想要学习其参数，首先需要一个目标函数。GAN的目标函数定义如下：</p>
 $$
\mathop {\min }\limits_G \mathop {\max }\limits_D V(D,G) = {\rm E}_{x\sim{p_{data}(x)}}[\log D(x)] + {\rm E}_{z\sim{p_z}(z)}[\log (1 - D(G(z)))]
$$ 
<p>​	这个目标函数可以分为两个部分来理解：</p>
<p>​	第一部分：判别器的优化通过 $\mathop {\max}\limits_D V(D,G)$ 实现， $V(D,G)$ 为判别器的目标函数，其第一项 ${\rm E}_{x\sim{p_{data}(x)}}[\log D(x)]$ 表示对于从真实数据分布 中采用的样本 ,其被判别器判定为真实样本概率的数学期望。对于真实数据分布 中采样的样本，其预测为正样本的概率当然是越接近1越好。因此希望最大化这一项。第二项 ${\rm E}_{z\sim{p_z}(z)}[\log (1 - D(G(z)))]$ 表示：对于从噪声 $P_z(z)​$ 分布当中采样得到的样本，经过生成器生成之后得到的生成图片，然后送入判别器，其预测概率的负对数的期望，这个值自然是越大越好，这个值越大， 越接近0，也就代表判别器越好。</p>
<p>​	第二部分：生成器的优化通过 $\mathop {\min }\limits_G({\mathop {\max }\limits_D V(D,G)})$ 来实现。注意，生成器的目标不是 $\mathop {\min }\limits_GV(D,G)$ ，即生成器不是最小化判别器的目标函数，二是最小化判别器目标函数的最大值，判别器目标函数的最大值代表的是真实数据分布与生成数据分布的JS散度(详情可以参阅附录的推导)，JS散度可以度量分布的相似性，两个分布越接近，JS散度越小。</p>
<h3 id="7-1-4-GAN的目标函数和交叉熵有什么区别？">7.1.4 GAN的目标函数和交叉熵有什么区别？</h3>
<p>​	判别器目标函数写成离散形式即为:</p>
 $$
V(D,G)=-\frac{1}{m}\sum_{i=1}^{i=m}logD(x^i)-\frac{1}{m}\sum_{i=1}^{i=m}log(1-D(\tilde{x}^i))
$$ 
<p>​	可以看出，这个目标函数和交叉熵是一致的，即<strong>判别器的目标是最小化交叉熵损失，生成器的目标是最小化生成数据分布和真实数据分布的JS散度</strong>。</p>
<hr>
<p>[1]: Goodfellow, Ian, et al. “Generative adversarial nets.” Advances in neural information processing systems. 2014.</p>
<h3 id="7-1-5-GAN的Loss为什么降不下去？">7.1.5 GAN的Loss为什么降不下去？</h3>
<p>​	对于很多GAN的初学者在实践过程中可能会纳闷，为什么GAN的Loss一直降不下去。GAN到底什么时候才算收敛？其实，作为一个训练良好的GAN，其Loss就是降不下去的。衡量GAN是否训练好了，只能由人肉眼去看生成的图片质量是否好。不过，对于没有一个很好的评价是否收敛指标的问题，也有许多学者做了一些研究，后文提及的WGAN就提出了一种新的Loss设计方式，较好的解决了难以判断收敛性的问题。下面我们分析一下GAN的Loss为什么降不下去？<br>
​	对于判别器而言，GAN的Loss如下：</p>
 $$
\mathop {\min }\limits_G \mathop {\max }\limits_D V(D,G) = {\rm E}_{x\sim{p_{data}(x)}}[\log D(x)] + {\rm E}_{z\sim{p_z}(z)}[\log (1 - D(G(z)))]
$$ 
<p>​	从 $\mathop {\min }\limits_G \mathop {\max }\limits_D V(D,G)​$ 可以看出，生成器和判别器的目的相反，也就是说两个生成器网络和判别器网络互为对抗，此消彼长。不可能Loss一直降到一个收敛的状态。</p>
<ul>
<li>对于生成器，其Loss下降快，很有可能是判别器太弱，导致生成器很轻易的就&quot;愚弄&quot;了判别器。</li>
<li>对于判别器，其Loss下降快，意味着判别器很强，判别器很强则说明生成器生成的图像不够逼真，才使得判别器轻易判别，导致Loss下降很快。</li>
</ul>
<p>​	也就是说，无论是判别器，还是生成器。loss的高低不能代表生成器的好坏。一个好的GAN网络，其GAN Loss往往是不断波动的。</p>
<p>​	看到这里可能有点让人绝望，似乎判断模型是否收敛就只能看生成的图像质量了。实际上，后文探讨的WGAN，提出了一种新的loss度量方式，让我们可以通过一定的手段来判断模型是否收敛。</p>
<h3 id="7-1-6-生成式模型、判别式模型的区别？">7.1.6 生成式模型、判别式模型的区别？</h3>
<p>​	对于机器学习模型，我们可以根据模型对数据的建模方式将模型分为两大类，生成式模型和判别式模型。如果我们要训练一个关于猫狗分类的模型，对于判别式模型，只需要学习二者差异即可。比如说猫的体型会比狗小一点。而生成式模型则不一样，需要学习猫张什么样，狗张什么样。有了二者的长相以后，再根据长相去区分。具体而言：</p>
<ul>
<li>
<p>生成式模型：由数据学习联合概率分布P(X,Y), 然后由P(Y|X)=P(X,Y)/P(X)求出概率分布P(Y|X)作为预测的模型。该方法表示了给定输入X与产生输出Y的生成关系</p>
</li>
<li>
<p>判别式模型：由数据直接学习决策函数Y=f(X)或条件概率分布P(Y|X)作为预测模型，即判别模型。判别方法关心的是对于给定的输入X，应该预测什么样的输出Y。</p>
</li>
</ul>
<p>​	对于上述两种模型，从文字上理解起来似乎不太直观。我们举个例子来阐述一下，对于性别分类问题，分别用不同的模型来做：</p>
<p>​	1）如果用生成式模型：可以训练一个模型，学习输入人的特征X和性别Y的关系。比如现在有下面一批数据：</p>
<table>
<thead>
<tr>
<th>Y（性别）</th>
<th></th>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>X（特征）</td>
<td>0</td>
<td>1/4</td>
<td>3/4</td>
</tr>
<tr>
<td></td>
<td>1</td>
<td>3/4</td>
<td>1/4</td>
</tr>
</tbody>
</table>
<p>​	这个数据可以统计得到，即统计人的特征X=0,1….的时候，其类别为Y=0,1的概率。统计得到上述联合概率分布P(X, Y)后，可以学习一个模型，比如让二维高斯分布去拟合上述数据，这样就学习到了X，Y的联合分布。在预测时，如果我们希望给一个输入特征X，预测其类别，则需要通过贝叶斯公式得到条件概率分布才能进行推断：</p>
 $$
P(Y|X)={\frac{P(X,Y)}{P(X)}}={\frac{P(X,Y)}{P(X|Y)P(Y)}}
$$ 
<p>​	2）如果用判别式模型：可以训练一个模型，输入人的特征X，这些特征包括人的五官，穿衣风格，发型等。输出则是对于性别的判断概率，这个概率服从一个分布，分布的取值只有两个，要么男，要么女，记这个分布为Y。这个过程学习了一个条件概率分布P(Y|X)，即输入特征X的分布已知条件下，Y的概率分布。</p>
<p>​	显然，从上面的分析可以看出。判别式模型似乎要方便很多，因为生成式模型要学习一个X，Y的联合分布往往需要很多数据，而判别式模型需要的数据则相对少，因为判别式模型更关注输入特征的差异性。不过生成式既然使用了更多数据来生成联合分布，自然也能够提供更多的信息，现在有一个样本（X,Y）,其联合概率P（X,Y）经过计算特别小，那么可以认为这个样本是异常样本。这种模型可以用来做outlier detection。</p>
<h3 id="7-1-7-什么是mode-collapsing">7.1.7 什么是mode collapsing?</h3>
<p>​	某个模式(mode)出现大量重复样本，例如：<br>
<img src="model_collpsing.png" alt="model collapsing"><br>
​	上图左侧的蓝色五角星表示真实样本空间，黄色的是生成的。生成样本缺乏多样性，存在大量重复。比如上图右侧中，红框里面人物反复出现。</p>
<h3 id="7-1-8-如何解决mode-collapsing？">7.1.8 如何解决mode collapsing？</h3>
<p>方法一：<strong>针对目标函数的改进方法</strong></p>
<p>​	为了避免前面提到的由于优化maxmin导致mode跳来跳去的问题，UnrolledGAN采用修改生成器loss来解决。具体而言，UnrolledGAN在更新生成器时更新k次生成器，参考的Loss不是某一次的loss，是判别器后面k次迭代的loss。注意，判别器后面k次迭代不更新自己的参数，只计算loss用于更新生成器。这种方式使得生成器考虑到了后面k次判别器的变化情况，避免在不同mode之间切换导致的模式崩溃问题。此处务必和迭代k次生成器，然后迭代1次判别器区分开[8]。DRAGAN则引入博弈论中的无后悔算法，改造其loss以解决mode collapse问题[9]。前文所述的EBGAN则是加入VAE的重构误差以解决mode collapse。</p>
<p>方法二：<strong>针对网络结构的改进方法</strong></p>
<p>​	Multi agent diverse GAN(MAD-GAN)采用多个生成器，一个判别器以保障样本生成的多样性。具体结构如下：</p>
<p><img src="MAD_GAN.png" alt></p>
<p>​	相比于普通GAN，多了几个生成器，且在loss设计的时候，加入一个正则项。正则项使用余弦距离惩罚三个生成器生成样本的一致性。</p>
<p>​	MRGAN则添加了一个判别器来惩罚生成样本的mode collapse问题。具体结构如下：</p>
<p><img src="MRGAN.png" alt></p>
<p>​	输入样本 $x​$ 通过一个Encoder编码为隐变量 $E(x)​$ ，然后隐变量被Generator重构，训练时，Loss有三个。 $D_M​$ 和 $R​$ （重构误差）用于指导生成real-like的样本。而 $D_D​$ 则对 $E(x)​$ 和 $z​$ 生成的样本进行判别，显然二者生成样本都是fake samples，所以这个判别器主要用于判断生成的样本是否具有多样性，即是否出现mode collapse。</p>
<p>方法三：<strong>Mini-batch Discrimination</strong></p>
<p>​	Mini-batch discrimination在判别器的中间层建立一个mini-batch layer用于计算基于L1距离的样本统计量，通过建立该统计量，实现了一个batch内某个样本与其他样本有多接近。这个信息可以被判别器利用到，从而甄别出哪些缺乏多样性的样本。对生成器而言，则要试图生成具有多样性的样本。</p>
<h2 id="7-2-GAN的生成能力评价">7.2 GAN的生成能力评价</h2>
<h3 id="7-2-1-如何客观评价GAN的生成能力？">7.2.1 如何客观评价GAN的生成能力？</h3>
<p>​	最常见评价GAN的方法就是主观评价。主观评价需要花费大量人力物力，且存在以下问题：</p>
<ul>
<li>
<p>评价带有主管色彩，有些bad case没看到很容易造成误判</p>
</li>
<li>
<p>如果一个GAN过拟合了，那么生成的样本会非常真实，人类主观评价得分会非常高，可是这并不是一个好的GAN。</p>
</li>
</ul>
<p>因此，就有许多学者提出了GAN的客观评价方法。</p>
<h3 id="7-2-2-Inception-Score">7.2.2 Inception Score</h3>
<p>​	对于一个在ImageNet训练良好的GAN，其生成的样本丢给Inception网络进行测试的时候，得到的判别概率应该具有如下特性：</p>
<ul>
<li>对于同一个类别的图片，其输出的概率分布应该趋向于一个脉冲分布。可以保证生成样本的准确性。</li>
<li>对于所有类别，其输出的概率分布应该趋向于一个均匀分布，这样才不会出现mode dropping等，可以保证生成样本的多样性。</li>
</ul>
<p>​	因此，可以设计如下指标：</p>
 $$
IS(P_g)=e^{E_{x\sim P_g}[KL(p_M(y|x)\Vert{p_M(y)})]}
根据前面分析，如果是一个训练良好的GAN， {% raw%}$p_M(y|x)​${% endraw %} 趋近于脉冲分布， {% raw%}$p_M(y)​${% endraw %} 趋近于均匀分布。二者KL散度会很大。Inception Score自然就高。实际实验表明，Inception Score和人的主观判别趋向一致。IS的计算没有用到真实数据，具体值取决于模型M的选择
<p>$$<br>
​	根据前面分析，如果是一个训练良好的GAN， $p_M(y|x)$ 趋近于脉冲分布， $p_M(y)$ 趋近于均匀分布。二者KL散度会很大。Inception Score自然就高。实际实验表明，Inception Score和人的主观判别趋向一致。IS的计算没有用到真实数据，具体值取决于模型M的选择。</p>
<p>​	<strong>特点：可以一定程度上衡量生成样本的多样性和准确性，但是无法检测过拟合。Mode Score也是如此。不推荐在和ImageNet数据集差别比较大的数据上使用。</strong></p>
<h3 id="7-2-3-Mode-Score">7.2.3 Mode Score</h3>
<p>​	Mode Score作为Inception Score的改进版本，添加了关于生成样本和真实样本预测的概率分布相似性度量一项。具体公式如下：</p>
 $$
MS(P_g)=e^{E_{x\sim P_g}[KL(p_M(y|x)\Vert{p_M(y)})-KL(p_M(y)\Vert p_M(y^*))]}
$$ 
<h3 id="7-2-4-Kernel-MMD-Maximum-Mean-Discrepancy">7.2.4 Kernel MMD (Maximum Mean Discrepancy)</h3>
<p>计算公式如下：</p>
 $$
MMD^2(P_r,P_g)=E_{x_r\sim{P_r},x_g\sim{P_g}}[\lVert\Sigma_{i=1}^{n1}k(x_r)-\Sigma_{i=1}^{n2}k(x_g)\rVert]
$$ 
<p>​	对于Kernel MMD值的计算，首先需要选择一个核函数 $k$ ，这个核函数把样本映射到再生希尔伯特空间(Reproducing Kernel Hilbert Space, RKHS) ，RKHS相比于欧几里得空间有许多优点，对于函数内积的计算是完备的。将上述公式展开即可得到下面的计算公式：</p>
 $$
MMD^2(P_r,P_g)=E_{x_r,x_r{'}\sim{P_r},x_g,x_g{'}\sim{P_g}}[k(x_r,x_r{'})-2k(x_r,x_g)+k(x_g,x_g{'})]
$$ 
<p>MMD值越小，两个分布越接近。</p>
<p><strong>特点：可以一定程度上衡量模型生成图像的优劣性，计算代价小。推荐使用。</strong></p>
<h3 id="7-2-5-Wasserstein-distance">7.2.5 Wasserstein distance</h3>
<p>​	Wasserstein distance在最优传输问题中通常也叫做推土机距离。这个距离的介绍在WGAN中有详细讨论。公式如下：</p>
 $$
WD(P_r,P_g)=min_{\omega\in\mathbb{R}^{m\times n}}\Sigma_{i=1}^n\Sigma_{i=1}^m\omega_{ij}d(x_i^r,x_j^g)
$$ 
 $$
s.t. \Sigma_{i=1}^mw_{i,j}=p_r(x_i^r),  \forall i;\Sigma_{j=1}^nw_{i,j}=p_g(x_j^g),  \forall j
$$ 
<p>​	Wasserstein distance可以衡量两个分布之间的相似性。距离越小，分布越相似。</p>
<p><strong>特点：如果特征空间选择合适，会有一定的效果。但是计算复杂度为 $O(n^3)​$ 太高</strong></p>
<h3 id="7-2-6-Frechet-Inception-Distance-FID">7.2.6 Fréchet Inception Distance (FID)</h3>
<p>​	FID距离计算真实样本，生成样本在特征空间之间的距离。首先利用Inception网络来提取特征，然后使用高斯模型对特征空间进行建模。根据高斯模型的均值和协方差来进行距离计算。具体公式如下：</p>
 $$
FID(\mathbb P_r,\mathbb P_g)=\lVert\mu_r-\mu_g\rVert+Tr(C_r+C_g-2(C_rC_g)^{1/2})
 {% raw%}$\mu,C${% endraw %} 分别代表协方差和均值。
<p>$$</p>
 $\mu,C​$ 分别代表协方差和均值。
<p>​	<strong>特点：尽管只计算了特征空间的前两阶矩，但是鲁棒，且计算高效。</strong></p>
<h3 id="7-2-7-1-Nearest-Neighbor-classifier">7.2.7 1-Nearest Neighbor classifier</h3>
<p>​	使用留一法，结合1-NN分类器（别的也行）计算真实图片，生成图像的精度。如果二者接近，则精度接近50%，否则接近0%。对于GAN的评价问题，作者分别用正样本的分类精度，生成样本的分类精度去衡量生成样本的真实性，多样性。</p>
<ul>
<li>对于真实样本 $x_r$ ，进行1-NN分类的时候，如果生成的样本越真实。则真实样本空间 $\mathbb R$ 将被生成的样本 $x_g$ 包围。那么 $x_r$ 的精度会很低。</li>
<li>对于生成的样本 $x_g​$ ，进行1-NN分类的时候，如果生成的样本多样性不足。由于生成的样本聚在几个mode，则 $x_g​$ 很容易就和 $x_r​$ 区分，导致精度会很高。</li>
</ul>
<p><strong>特点：理想的度量指标，且可以检测过拟合。</strong></p>
<h3 id="7-2-8-其他评价方法">7.2.8 其他评价方法</h3>
<p>​	AIS，KDE方法也可以用于评价GAN，但这些方法不是model agnostic metrics。也就是说，这些评价指标的计算无法只利用：生成的样本，真实样本来计算。</p>
<h2 id="7-3-其他常见的生成式模型有哪些？">7.3 其他常见的生成式模型有哪些？</h2>
<h3 id="7-3-1-什么是自回归模型：pixelRNN与pixelCNN？">7.3.1 什么是自回归模型：pixelRNN与pixelCNN？</h3>
<p>​	自回归模型通过对图像数据的概率分布 $p_{data}(x)$ 进行显式建模，并利用极大似然估计优化模型。具体如下：</p>
 $$
p_{data}(x)=\prod_{i=1}^np(x_i|x_1,x_2,...,x_{i-1})
$$ 
<p>​	上述公式很好理解，给定 $x_1,x_2,...,x_{i-1}$ 条件下，所有 $p(x_i)$ 的概率乘起来就是图像数据的分布。如果使用RNN对上述依然关系建模，就是pixelRNN。如果使用CNN，则是pixelCNN。具体如下[5]：</p>
<p><img src="pixRNN.png" alt></p>
<p><img src="pixCNN.png" alt></p>
<p>​	显然，不论是对于pixelCNN还是pixelRNN，由于其像素值是一个个生成的，速度会很慢。语音领域大火的WaveNet就是一个典型的自回归模型。</p>
<h3 id="7-3-2-什么是VAE？">7.3.2 什么是VAE？</h3>
<p>​	PixelCNN/RNN定义了一个易于处理的密度函数，我们可以直接优化训练数据的似然；对于变分自编码器我们将定义一个不易处理的密度函数，通过附加的隐变量 $z$ 对密度函数进行建模。 VAE原理图如下[6]：</p>
<p><img src="VAE.png" alt></p>
<p>​	在VAE中，真实样本 $X$ 通过神经网络计算出均值方差（假设隐变量服从正太分布），然后通过采样得到采样变量 $Z$ 并进行重构。VAE和GAN均是学习了隐变量 $z$ 到真实数据分布的映射。但是和GAN不同的是：</p>
<ul>
<li>GAN的思路比较粗暴，使用一个判别器去度量分布转换模块（即生成器）生成分布与真实数据分布的距离。</li>
<li>VAE则没有那么直观，VAE通过约束隐变量 $z$ 服从标准正太分布以及重构数据实现了分布转换映射 $X=G(z)$</li>
</ul>
<p><strong>生成式模型对比</strong></p>
<ul>
<li>自回归模型通过对概率分布显式建模来生成数据</li>
<li>VAE和GAN均是：假设隐变量 $z$ 服从某种分布，并学习一个映射 $X=G(z)$ ，实现隐变量分布 $z$ 与真实数据分布 $p_{data}(x)$ 的转换。</li>
<li>GAN使用判别器去度量映射 $X=G(z)$ 的优劣，而VAE通过隐变量 $z$ 与标准正太分布的KL散度和重构误差去度量。</li>
</ul>
<h2 id="7-4-GAN的改进与优化">7.4 GAN的改进与优化</h2>
<h3 id="7-4-1-如何生成指定类型的图像——条件GAN">7.4.1 如何生成指定类型的图像——条件GAN</h3>
<p>​	条件生成对抗网络（CGAN, Conditional Generative Adversarial Networks）作为一个GAN的改进，其一定程度上解决了GAN生成结果的不确定性。如果在Mnist数据集上训练原始GAN，GAN生成的图像是完全不确定的，具体生成的是数字1，还是2，还是几，根本不可控。为了让生成的数字可控，我们可以把数据集做一个切分，把数字0~9的数据集分别拆分开训练9个模型，不过这样太麻烦了，也不现实。因为数据集拆分不仅仅是分类麻烦，更主要在于，每一个类别的样本少，拿去训练GAN很有可能导致欠拟合。因此，CGAN就应运而生了。我们先看一下CGAN的网络结构：<br>
<img src="CGAN%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png" alt="CGAN网络结构"><br>
​	从网络结构图可以看到，对于生成器Generator，其输入不仅仅是随机噪声的采样z，还有欲生成图像的标签信息。比如对于mnist数据生成，就是一个one-hot向量，某一维度为1则表示生成某个数字的图片。同样地，判别器的输入也包括样本的标签。这样就使得判别器和生成器可以学习到样本和标签之间的联系。Loss如下：</p>
 $$
\mathop {\min }\limits_G \mathop {\max }\limits_D V(D,G) = {\rm E}_{x\sim{p_{data}(x)}}[\log D(x|y)] + {\rm E}_{z\sim{p_z}(z)}[\log (1 - D(G(z|y)))]
$$ 
<p>​	Loss设计和原始GAN基本一致，只不过生成器，判别器的输入数据是一个条件分布。在具体编程实现时只需要对随机噪声采样z和输入条件y做一个级联即可。</p>
<h3 id="7-4-2-CNN与GAN——DCGAN">7.4.2 CNN与GAN——DCGAN</h3>
<p>​	前面我们聊的GAN都是基于简单的神经网络构建的。可是对于视觉问题，如果使用原始的基于DNN的GAN，则会出现许多问题。如果输入GAN的随机噪声为100维的随机噪声，输出图像为256x256大小。也就是说，要将100维的信息映射为65536维。如果单纯用DNN来实现，那么整个模型参数会非常巨大，而且学习难度很大（低维度映射到高维度需要添加许多信息）。因此，DCGAN就出现了。具体而言，DCGAN将传统GAN的生成器，判别器均采用GAN实现，且使用了一下tricks：</p>
<ul>
<li>将pooling层convolutions替代，其中，在discriminator上用strided convolutions替代，在generator上用fractional-strided convolutions替代。</li>
<li>在generator和discriminator上都使用batchnorm。</li>
<li>移除全连接层，global pooling增加了模型的稳定性，但伤害了收敛速度。</li>
<li>在generator的除了输出层外的所有层使用ReLU，输出层采用tanh。</li>
<li>在discriminator的所有层上使用LeakyReLU。</li>
</ul>
<p>网络结构图如下：<br>
<img src="DCGAN%E7%BB%93%E6%9E%84%E5%9B%BE.png" alt="CGAN网络结构图"></p>
<h3 id="7-4-3-如何理解GAN中的输入随机噪声？">7.4.3 如何理解GAN中的输入随机噪声？</h3>
<p>​	为了了解输入随机噪声每一个维度代表的含义，作者做了一个非常有趣的工作。即在隐空间上，假设知道哪几个变量控制着某个物体，那么僵这几个变量挡住是不是就可以将生成图片中的某个物体消失？论文中的实验是这样的：首先，生成150张图片，包括有窗户的和没有窗户的，然后使用一个逻辑斯底回归函数来进行分类，对于权重不为0的特征，认为它和窗户有关。将其挡住，得到新的生成图片，结果如下：<br>
<img src="7.4.3.png" alt="DCGAN输入噪声理解"><br>
此外，将几个输入噪声进行算数运算，可以得到语义上进行算数运算的非常有趣的结果。类似于word2vec。<br>
<img src="7.4.3-2.png" alt="DCGAN输入噪声算术运算"></p>
<h3 id="7-4-4-GAN为什么容易训练崩溃？">7.4.4 GAN为什么容易训练崩溃？</h3>
<p>​	所谓GAN的训练崩溃，指的是训练过程中，生成器和判别器存在一方压倒另一方的情况。<br>
GAN原始判别器的Loss在判别器达到最优的时候，等价于最小化生成分布与真实分布之间的JS散度，由于随机生成分布很难与真实分布有不可忽略的重叠以及JS散度的突变特性，使得生成器面临梯度消失的问题；可是如果不把判别器训练到最优，那么生成器优化的目标就失去了意义。因此需要我们小心的平衡二者，要把判别器训练的不好也不坏才行。否则就会出现训练崩溃，得不到想要的结果</p>
<h3 id="7-4-5-WGAN如何解决训练崩溃问题？">7.4.5 WGAN如何解决训练崩溃问题？</h3>
<p>​	WGAN作者提出了使用Wasserstein距离，以解决GAN网络训练过程难以判断收敛性的问题。Wasserstein距离定义如下:</p>
 $$
L={\rm E}_{x\sim{p_{data}}(x)}[f_w(x)] - {\rm E}_{x\sim{p_g}(x)}[f_w(x)]
$$ 
<p>通过最小化Wasserstein距离，得到了WGAN的Loss：</p>
<ul>
<li>WGAN生成器Loss： $- {\rm E}_{x\sim{p_g}(x)}[f_w(x)]​$</li>
<li>WGAN判别器Loss： $L=-{\rm E}_{x\sim{p_{data}}(x)}[f_w(x)] + {\rm E}_{x\sim{p_g}(x)}[f_w(x)]$</li>
</ul>
<p>从公式上GAN似乎总是让人摸不着头脑，在代码实现上来说，其实就以下几点：</p>
<ul>
<li>判别器最后一层去掉sigmoid</li>
<li>生成器和判别器的loss不取log</li>
<li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li>
</ul>
<h3 id="7-4-6-WGAN-GP：带有梯度正则的WGAN">7.4.6 WGAN-GP：带有梯度正则的WGAN</h3>
<p>​	实际实验过程发现，WGAN没有那么好用，主要原因在于WAGN进行梯度截断。梯度截断将导致判别网络趋向于一个二值网络，造成模型容量的下降。<br>
于是作者提出使用梯度惩罚来替代梯度裁剪。公式如下：</p>
 $$
L=-{\rm E}_{x\sim{p_{data}}(x)}[f_w(x)] + {\rm E}_{x\sim{p_g}(x)}[f_w(x)]+\lambda{\rm E}_{x\sim{p_x}(x)}[\lVert\nabla_x(D(x))\rVert_p-1]^2
由于上式是对每一个梯度进行惩罚，所以不适合使用BN，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择Layer Normalization。实际训练过程中，就可以通过Wasserstein距离来度量模型收敛程度了：
![Wass距离随迭代次数变化](ch7/Wass%E8%B7%9D%E7%A6%BB%E9%9A%8F%E8%BF%AD%E4%BB%A3%E6%AC%A1%E6%95%B0%E5%8F%98%E5%8C%96.png)
上图纵坐标是Wasserstein距离，横坐标是迭代次数。可以看出，随着迭代的进行，Wasserstein距离趋于收敛，生成图像也趋于稳定。
$$ 
<p>​	由于上式是对每一个梯度进行惩罚，所以不适合使用BN，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择Layer Normalization。实际训练过程中，就可以通过Wasserstein距离来度量模型收敛程度了：<br>
<img src="Wass%E8%B7%9D%E7%A6%BB%E9%9A%8F%E8%BF%AD%E4%BB%A3%E6%AC%A1%E6%95%B0%E5%8F%98%E5%8C%96.png" alt="Wass距离随迭代次数变化"><br>
​	上图纵坐标是Wasserstein距离，横坐标是迭代次数。可以看出，随着迭代的进行，Wasserstein距离趋于收敛，生成图像也趋于稳定。</p>
<h3 id="7-4-7-LSGAN">7.4.7 LSGAN</h3>
<p>​	LSGAN（Least Squares GAN）这篇文章主要针对标准GAN的稳定性和图片生成质量不高做了一个改进。作者将原始GAN的交叉熵损失采用最小二乘损失替代。LSGAN的Loss：</p>
 $$
\mathop{\min }\limits_DJ(D)=\mathop{\min}\limits_D[{\frac{1}{2}}{\rm E}_{x\sim{p_{data}}(x)}[D(x)-a]^2 + {\frac{1}{2}}{\rm E}_{z\sim{p_z}(z)}[D(G(z))-b]^2]
$$ 
 $$
\mathop{\min }\limits_GJ(G)=\mathop{\min}\limits_G{\frac{1}{2}}{\rm E}_{z\sim{p_z}(z)}[D(G(z))-c]^2
$$ 
<p>​	实际实现的时候非常简单，最后一层去掉sigmoid，并且计算Loss的时候用平方误差即可。之所以这么做，作者在原文给出了一张图:<br>
![LSGAN交叉熵与最小二乘损失对比图](ch7/lsgan loss compare.png)<br>
​	上面是作者给出的基于交叉熵损失以及最小二乘损失的Loss函数。横坐标代表Loss函数的输入，纵坐标代表输出的Loss值。可以看出，随着输入的增大，sigmoid交叉熵损失很快趋于0，容易导致梯度饱和问题。如果使用右边的Loss设计，则只在x=0点处饱和。因此使用LSGAN可以很好的解决交叉熵损失的问题。</p>
<h3 id="7-4-8-如何尽量避免GAN的训练崩溃问题？">7.4.8 如何尽量避免GAN的训练崩溃问题？</h3>
<ul>
<li>归一化图像输入到（-1，1）之间；Generator最后一层使用tanh激活函数</li>
<li>生成器的Loss采用：min (log 1-D)。因为原始的生成器Loss存在梯度消失问题；训练生成器的时候，考虑反转标签，real=fake, fake=real</li>
<li>不要在均匀分布上采样，应该在高斯分布上采样</li>
<li>一个Mini-batch里面必须只有正样本，或者负样本。不要混在一起；如果用不了Batch Norm，可以用Instance Norm</li>
<li>避免稀疏梯度，即少用ReLU，MaxPool。可以用LeakyReLU替代ReLU，下采样可以用Average Pooling或者Convolution + stride替代。上采样可以用PixelShuffle, ConvTranspose2d + stride</li>
<li>平滑标签或者给标签加噪声；平滑标签，即对于正样本，可以使用0.7-1.2的随机数替代；对于负样本，可以使用0-0.3的随机数替代。  给标签加噪声：即训练判别器的时候，随机翻转部分样本的标签。</li>
<li>如果可以，请用DCGAN或者混合模型：KL+GAN，VAE+GAN。</li>
<li>使用LSGAN，WGAN-GP</li>
<li>Generator使用Adam，Discriminator使用SGD</li>
<li>尽快发现错误；比如：判别器Loss为0，说明训练失败了；如果生成器Loss稳步下降，说明判别器没发挥作用</li>
<li>不要试着通过比较生成器，判别器Loss的大小来解决训练过程中的模型坍塌问题。比如：<br>
While Loss D &gt; Loss A:<br>
Train D<br>
While Loss A &gt; Loss D:<br>
Train A</li>
<li>如果有标签，请尽量利用标签信息来训练</li>
<li>给判别器的输入加一些噪声，给G的每一层加一些人工噪声。</li>
<li>多训练判别器，尤其是加了噪声的时候</li>
<li>对于生成器，在训练，测试的时候使用Dropout</li>
</ul>
<h2 id="7-3-GAN的应用（图像翻译）">7.3 GAN的应用（图像翻译）</h2>
<h3 id="7-3-1-什么是图像翻译？">7.3.1 什么是图像翻译？</h3>
<p>​	GAN作为一种强有力的生成模型，其应用十分广泛。最为常见的应用就是图像翻译。所谓图像翻译，指从一副图像到另一副图像的转换。可以类比机器翻译，一种语言转换为另一种语言。常见的图像翻译任务有：</p>
<ul>
<li>
<p>图像去噪</p>
</li>
<li>
<p>图像超分辨</p>
</li>
<li>
<p>图像补全</p>
</li>
<li>
<p>风格迁移</p>
</li>
<li>
<p>…</p>
<p>本节将介绍一个经典的图像翻译网络及其改进。图像翻译可以分为有监督图像翻译和无监督图像翻译：</p>
</li>
<li>
<p>有监督图像翻译：原始域与目标域存在一一对应数据</p>
</li>
<li>
<p>无监督图像翻译：原始域与目标域不存在一一对应数据</p>
</li>
</ul>
<h3 id="7-3-2-有监督图像翻译：pix2pix">7.3.2 有监督图像翻译：pix2pix</h3>
<p>​	在这篇paper里面，作者提出的框架十分简洁优雅（好用的算法总是简洁优雅的）。相比以往算法的大量专家知识，手工复杂的loss。这篇paper非常粗暴，使用CGAN处理了一系列的转换问题。下面是一些转换示例：<br>
<img src="pix2pix%E7%BB%93%E6%9E%9C%E7%A4%BA%E4%BE%8B.png" alt="pix2pix结果示例"></p>
<p>​	上面展示了许多有趣的结果，比如分割图 $\longrightarrow$ 街景图，边缘图 $\longrightarrow$ 真实图。对于第一次看到的时候还是很惊艳的，那么这个是怎么做到的呢？我们可以设想一下，如果是我们，我们自己会如何设计这个网络？</p>
<p><strong>直观的想法</strong>？</p>
<p>​	最直接的想法就是，设计一个CNN网络，直接建立输入-输出的映射，就像图像去噪问题一样。可是对于上面的问题，这样做会带来一个问题。<strong>生成图像质量不清晰。</strong></p>
<p>​	拿左上角的分割图 $\longrightarrow$ 街景图为例，语义分割图的每个标签比如“汽车”可能对应不同样式，颜色的汽车。那么模型学习到的会是所有不同汽车的评均，这样会造成模糊。<img src="pix2pix%E8%AF%AD%E4%B9%89%E5%9C%B0%E5%9B%BEL1loss%E7%BB%93%E6%9E%9C.png" alt="pix2pix语义地图L1loss结果"></p>
<p><strong>如何解决生成图像的模糊问题</strong>？</p>
<p>​	这里作者想了一个办法，即加入GAN的Loss去惩罚模型。GAN相比于传统生成式模型可以较好的生成高分辨率图片。思路也很简单，在上述直观想法的基础上加入一个判别器，判断输入图片是否是真实样本。模型示意图如下：<br>
<img src="pix2pix%E6%A8%A1%E5%9E%8B%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="pix2pix模型示意图"></p>
<p>​	上图模型和CGAN有所不同，但它是一个CGAN，只不过输入只有一个，这个输入就是条件信息。原始的CGAN需要输入随机噪声，以及条件。这里之所有没有输入噪声信息，是因为在实际实验中，如果输入噪声和条件，噪声往往被淹没在条件C当中，所以这里直接省去了。</p>
<h3 id="7-3-3-其他图像翻译的tricks">7.3.3  其他图像翻译的tricks</h3>
<p>从上面两点可以得到最终的Loss由两部分构成：</p>
<ul>
<li>
<p>输出和标签信息的L1 Loss。</p>
</li>
<li>
<p>GAN Loss</p>
</li>
<li>
<p>测试也使用Dropout，以使输出多样化</p>
 $$
  G^*=arg\mathop {\min }\limits_G \mathop {\max }\limits_D \Gamma_{cGAN}(G,D)+\lambda\Gamma_{L1}(G)
  $$ 
</li>
</ul>
<p>​	采用L1 Loss而不是L2 Loss的理由很简单，L1 Loss相比于L2 Loss保边缘（L2 Loss基于高斯先验，L1 Loss基于拉普拉斯先验）。</p>
<p>​	GAN Loss为LSGAN的最小二乘Loss，并使用PatchGAN(进一步保证生成图像的清晰度)。PatchGAN将图像换分成很多个Patch，并对每一个Patch使用判别器进行判别（实际代码实现有更取巧的办法），将所有Patch的Loss求平均作为最终的Loss。</p>
<h3 id="7-3-4-如何生成高分辨率图像和高分辨率视频？">7.3.4 如何生成高分辨率图像和高分辨率视频？</h3>
<p>​	pix2pix提出了一个通用的图像翻译框架。对于高分辨率的图像生成以及高分辨率的视频生成，则需要利用更好的网络结构以及更多的先验只是。pix2pixHD提出了一种多尺度的生成器以及判别器等方式从而生成高分辨率图像。Vid2Vid则在pix2pixHD的基础上利用光流，时序约束生成了高分辨率视频。</p>
<h3 id="7-3-5-有监督的图像翻译的缺点？">7.3.5 有监督的图像翻译的缺点？</h3>
<p>​	许多图像翻译算法如前面提及的pix2pix系列，需要一一对应的图像。可是在许多应用场景下，往往没有这种一一对应的强监督信息。比如说以下一些应用场景：<br>
<img src="CycleGAN%E7%BB%93%E6%9E%9C%E4%BE%8B%E5%AD%90.png" alt="CycleGAN结果例子"><br>
以第一排第一幅图为例，要找到这种一一配对的数据是不现实的。因此，无监督图像翻译算法就被引入了。</p>
<h3 id="7-3-6-无监督图像翻译：CycleGAN">7.3.6 无监督图像翻译：CycleGAN</h3>
<p><strong>模型结构</strong></p>
<p>​	总体思路如下，假设有两个域的数据，记为A，B。对于上图第一排第一幅图A域就是普通的马，B域就是斑马。由于A-&gt;B的转换缺乏监督信息，于是，作者提出采用如下方法进行转换：</p>
<blockquote>
<p>a.	A-&gt;fake_B-&gt;rec_A<br>
b.	B-&gt;fake_A-&gt;rec_B</p>
</blockquote>
<p>​	对于A域的所有图像，学习一个网络G_B，该网络可以生成B。对于B域的所有图像，也学习一个网络G_A，该网络可以生成G_B。</p>
<p>​	训练过程分成两步，首先对于A域的某张图像，送入G_B生成fake_B，然后对fake_B送入G_A，得到重构后的A图像rec_A。对于B域的某一张图像也是类似。重构后的图像rec_A/rec_B可以和原图A/B做均方误差，实现了有监督的训练。此处值得注意的是A-&gt;fake_B(B-&gt;fake_A)和fake_A-&gt;rec_B(fake_B-&gt;rec_A)的网络是一模一样的。下图是形象化的网络结构图：<br>
<img src="CycleGAN%E6%A8%A1%E5%9E%8B%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="CycleGAN模型示意图"><br>
​	cycleGAN的生成器采用U-Net，判别器采用LS-GAN。</p>
<p><strong>Loss设计</strong></p>
<p>​	总的Loss就是X域和Y域的GAN Loss，以及Cycle consistency loss：</p>
 $$
L(G,F,D_X,D_Y)=L_{GAN}(G,D_Y,X,Y)+L_{GAN}(F,D_X,Y,X)+\lambda L_{cycle}(G,F)
$$ 
<p>整个过程End to end训练，效果非常惊艳，利用这一框架可以完成非常多有趣的任务</p>
<h3 id="7-3-7-多领域的无监督图像翻译：StarGAN">7.3.7 多领域的无监督图像翻译：StarGAN</h3>
<p>cycleGAN模型较好的解决了无监督图像转换问题，可是这种单一域的图像转换还存在一些问题：</p>
<ul>
<li>
<p>要针对每一个域训练一个模型，效率太低。举例来说，我希望可以将橘子转换为红苹果和青苹果。对于cycleGAN而言，需要针对红苹果，青苹果分别训练一个模型。</p>
</li>
<li>
<p>对于每一个域都需要搜集大量数据，太麻烦。还是以橘子转换为红苹果和青苹果为例。不管是红苹果还是青苹果，都是苹果，只是颜色不一样而已。这两个任务信息是可以共享的，没必要分别训练两个模型。而且针对红苹果，青苹果分别取搜集大量数据太费事。</p>
<p>starGAN则提出了一个多领域的无监督图像翻译框架，实现了多个领域的图像转换，且对于不同领域的数据可以混合在一起训练，提高了数据利用率</p>
</li>
</ul>
<h2 id="7-4-GAN的应用（文本生成）">7.4 GAN的应用（文本生成）</h2>
<h3 id="7-4-1-GAN为什么不适合文本任务？">7.4.1 GAN为什么不适合文本任务？</h3>
<p>​	GAN在2014年被提出之后，在图像生成领域取得了广泛的研究应用。然后在文本领域却一直没有很惊艳的效果。主要在于文本数据是离散数据，而GAN在应用于离散数据时存在以下几个问题：</p>
<ul>
<li>GAN的生成器梯度来源于判别器对于正负样本的判别。然而，对于文本生成问题，RNN输出的是一个概率序列，然后取argmax。这会导致生成器Loss不可导。还可以站在另一个角度理解，由于是argmax，所以参数更新一点点并不会改变argmax的结果，这也使得GAN不适合离散数据。</li>
<li>GAN只能评估整个序列的loss，但是无法评估半句话，或者是当前生成单词对后续结果好坏的影响。</li>
<li>如果不加argmax，那么由于生成器生成的都是浮点数值，而ground truth都是one-hot encoding，那么判别器只要判别生成的结果是不是0/1序列组成的就可以了。这容易导致训练崩溃。</li>
</ul>
<h3 id="7-4-2-seqGAN用于文本生成">7.4.2 seqGAN用于文本生成</h3>
<p>​	seqGAN在GAN的框架下，结合强化学习来做文本生成。 模型示意图如下：</p>
<p><img src="seqGAN%E6%A8%A1%E5%9E%8B.png" alt="seqGAN模型"><br>
在文本生成任务，seqGAN相比较于普通GAN区别在以下几点：</p>
<ul>
<li>生成器不取argmax。</li>
<li>每生成一个单词，则根据当前的词语序列进行蒙特卡洛采样生成完成的句子。然后将句子送入判别器计算reward。</li>
<li>根据得到的reward进行策略梯度下降优化模型。</li>
</ul>
<h2 id="7-5-GAN在其他领域的应用">7.5 GAN在其他领域的应用</h2>
<h3 id="7-5-1-数据增广">7.5.1 数据增广</h3>
<p>​	GAN的良好生成特性近年来也开始被用于数据增广。以行人重识别为例，有许多GAN用于数据增广的工作[1-4]。行人重识别问题一个难点在于不同摄像头下拍摄的人物环境，角度差别非常大，导致存在较大的Domain gap。因此，可以考虑使用GAN来产生不同摄像头下的数据进行数据增广。以论文[1]为例，本篇paper提出了一个cycleGAN用于数据增广的方法。具体模型结构如下：</p>
<p><img src="cycleGAN%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF.png" alt="cycleGAN数据增广"></p>
<p>​	对于每一对摄像头都训练一个cycleGAN，这样就可以实现将一个摄像头下的数据转换成另一个摄像头下的数据，但是内容（人物）保持不变。<br>
在CVPR19中，[9]进一步提升了图像的生成质量，进行了“淘宝换衣”式的高质量图像生成（如下图），提供了更高质量的行人训练数据。</p>
<p><img src="https://github.com/NVlabs/DG-Net/raw/master/NxN.jpg" alt="DG-Net数据增广"></p>
<h3 id="7-5-2-图像超分辨与图像补全">7.5.2 图像超分辨与图像补全</h3>
<p>​	图像超分辨与补全均可以作为图像翻译问题，该类问题的处理办法也大都是训练一个端到端的网络，输入是原始图片，输出是超分辨率后的图片，或者是补全后的图片。文献[5]利用GAN作为判别器，使得超分辨率模型输出的图片更加清晰，更符合人眼主管感受。日本早稻田大学研究人员[6]提出一种全局+局部一致性的GAN实现图像补全，使得修复后的图像不仅细节清晰，且具有整体一致性。</p>
<h3 id="7-5-3-语音领域">7.5.3 语音领域</h3>
<p>​	相比于图像领域遍地开花，GAN在语音领域则应用相对少了很多。这里零碎的找一些GAN在语音领域进行应用的例子作为介绍。文献[7]提出了一种音频去噪的SEGAN，缓解了传统方法支持噪声种类稀少，泛化能力不强的问题。Donahue利用GAN进行语音增强，提升了ASR系统的识别率。</p>
<h2 id="参考文献">参考文献</h2>
<p>[1] Zheng Z , Zheng L , Yang Y . Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro[C]// 2017 IEEE International Conference on Computer Vision (ICCV). IEEE Computer Society, 2017.</p>
<p>[2]  Zhong Z ,  Zheng L ,  Zheng Z , et al. Camera Style Adaptation for Person Re-identification[J].  2017.</p>
<p>[3]  Deng W ,  Zheng L ,  Ye Q , et al. Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification[J].  2017.</p>
<p>[4]  Wei L ,  Zhang S ,  Gao W , et al. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification[J].  CVPR, 2017.</p>
<p>[5]  Ledig C ,  Theis L ,  Huszar F , et al. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network[J].  CVPR, 2016.</p>
<p>[6]  Iizuka S ,  Simo-Serra E ,  Ishikawa H . Globally and locally consistent image completion[J]. ACM Transactions on Graphics, 2017, 36(4):1-14.</p>
<p>[7] Pascual S , Bonafonte A , Serrà, Joan. SEGAN: Speech Enhancement Generative Adversarial Network[J]. 2017.</p>
<p>[8]  Donahue C ,  Li B ,  Prabhavalkar R . Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition[J].  2017.</p>
<p>[9] Zheng, Z., Yang, X., Yu, Z., Zheng, L., Yang, Y., &amp; Kautz, J. Joint discriminative and generative learning for person re-identification. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)[C]. 2019.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/leetcode/%E4%BA%8C%E5%88%86%E5%92%8C%E6%9F%A5%E6%89%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/leetcode/%E4%BA%8C%E5%88%86%E5%92%8C%E6%9F%A5%E6%89%BE/" class="post-title-link" itemprop="url">二分和查找</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-20 22:47:23" itemprop="dateModified" datetime="2024-03-20T22:47:23+08:00">2024-03-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LeetCode/" itemprop="url" rel="index"><span itemprop="name">LeetCode</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LeetCode/%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/" itemprop="url" rel="index"><span itemprop="name">题目汇总</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>二分和查找类</h1>
<h2 id="基础思路">基础思路</h2>
<p>在二分查找中，需要注意的是边界的问题，其中很多小的点，很容易出现问题，一般的解法如下所示：注意哈，写的时候，要hihg = len(nums) - 1, 不要忘了写成len(nums)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找具体的数值</span></span><br><span class="line"><span class="comment"># 方案1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search</span>(<span class="params">nums, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(nums) - <span class="number">1</span> </span><br><span class="line">    <span class="keyword">while</span> low &lt;= high: <span class="comment">#注意</span></span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span> <span class="comment"># 注意</span></span><br><span class="line">        <span class="comment"># 不要使用else,要使用elif，不然有些情况会报错</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid + <span class="number">1</span> <span class="comment"># 注意</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            high = mid - <span class="number">1</span> <span class="comment"># 这里</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target: </span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line"> <span class="comment"># 方案2</span></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">binary_search</span>(<span class="params">nums, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(nums)  <span class="comment"># 这里不能改成-1，不然有些值查不到</span></span><br><span class="line">    <span class="keyword">while</span> low &lt; high: <span class="comment">#注意</span></span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span> <span class="comment"># 注意</span></span><br><span class="line">        <span class="comment"># 不要使用else,要使用elif，不然有些情况会报错</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid + <span class="number">1</span> <span class="comment"># 注意</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            high = mid <span class="comment"># 这里</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target: </span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找左边界</span></span><br><span class="line"><span class="comment"># 方案1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">left_bound</span>(<span class="params">nums, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">    ans = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target: <span class="comment"># 右边界往里</span></span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid]==target:</span><br><span class="line">            ans = mid</span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> ans</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 方案2</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">left_bound</span>(<span class="params">nums, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(nums)</span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target: <span class="comment"># 右边界往里</span></span><br><span class="line">            high = mid</span><br><span class="line">        <span class="keyword">elif</span> nums[mid]==target:</span><br><span class="line">            ans = mid</span><br><span class="line">            high = mid</span><br><span class="line">    <span class="keyword">return</span> ans</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找右边界</span></span><br><span class="line"><span class="comment"># 方案1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">right_bound</span>(<span class="params">nums, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">    ans = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt;  target: <span class="comment"># 左边界往里</span></span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target: </span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">            ans = mid</span><br><span class="line">    <span class="keyword">return</span> ans</span><br><span class="line"><span class="comment"># 方案2</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">right_bound</span>(<span class="params">nums, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(nums)</span><br><span class="line">    ans = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target: <span class="comment"># 左边界往里</span></span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            high = mid</span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">            ans = mid</span><br><span class="line">    <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nums = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">target = <span class="number">4</span></span><br><span class="line"><span class="built_in">print</span>(binary_search(nums, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(left_bound(nums, target))</span><br><span class="line"><span class="built_in">print</span>(left_bound2(nums, target))</span><br><span class="line"><span class="built_in">print</span>(right_bound(nums, target))</span><br><span class="line"><span class="built_in">print</span>(right_bound2(nums, target))</span><br></pre></td></tr></table></figure>
<p>建议方案1，另外看如下的两个例子，明确下为啥low不能=mid，而需要=mid+1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">search_correct</span>(<span class="params">nums, target</span>):</span><br><span class="line">    low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = mid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_error</span>(<span class="params">nums, target</span>):</span><br><span class="line">    low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid <span class="comment"># 这里不更新会导致mid在经过（low+high）取值后，一直停留在mid</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nums = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">target = <span class="number">8</span></span><br><span class="line"><span class="built_in">print</span>(search_correct(nums, target))  <span class="comment"># None</span></span><br><span class="line"><span class="built_in">print</span>(search_error(nums, target))  <span class="comment"># 循环</span></span><br><span class="line">target = -<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(search_correct(nums, target))  <span class="comment"># None</span></span><br><span class="line"><span class="built_in">print</span>(search_error(nums, target))  <span class="comment"># None</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看到不同的参数设置，会导致不同的运行结果。</p>
<p>为什么二分查找中用一些特别的+1或者&lt;=,看个例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search</span>(<span class="params">nums, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(nums) </span><br><span class="line">    <span class="keyword">while</span> low &lt;= high: <span class="comment">#注意</span></span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span> <span class="comment"># 注意</span></span><br><span class="line">        <span class="comment"># 不要使用else,要使用elif，不然有些情况会报错</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid  <span class="comment"># 注意</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &gt; target:</span><br><span class="line">            high = mid <span class="comment"># 这里</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == target: </span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">binary_search(【<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>】, <span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<p>上面自己调一下就知道了，  从参考的文献来看的话，建议使用左闭右闭的的方式，也就是《=的基础方式的。但是对于一些需要旋转数组这些题，因为不知道要找的数最终是什么，所以一般用low&lt;high，  然后结合low=mid+1, high=mid来 实现。对于一些寻找旋转数组中值的情况，因为是确切找值的，所以的话，一般用while low&lt;=high, 然后结合 low=mid+1, high=mid-1 来实现。</p>
<p>参考如下：<br>
[1] <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38235017/article/details/115177238?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.pc_relevant_paycolumn_v2&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.pc_relevant_paycolumn_v2&amp;utm_relevant_index=1">https://blog.csdn.net/qq_38235017/article/details/115177238?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_paycolumn_v2&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_paycolumn_v2&amp;utm_relevant_index=1</a><br>
[2] <a target="_blank" rel="noopener" href="https://www.cnblogs.com/mxj961116/p/11945444.html">https://www.cnblogs.com/mxj961116/p/11945444.html</a><br>
[3] <a target="_blank" rel="noopener" href="https://leetcode.cn/circle/discuss/ooxfo8/">https://leetcode.cn/circle/discuss/ooxfo8/</a> 这里说的很好，说道了上面讲述的为何要&lt;问题，以及循环的问题。</p>
<h2 id="单数组">单数组</h2>
<h3 id="排序数组中查找元素的第一个和最后一个位置-34">排序数组中查找元素的第一个和最后一个位置[34]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/description/">https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">searchRange</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">lower_bound</span>(<span class="params">nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">            left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span>  <span class="comment"># 闭区间 [left, right]</span></span><br><span class="line">            <span class="keyword">while</span> left &lt;= right:  <span class="comment"># 区间不为空</span></span><br><span class="line">                mid = (left + right) // <span class="number">2</span></span><br><span class="line">                <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">                    left = mid + <span class="number">1</span>  <span class="comment"># 范围缩小到 [mid+1, right]</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    right = mid - <span class="number">1</span>  <span class="comment"># 范围缩小到 [left, mid-1]</span></span><br><span class="line">            <span class="keyword">return</span> left  <span class="comment"># 或者 right+1</span></span><br><span class="line"></span><br><span class="line">        start = lower_bound(nums, target)  <span class="comment"># 选择其中一种写法即可</span></span><br><span class="line">        <span class="keyword">if</span> start == <span class="built_in">len</span>(nums) <span class="keyword">or</span> nums[start] != target:</span><br><span class="line">            <span class="keyword">return</span> [-<span class="number">1</span>, -<span class="number">1</span>]        </span><br><span class="line">        end = lower_bound(nums, target + <span class="number">1</span>) - <span class="number">1</span> <span class="comment"># 如果 start 存在，那么 end 必定存在</span></span><br><span class="line">        <span class="keyword">return</span> [start, end]</span><br></pre></td></tr></table></figure>
<h3 id="H-指数-274">H 指数[274]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/h-index/description/?envType=study-plan-v2&amp;envId=top-interview-150">https://leetcode.cn/problems/h-index/description/?envType=study-plan-v2&amp;envId=top-interview-150</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hIndex</span>(<span class="params">self, citations: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        citations.sort()</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(citations)+<span class="number">1</span>):</span><br><span class="line">            index = bisect.bisect_left(citations, i)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(citations) - index &gt;= i:</span><br><span class="line">                res = <span class="built_in">max</span>(res, i)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="山脉数组的峰顶索引">山脉数组的峰顶索引</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/peak-index-in-a-mountain-array/%EF%BC%8C">https://leetcode-cn.com/problems/peak-index-in-a-mountain-array/，</a> 题解入下面的那道题</p>
<h3 id="寻找峰值">寻找峰值</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/find-peak-element/">https://leetcode-cn.com/problems/find-peak-element/</a> ，题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不知道确切的值是啥，所以用low&lt;high，当然这里也可以用都闭合的方式。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findPeakElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        nums = [-<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)] + nums + [-<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)] <span class="comment"># 这里加一下，方便操作</span></span><br><span class="line">        low = <span class="number">0</span></span><br><span class="line">        high = <span class="built_in">len</span>(nums) - <span class="number">1</span> <span class="comment"># attention</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            mid = low + (high - low)//<span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid+<span class="number">1</span>] &gt;= nums[mid]: <span class="comment"># attention</span></span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid</span><br><span class="line">        <span class="keyword">return</span> low - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="有序数组中的单一元素-540">有序数组中的单一元素[540]</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不知道这个数是啥，用low&lt;high</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">singleNonDuplicate</span>(<span class="params">nums</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        mid = (high + low) &gt;&gt; <span class="number">1</span></span><br><span class="line">        halvesAreEven = (high - mid) % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] == nums[mid + <span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">if</span> halvesAreEven:</span><br><span class="line">                low = low + <span class="number">2</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid - <span class="number">1</span> <span class="comment"># 注意不是high=mid</span></span><br><span class="line">        <span class="keyword">elif</span> nums[mid] == nums[mid - <span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">if</span> halvesAreEven:</span><br><span class="line">                high = mid - <span class="number">2</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> nums[mid]</span><br><span class="line">    <span class="keyword">return</span> nums[low]</span><br></pre></td></tr></table></figure>
<h3 id="寻找旋转排序数组中的最小值-153">寻找旋转排序数组中的最小值[153]</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  不知道最小数是啥，用low&lt;high</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMin</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            pivot = low + (high - low) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[pivot] &lt; nums[high]:<span class="comment"># 这里high改为len(nums)-1也可以</span></span><br><span class="line">                high = pivot</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                low = pivot + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> nums[low]</span><br></pre></td></tr></table></figure>
<h3 id="寻找旋转排序数组中的最小值-II-154">寻找旋转排序数组中的最小值 II[154]</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  不知道最小数是啥，用low&lt;high</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMin</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(nums)&gt;<span class="number">1</span> <span class="keyword">and</span> nums[<span class="number">0</span>]==nums[-<span class="number">1</span>]: <span class="comment"># 注意</span></span><br><span class="line">            nums.pop()</span><br><span class="line">        low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            pivot = low + (high - low) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[pivot] &lt; nums[high]:</span><br><span class="line">                high = pivot</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                low = pivot + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> nums[low]</span><br></pre></td></tr></table></figure>
<h3 id="搜索旋转排序数组-33">搜索旋转排序数组[33]</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 知道搜的是啥，用low&lt;=high</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        l, r = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt;= r:</span><br><span class="line">            mid = (l + r) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] == target:</span><br><span class="line">                <span class="keyword">return</span> mid</span><br><span class="line">            <span class="keyword">if</span> nums[<span class="number">0</span>] &lt;= nums[mid]: <span class="comment"># 这里是&gt;= 不是&gt;</span></span><br><span class="line">                <span class="keyword">if</span> nums[<span class="number">0</span>] &lt;= target &lt; nums[mid]: <span class="comment"># 注意：是小于不是小于等于</span></span><br><span class="line">                    r = mid - <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    l = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> nums[mid] &lt; target &lt;= nums[<span class="built_in">len</span>(nums) - <span class="number">1</span>]: <span class="comment"># 左边是开，右边是闭</span></span><br><span class="line">                    l = mid + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    r = mid -<span class="number">1</span> </span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="搜索旋转排序数组II-81">搜索旋转排序数组II[81]</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 知道搜的是啥，用low&lt;=high</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(nums)&gt;<span class="number">1</span> <span class="keyword">and</span> nums[<span class="number">0</span>]==nums[-<span class="number">1</span>]: <span class="comment"># 加这段代码处理一下重复的问题</span></span><br><span class="line">            nums.pop()</span><br><span class="line">        low = <span class="number">0</span></span><br><span class="line">        high = <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">            mid = low + (high -low)//<span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid]==target:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span>           </span><br><span class="line">            <span class="keyword">if</span> nums[mid] &gt;= nums[<span class="number">0</span>]:</span><br><span class="line">                <span class="keyword">if</span> nums[<span class="number">0</span>]&lt;=target&lt;nums[mid]:</span><br><span class="line">                    high = mid - <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    low = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> nums[mid]&lt;nums[<span class="number">0</span>]:</span><br><span class="line">                <span class="keyword">if</span> nums[mid]&lt;target&lt;=nums[-<span class="number">1</span>]:</span><br><span class="line">                    low =mid+<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    high=mid-<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="数组中的k-diff数对-532">数组中的k-diff数对[532]</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findPairs</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">        nums_set = Counter(nums)</span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">sum</span>([i&gt;<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> nums_set.values()]) <span class="comment"># 注意这里</span></span><br><span class="line"></span><br><span class="line">        nums.sort()</span><br><span class="line">        diff_min, diff_max = nums[<span class="number">0</span>], nums[-<span class="number">1</span>]</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> nums_set.keys():</span><br><span class="line">            index = bisect.bisect_left(nums, j+k)</span><br><span class="line">            <span class="keyword">if</span> index &lt; <span class="built_in">len</span>(nums):</span><br><span class="line">                <span class="keyword">if</span> nums[index] == j+k:</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> cnt</span><br></pre></td></tr></table></figure>
<p>注意，对于k=0是需要单独计算的，因此需要分开。</p>
<h3 id="数组中的逆序对-LCR-170">数组中的逆序对[LCR 170]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/shu-zu-zhong-de-ni-xu-dui-lcof/">https://leetcode.cn/problems/shu-zu-zhong-de-ni-xu-dui-lcof/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reversePairs</span>(<span class="params">self, record: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        record = record[::-<span class="number">1</span>] <span class="comment"># 这步很重要</span></span><br><span class="line">        res = []</span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> record:</span><br><span class="line">            index = bisect.bisect_left(res, i)</span><br><span class="line">            res[index:index] = [i] <span class="comment"># bisect.insort(res, i)也可以</span></span><br><span class="line">            s = s + index</span><br><span class="line">        <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p>通过二分查找排序的方法来做的话，更快。看下面这道题是一样的</p>
<h3 id="翻转对-493">翻转对[493]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/reverse-pairs/">https://leetcode-cn.com/problems/reverse-pairs/</a> 和上面的题目是一样的，题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reversePairs</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        nums = nums[::-<span class="number">1</span>]</span><br><span class="line">        res = []</span><br><span class="line">        sums = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            index = bisect.bisect_left(res, i) <span class="comment"># 查看插入的位置，就是数量</span></span><br><span class="line">            sums = sums + index <span class="comment"># 累加</span></span><br><span class="line">            index2 = bisect.bisect_left(res, <span class="number">2</span>*i) <span class="comment"># 实际要对res进行处理，加入2*i这个数</span></span><br><span class="line">            res[index2:index2] = [<span class="number">2</span>*i] <span class="comment"># 加入数</span></span><br><span class="line">        <span class="keyword">return</span> sums</span><br></pre></td></tr></table></figure>
<h3 id="计算右侧小于当前元素的个数-315">计算右侧小于当前元素的个数[315]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/count-of-smaller-numbers-after-self/">https://leetcode-cn.com/problems/count-of-smaller-numbers-after-self/</a> 一看题目是没法使用二分查找的，但是只要转换下思路就可以了。<br>
题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countSmaller</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        data = []</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums[::-<span class="number">1</span>]:</span><br><span class="line">            index = bisect.bisect_left(data, i)</span><br><span class="line">            data[index:index] = [i] <span class="comment">#bisect.insort(data, i)也行</span></span><br><span class="line">            res.append(index)</span><br><span class="line">        <span class="keyword">return</span> res[::-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="马戏团人塔-面试题-17-08">马戏团人塔[面试题 17.08]</h3>
<p>这道题其实就是求最长上升子序列而已，题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/circus-tower-lcci/">https://leetcode-cn.com/problems/circus-tower-lcci/</a>  题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解法1</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">import</span> bisect</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bestSeqAtIndex</span>(<span class="params">self, height: <span class="type">List</span>[<span class="built_in">int</span>], weight: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:       </span><br><span class="line">       dp=[]</span><br><span class="line">       <span class="keyword">for</span> a,b <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">zip</span>(height,weight),key = <span class="keyword">lambda</span> x:[x[<span class="number">0</span>],-x[<span class="number">1</span>]]):</span><br><span class="line">           pos = bisect.bisect_left(dp,b)</span><br><span class="line">           dp[pos:pos+<span class="number">1</span>] = [b]</span><br><span class="line">       <span class="keyword">return</span> <span class="built_in">len</span>(dp)</span><br><span class="line"><span class="comment"># 解法2</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bestSeqAtIndex</span>(<span class="params">self, height: <span class="type">List</span>[<span class="built_in">int</span>], weight: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        hw = <span class="built_in">list</span>(<span class="built_in">zip</span>(height, weight))</span><br><span class="line">        hw.sort(key=<span class="keyword">lambda</span> x:(x[<span class="number">0</span>],-x[<span class="number">1</span>]))</span><br><span class="line">        v = [j[<span class="number">1</span>] <span class="keyword">for</span> j <span class="keyword">in</span> hw]</span><br><span class="line">        stk = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> v:</span><br><span class="line">            <span class="keyword">if</span> stk <span class="keyword">and</span> x &lt;= stk[-<span class="number">1</span>]:</span><br><span class="line">                idx = bisect_left(stk, x)</span><br><span class="line">                stk[idx] = x</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stk.append(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(stk)</span><br><span class="line"><span class="comment"># 解法3，DP</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">import</span> bisect</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bestSeqAtIndex</span>(<span class="params">self, height: <span class="type">List</span>[<span class="built_in">int</span>], weight: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:       </span><br><span class="line">        t = <span class="built_in">sorted</span>(<span class="built_in">zip</span>(height, weight),key=<span class="keyword">lambda</span> x:(x[<span class="number">0</span>],-x[<span class="number">1</span>]))</span><br><span class="line">        new_height = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> t]</span><br><span class="line">        new_weight = [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> t]</span><br><span class="line">        dp = [<span class="number">1</span>] * <span class="built_in">len</span>(new_weight)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(dp)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">                <span class="keyword">if</span> new_weight[i] &gt; new_weight[j]:</span><br><span class="line">                    dp[i] = <span class="built_in">max</span>(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></table></figure>
<p>注意这里是[pos:pos+1]和上面的是不一样的</p>
<h3 id="绝对差值和-1818">绝对差值和[1818]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/minimum-absolute-sum-difference/description/">https://leetcode.cn/problems/minimum-absolute-sum-difference/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minAbsoluteSumDiff</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        diff = <span class="built_in">sum</span>(<span class="built_in">abs</span>(nums1[i] - nums2[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums1)))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> diff: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        ans = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        nums1_sort = <span class="built_in">sorted</span>(nums1)</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        <span class="keyword">for</span> i, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums2):</span><br><span class="line">            idx = bisect.bisect_left(nums1_sort, num)</span><br><span class="line">            <span class="keyword">if</span> idx == <span class="built_in">len</span>(nums1):</span><br><span class="line">                ans = <span class="built_in">min</span>(ans, diff - <span class="built_in">abs</span>(nums1[i] - nums2[i]) + <span class="built_in">abs</span>(nums1_sort[idx-<span class="number">1</span>] - nums2[i]))</span><br><span class="line">            <span class="keyword">if</span> idx == <span class="number">0</span>:</span><br><span class="line">                ans = <span class="built_in">min</span>(ans, diff - <span class="built_in">abs</span>(nums1[i] - nums2[i]) + <span class="built_in">abs</span>(nums1_sort[idx] - nums2[i]))</span><br><span class="line">            <span class="keyword">if</span> idx&lt;<span class="built_in">len</span>(nums1) <span class="keyword">and</span> idx&gt;<span class="number">0</span>:</span><br><span class="line">                ans = <span class="built_in">min</span>(ans, diff - <span class="built_in">abs</span>(nums1[i] - nums2[i]) + <span class="built_in">abs</span>(nums1_sort[idx-<span class="number">1</span>] - nums2[i]))</span><br><span class="line">                ans = <span class="built_in">min</span>(ans, diff - <span class="built_in">abs</span>(nums1[i] - nums2[i]) + <span class="built_in">abs</span>(nums1_sort[idx] - nums2[i]))</span><br><span class="line">        <span class="keyword">return</span> ans%(<span class="number">10</span>**<span class="number">9</span>+<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<p>简化为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minAbsoluteSumDiff</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        diff = <span class="built_in">sum</span>(<span class="built_in">abs</span>(nums1[i] - nums2[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums1)))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> diff: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        ans = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        nums1_sort = <span class="built_in">sorted</span>(nums1)</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        <span class="keyword">for</span> i, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums2):</span><br><span class="line">            idx = bisect.bisect_left(nums1_sort, num)</span><br><span class="line">            <span class="keyword">if</span> idx:</span><br><span class="line">                ans = <span class="built_in">min</span>(ans, diff - <span class="built_in">abs</span>(nums1[i] - nums2[i]) + <span class="built_in">abs</span>(nums1_sort[idx-<span class="number">1</span>] - nums2[i]))</span><br><span class="line">            <span class="keyword">if</span> idx&lt;<span class="built_in">len</span>(nums1):</span><br><span class="line">                ans = <span class="built_in">min</span>(ans, diff - <span class="built_in">abs</span>(nums1[i] - nums2[i]) + <span class="built_in">abs</span>(nums1_sort[idx] - nums2[i]))</span><br><span class="line">        <span class="keyword">return</span> ans%(<span class="number">10</span>**<span class="number">9</span>+<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<h3 id="最小差-面试题-16-06">最小差[面试题 16.06]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/smallest-difference-lcci/">https://leetcode-cn.com/problems/smallest-difference-lcci/</a> 这道题和<br>
绝对差值和 一样，需要判断插入点的位置，然后再进行判断。<br>
题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自己解法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">smallestDifference</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type a: List[int]</span></span><br><span class="line"><span class="string">        :type b: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        a.sort()</span><br><span class="line">        b.sort()</span><br><span class="line"></span><br><span class="line">        res = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">            index = bisect.bisect_left(b, i)</span><br><span class="line">            <span class="keyword">if</span> index==<span class="built_in">len</span>(b):</span><br><span class="line">                diff = <span class="built_in">abs</span>(i-b[index-<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">elif</span> index == <span class="number">0</span>:</span><br><span class="line">                diff = <span class="built_in">abs</span>(i - b[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">elif</span> index&gt;<span class="number">0</span>:</span><br><span class="line">                diff = <span class="built_in">min</span>(<span class="built_in">abs</span>(i-b[index-<span class="number">1</span>]), <span class="built_in">abs</span>(i-b[index]))</span><br><span class="line">            res = <span class="built_in">min</span>(diff, res)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="两球之间的磁力-1552-SKIP">两球之间的磁力[1552][SKIP]</h3>
<p>这道题看起来没啥意思，但是却考差了基本的问题的分析能力，以及对二分的应用的能力，题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/magnetic-force-between-two-balls/">https://leetcode-cn.com/problems/magnetic-force-between-two-balls/</a> 题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxDistance</span>(<span class="params">self, position: <span class="type">List</span>[<span class="built_in">int</span>], m: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        position.sort()</span><br><span class="line">        low, high = <span class="number">1</span>, position[-<span class="number">1</span>]- position[<span class="number">0</span>]</span><br><span class="line">        ans = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">check</span>(<span class="params">mid</span>):</span><br><span class="line">            pre = position[<span class="number">0</span>]</span><br><span class="line">            cnt = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(position)):</span><br><span class="line">                <span class="keyword">if</span> position[i] - pre &gt;= mid:</span><br><span class="line">                    pre = position[i]</span><br><span class="line">                    cnt = cnt + <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> cnt &gt;= m</span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> check(mid):</span><br><span class="line">                ans = mid</span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid</span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line"><span class="comment"># 正确</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxDistance</span>(<span class="params">self, position: <span class="type">List</span>[<span class="built_in">int</span>], m: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        position.sort()</span><br><span class="line">        low, high = <span class="number">1</span>, position[-<span class="number">1</span>]- position[<span class="number">0</span>]</span><br><span class="line">        ans = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">check</span>(<span class="params">mid</span>):</span><br><span class="line">            pre = position[<span class="number">0</span>]</span><br><span class="line">            cnt = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(position)):</span><br><span class="line">                <span class="keyword">if</span> position[i] - pre &gt;= mid:</span><br><span class="line">                    pre = position[i]</span><br><span class="line">                    cnt = cnt + <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> cnt &gt;= m</span><br><span class="line">        <span class="keyword">while</span> low &lt;= high: <span class="comment">#这里</span></span><br><span class="line">            mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> check(mid):</span><br><span class="line">                ans = mid</span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid - <span class="number">1</span> <span class="comment"># 这类</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line"><span class="comment"># 或者将错误的那个地方换一下</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxDistance</span>(<span class="params">self, position: <span class="type">List</span>[<span class="built_in">int</span>], m: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        position.sort()</span><br><span class="line">        low, high = <span class="number">1</span>, position[-<span class="number">1</span>]- position[<span class="number">0</span>] + <span class="number">1</span> <span class="comment"># 这里</span></span><br><span class="line"></span><br><span class="line">        ans = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">check</span>(<span class="params">mid</span>):</span><br><span class="line">            pre = position[<span class="number">0</span>]</span><br><span class="line">            cnt = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(position)):</span><br><span class="line">                <span class="keyword">if</span> position[i] - pre &gt;= mid:</span><br><span class="line">                    pre = position[i]</span><br><span class="line">                    cnt = cnt + <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> cnt &gt;= m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> check(mid):</span><br><span class="line">                ans = mid</span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<p>主要的思路就是一个一个试试，看看哪个间隔比较适合，最大的间隔就是position[-1] -position[0],最小的是0，我们一个一个来试一下就可以了。</p>
<h2 id="单数组前缀和">单数组前缀和</h2>
<h3 id="在-D-天内送达包裹的能力-1011">在 D 天内送达包裹的能力[1011]</h3>
<p>这道题相当给一个数组划分为k份，每份的和加起来最小数，题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/capacity-to-ship-packages-within-d-days/">https://leetcode-cn.com/problems/capacity-to-ship-packages-within-d-days/</a> ，题目是很好理解的，题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shipWithinDays</span>(<span class="params">self, weights: <span class="type">List</span>[<span class="built_in">int</span>], days: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">get_shop_times</span>(<span class="params">weights, v</span>):</span><br><span class="line">            need = <span class="number">1</span></span><br><span class="line">            cur = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> weights:</span><br><span class="line">                <span class="keyword">if</span> cur + i &gt; v:</span><br><span class="line">                    need += <span class="number">1</span></span><br><span class="line">                    cur = <span class="number">0</span></span><br><span class="line">                cur += i</span><br><span class="line">            <span class="keyword">return</span> need</span><br><span class="line"></span><br><span class="line">        low = <span class="built_in">max</span>(weights)</span><br><span class="line">        high = <span class="built_in">sum</span>(weights)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> low &lt; high: <span class="comment"># 注意的</span></span><br><span class="line">            mid = (high+low)//<span class="number">2</span></span><br><span class="line">            t = get_shop_times(weights, mid)</span><br><span class="line">            <span class="keyword">if</span> t &lt;= days: <span class="comment"># 压缩右边的</span></span><br><span class="line">                high = mid <span class="comment"># 注意的</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> low</span><br><span class="line">        或者 low&lt;=high 然后 high=mid-<span class="number">1</span>也可以的</span><br></pre></td></tr></table></figure>
<h3 id="最高频元素的频数-1838">最高频元素的频数[1838]</h3>
<p>题目在这里 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/frequency-of-the-most-frequent-element/">https://leetcode-cn.com/problems/frequency-of-the-most-frequent-element/</a> 这里我自己做了一种基于二分的，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxFrequency</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        nums.sort()</span><br><span class="line">        low = <span class="number">0</span></span><br><span class="line">        high = <span class="built_in">len</span>(nums)</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        prefix = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            prefix.append(prefix[-<span class="number">1</span>] + i)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            low = <span class="number">0</span></span><br><span class="line">            high = i</span><br><span class="line">            <span class="keyword">while</span> low &lt; high:</span><br><span class="line">                mid = (low + high) &gt;&gt; <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> nums[i] * (i - mid + <span class="number">1</span>) - prefix[i + <span class="number">1</span>] + prefix[mid] &lt;= k:</span><br><span class="line">                    high = mid</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    low = mid + <span class="number">1</span></span><br><span class="line">            res = <span class="built_in">max</span>(res, i - low + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>还有双指针的做法，具体如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxFrequency</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        nums.sort()</span><br><span class="line">        i = ans = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 问题转化一下，排序后 nums[j] * (j - i + 1) &lt;= k + presum[j + 1] - presum[i]</span></span><br><span class="line">        <span class="keyword">for</span> j, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            k += num</span><br><span class="line">            <span class="keyword">while</span> k &lt; num * (j - i + <span class="number">1</span>):</span><br><span class="line">                k -= nums[i]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 对于当前j最远的i</span></span><br><span class="line">            ans = <span class="built_in">max</span>(ans, j - i + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<p>其中就是老的思路而已，没有新的变化的，还是注意端点移动的条件。</p>
<h3 id="转变数组后最接近目标值的数组和-1300">转变数组后最接近目标值的数组和[1300]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/sum-of-mutated-array-closest-to-target/">https://leetcode-cn.com/problems/sum-of-mutated-array-closest-to-target/</a> 主要的思路是用二分来做，其实主要是找到一个值，让插入后后值全部变成这个值，比如[1,2,3,4,8,9], 我们设置为7，那么在后面的8和9就变成7。还需要注意的是，这里的prefix开始的值要为0，然后进行append才可以。<br>
题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findBestValue</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        n = <span class="built_in">len</span>(arr)</span><br><span class="line">        arr.sort()</span><br><span class="line">        prefix = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">            prefix.append(prefix[-<span class="number">1</span>] + i)</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        min_diff = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(arr[-<span class="number">1</span>]+<span class="number">1</span>):</span><br><span class="line">            index = bisect.bisect_left(arr, i)</span><br><span class="line">            sums = prefix[index] + (n-index)*i</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">abs</span>(sums - target) &lt; min_diff:</span><br><span class="line">                min_diff = <span class="built_in">abs</span>(sums - target)</span><br><span class="line">                res = i</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="区间和的个数-327-SKIP">区间和的个数[327][SKIP]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/count-of-range-sum/">https://leetcode-cn.com/problems/count-of-range-sum/</a> 题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 暴力</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countRangeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], lower: <span class="built_in">int</span>, upper: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        prefix = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            prefix.append(i + prefix[-<span class="number">1</span>])</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i, <span class="built_in">len</span>(nums)):</span><br><span class="line">                <span class="keyword">if</span> (prefix[j + <span class="number">1</span>] - prefix[i]) &gt;= lower <span class="keyword">and</span> (prefix[j + <span class="number">1</span>] - prefix[i]) &lt;= upper:</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> cnt</span><br><span class="line"><span class="comment"># 通过</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countRangeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], lower: <span class="built_in">int</span>, upper: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        res, pre, now = <span class="number">0</span>, [<span class="number">0</span>], <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> nums:</span><br><span class="line">            now += n</span><br><span class="line">            res += bisect.bisect_right(pre, now - lower) - bisect.bisect_left(pre, now - upper)</span><br><span class="line">            bisect.insort(pre, now)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"><span class="comment"># AC [通俗版]</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countRangeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], lower: <span class="built_in">int</span>, upper: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        sl = []</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        pre_sum = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            pre_sum.append(pre_sum[-<span class="number">1</span>] + i)</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> pre_sum:</span><br><span class="line">            res += bisect.bisect_right(sl, x - lower) - bisect.bisect_left(sl, x - upper)</span><br><span class="line">            bisect.insort(sl, x)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>乍一想前缀和不是单调的，没法进行插入排序，但这里的思路在于每个循环考虑以该index为结尾的符合条件的数量。这里的解法非常 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/count-of-range-sum/solutions/2417725/sortedlist-da-fa-hao-a-bu-dao-shi-xing-d-7yyh/">https://leetcode.cn/problems/count-of-range-sum/solutions/2417725/sortedlist-da-fa-hao-a-bu-dao-shi-xing-d-7yyh/</a></p>
<h3 id="将-x-减到-0-的最小操作数-1658">将 x 减到 0 的最小操作数[1658]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/minimum-operations-to-reduce-x-to-zero/">https://leetcode-cn.com/problems/minimum-operations-to-reduce-x-to-zero/</a> ，解法如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我的解法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minOperations</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], x: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">get_prefix_sum</span>(<span class="params">nums</span>):</span><br><span class="line">            prefix = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">                val = prefix[-<span class="number">1</span>] <span class="keyword">if</span> prefix <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">                prefix.append(i+val)</span><br><span class="line">            <span class="keyword">return</span> prefix</span><br><span class="line"></span><br><span class="line">        res = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            nums2 = nums[<span class="number">0</span>:i][::-<span class="number">1</span>] + nums[i:][::-<span class="number">1</span>]</span><br><span class="line">            prefix = get_prefix_sum(nums2)</span><br><span class="line">            <span class="built_in">print</span>(prefix)</span><br><span class="line">            <span class="keyword">if</span> x <span class="keyword">in</span> prefix:</span><br><span class="line">                res = <span class="built_in">min</span>(res, prefix.index(x)+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span> <span class="keyword">if</span> res==<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="keyword">else</span> res</span><br><span class="line"><span class="comment"># 正确的</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minOperations</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], x: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        diff = <span class="built_in">sum</span>(nums) - x</span><br><span class="line">        <span class="keyword">if</span> diff &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        sm = <span class="number">0</span></span><br><span class="line">        res = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> right &lt; <span class="built_in">len</span>(nums):</span><br><span class="line">            sm += nums[right]</span><br><span class="line">            <span class="keyword">while</span> sm &gt; diff:</span><br><span class="line">                sm -= nums[left]</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> sm == diff:</span><br><span class="line">                res = <span class="built_in">max</span>(res,right - left + <span class="number">1</span>)</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span> <span class="keyword">if</span> res==-<span class="number">1</span> <span class="keyword">else</span> <span class="built_in">len</span>(nums)-res</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="和大于等于-target-的最短子数组-LCR-008">和大于等于 target 的最短子数组[LCR 008]</h3>
<p>链接为：<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/2VG8Kg/description/">https://leetcode.cn/problems/2VG8Kg/description/</a> 题解如下，注意一下边界的条件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, target: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        prefix = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            prefix.append(prefix[-<span class="number">1</span>] + i)</span><br><span class="line">        <span class="keyword">if</span> prefix[-<span class="number">1</span>]&lt;target:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        res = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prefix)):</span><br><span class="line">            index = bisect.bisect_left(prefix, prefix[i] + target)</span><br><span class="line">            <span class="keyword">if</span> index != <span class="built_in">len</span>(prefix):           </span><br><span class="line">                res = <span class="built_in">min</span>(index - i, res)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> res==<span class="built_in">len</span>(prefix) <span class="keyword">else</span> res</span><br></pre></td></tr></table></figure>
<p>也可以使用滑窗解法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, s: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        ans = n + <span class="number">1</span></span><br><span class="line">        start, end = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> end &lt; n:</span><br><span class="line">            total += nums[end]</span><br><span class="line">            <span class="keyword">while</span> total &gt;= s:</span><br><span class="line">                ans = <span class="built_in">min</span>(ans, end - start + <span class="number">1</span>)</span><br><span class="line">                total -= nums[start]</span><br><span class="line">                start += <span class="number">1</span></span><br><span class="line">            end += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> ans == n + <span class="number">1</span> <span class="keyword">else</span> ans</span><br></pre></td></tr></table></figure>
<h3 id="和至少为k的最短子数组-862">和至少为k的最短子数组[862]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/shortest-subarray-with-sum-at-least-k/">https://leetcode-cn.com/problems/shortest-subarray-with-sum-at-least-k/</a> 题解用滑窗如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超时</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shortestSubarray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        res = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> left <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            right = left</span><br><span class="line">            sm =<span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> right &lt; n:</span><br><span class="line">                sm += nums[right]</span><br><span class="line">                <span class="keyword">while</span> sm &gt;= k:</span><br><span class="line">                    res = <span class="built_in">min</span>(res, right - left + <span class="number">1</span>)</span><br><span class="line">                    sm -= nums[left] </span><br><span class="line">                    left += <span class="number">1</span></span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span> <span class="keyword">if</span> res==<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="keyword">else</span> res</span><br><span class="line"><span class="comment"># 超时2：这个思路要弄懂 模仿区间和的个数区间和的个数来的</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shortestSubarray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        res = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        pre_sum = [[<span class="number">0</span>, cnt]]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums) + <span class="number">1</span>):</span><br><span class="line">            pre_sum.append([pre_sum[-<span class="number">1</span>][<span class="number">0</span>] + nums[i - <span class="number">1</span>], i])</span><br><span class="line">        pre_sum.sort()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pre_sum:</span><br><span class="line">            cur_val = i[<span class="number">0</span>]</span><br><span class="line">            index = bisect.bisect_left(pre_sum, [cur_val + k, <span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(index, <span class="built_in">len</span>(pre_sum)):</span><br><span class="line">                <span class="keyword">if</span> j &lt;= <span class="built_in">len</span>(nums) <span class="keyword">and</span> pre_sum[j][<span class="number">0</span>] &gt;= cur_val + k <span class="keyword">and</span> pre_sum[j][<span class="number">1</span>] - i[<span class="number">1</span>] &gt;= <span class="number">0</span>:</span><br><span class="line">                    res = <span class="built_in">min</span>(res, pre_sum[j][<span class="number">1</span>] - i[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> res==<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>):</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"><span class="comment"># 超时的原因在于有多个循环，其实看下面的结果也是有多个循环，  也就是1个for里面加了两个while, 不过计算的时候，是通过单调队列来做的，减少了滑窗计算的时间而已。这里和滑窗窗口的最大值是一样的。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shortestSubarray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">import</span> collections</span><br><span class="line">        prefix = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            prefix.append(prefix[-<span class="number">1</span>] + i)</span><br><span class="line">        stack = collections.deque()</span><br><span class="line">        ans = <span class="built_in">len</span>(nums) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">enumerate</span>(prefix):</span><br><span class="line">            <span class="keyword">while</span> stack <span class="keyword">and</span> y &lt;= prefix[stack[-<span class="number">1</span>]]:</span><br><span class="line">                stack.pop()</span><br><span class="line">            <span class="keyword">while</span> stack <span class="keyword">and</span> y - prefix[stack[<span class="number">0</span>]] &gt;= k:</span><br><span class="line">                ans = <span class="built_in">min</span>(ans, x - stack.popleft()) </span><br><span class="line">            stack.append(x)</span><br><span class="line">        <span class="keyword">return</span> ans <span class="keyword">if</span> ans &lt; <span class="built_in">len</span>(nums) + <span class="number">1</span> <span class="keyword">else</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>这道题和上面的那道题不一样，在于这道题有负数，导致前缀和非单调，无法用滑窗以及直接用二分来做。</p>
<h2 id="双数组">双数组</h2>
<h3 id="寻找两个正序数组的中位数-4">寻找两个正序数组的中位数[4]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/">https://leetcode-cn.com/problems/median-of-two-sorted-arrays/</a> ，题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解法1，使用双指针</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMedianSortedArrays</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        m = <span class="built_in">len</span>(nums1)</span><br><span class="line">        n = <span class="built_in">len</span>(nums2)</span><br><span class="line">        lens = m + n</span><br><span class="line">        prev = -<span class="number">1</span></span><br><span class="line">        now = -<span class="number">1</span></span><br><span class="line">        num1_index = <span class="number">0</span></span><br><span class="line">        num2_index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(lens//<span class="number">2</span>+<span class="number">1</span>):</span><br><span class="line">            prev = now</span><br><span class="line">            <span class="keyword">if</span> num1_index&lt;m <span class="keyword">and</span> (num2_index&gt;=n <span class="keyword">or</span> nums1[num1_index]&lt;nums2[num2_index]):</span><br><span class="line">                now = nums1[num1_index]</span><br><span class="line">                num1_index += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                now = nums2[num2_index]</span><br><span class="line">                num2_index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> lens &amp; <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> (prev + now)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> now</span><br><span class="line"><span class="comment"># 解法2：使用二分查找</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMedianSortedArrays</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        n1 = <span class="built_in">len</span>(nums1)</span><br><span class="line">        n2 = <span class="built_in">len</span>(nums2)</span><br><span class="line">        <span class="keyword">if</span> n1 &gt; n2:</span><br><span class="line">            <span class="keyword">return</span> self.findMedianSortedArrays(nums2, nums1)</span><br><span class="line"></span><br><span class="line">        k = (n1 + n2 + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = n1</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            m1 = left + (right - left) // <span class="number">2</span>    </span><br><span class="line">            m2 = k - m1                        <span class="comment"># </span></span><br><span class="line">            <span class="keyword">if</span> nums1[m1] &lt; nums2[m2 - <span class="number">1</span>]:      <span class="comment"># 注意</span></span><br><span class="line">                left = m1 + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = m1</span><br><span class="line"></span><br><span class="line">        m1 = left <span class="comment"># 算出的m是第2个数，如果是&amp;1=1的话，直接取m-1，不是的话，取m-1和m的均值</span></span><br><span class="line">        m2 = k - m1</span><br><span class="line">        </span><br><span class="line">        c1 = <span class="built_in">max</span>(<span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>) <span class="keyword">if</span> m1 &lt;= <span class="number">0</span> <span class="keyword">else</span> nums1[m1 - <span class="number">1</span>], <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>) <span class="keyword">if</span> m2 &lt;= <span class="number">0</span> <span class="keyword">else</span> nums2[m2 - <span class="number">1</span>]) <span class="comment"># 情况1,容易写错为m1</span></span><br><span class="line">        <span class="keyword">if</span> (n1 + n2) % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> c1</span><br><span class="line"></span><br><span class="line">        c2 = <span class="built_in">min</span>(<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">if</span> m1 &gt;= n1 <span class="keyword">else</span> nums1[m1], <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">if</span> m2 &gt;= n2 <span class="keyword">else</span> nums2[m2]) <span class="comment"># 情况2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (c1 + c2) / <span class="number">2</span></span><br><span class="line"><span class="comment"># 自己做法</span></span><br><span class="line">A = [<span class="number">1</span>]</span><br><span class="line">B = [<span class="number">2</span>]</span><br><span class="line">nums = []</span><br><span class="line">left, right = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> left &lt; <span class="built_in">len</span>(A) <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(B):</span><br><span class="line">    <span class="keyword">if</span> A[left] &lt; B[right]:</span><br><span class="line">        nums.append(A[left])</span><br><span class="line">        left += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nums.append(B[right])</span><br><span class="line">        right += <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> left &gt;= <span class="built_in">len</span>(A):</span><br><span class="line">    nums.extend(B[right:])</span><br><span class="line"><span class="keyword">if</span> right &gt;= <span class="built_in">len</span>(B):</span><br><span class="line">    nums.extend(A[left:])</span><br><span class="line"><span class="built_in">print</span>(nums)</span><br></pre></td></tr></table></figure>
<p>可以从 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/median-of-two-sorted-arrays/solutions/8999/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by-w-2/">https://leetcode.cn/problems/median-of-two-sorted-arrays/solutions/8999/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by-w-2/</a> 查看。二分的思路在 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/median-of-two-sorted-arrays/solutions/3983/shuang-zhi-zhen-by-powcai/">https://leetcode.cn/problems/median-of-two-sorted-arrays/solutions/3983/shuang-zhi-zhen-by-powcai/</a></p>
<h3 id="尽可能使字符串相等-1208">尽可能使字符串相等[1208]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/get-equal-substrings-within-budget/%EF%BC%8C">https://leetcode-cn.com/problems/get-equal-substrings-within-budget/，</a> 题解如下，建议使用双指针啊，比较快的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">equalSubstring</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span>, maxCost: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        diff = [<span class="built_in">abs</span>(<span class="built_in">ord</span>(i)-<span class="built_in">ord</span>(j)) <span class="keyword">for</span> i,j <span class="keyword">in</span> <span class="built_in">zip</span>(s,t)]</span><br><span class="line">        start = end = res = <span class="number">0</span></span><br><span class="line">        ds = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> end &lt; <span class="built_in">len</span>(diff):</span><br><span class="line">            ds += diff[end]</span><br><span class="line">            <span class="keyword">while</span> ds &gt; maxCost:</span><br><span class="line">                ds -= diff[start]</span><br><span class="line">                start += <span class="number">1</span></span><br><span class="line">            res = <span class="built_in">max</span>(res, end - start + <span class="number">1</span>)</span><br><span class="line">            end += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>二分查找的思路也是比较简单的，主要如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">equalSubstring</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span>, maxCost: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        accDiff = [<span class="number">0</span>] + <span class="built_in">list</span>(accumulate(<span class="built_in">abs</span>(<span class="built_in">ord</span>(sc) - <span class="built_in">ord</span>(tc)) <span class="keyword">for</span> sc, tc <span class="keyword">in</span> <span class="built_in">zip</span>(s, t)))</span><br><span class="line">        maxLength = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            start = bisect.bisect_left(accDiff, accDiff[i] - maxCost)</span><br><span class="line">            maxLength = <span class="built_in">max</span>(maxLength, i - start)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> maxLength</span><br></pre></td></tr></table></figure>
<h3 id="水位上升的泳池中游泳-778">水位上升的泳池中游泳[778]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/swim-in-rising-water/">https://leetcode-cn.com/problems/swim-in-rising-water/</a> 主要的思路就是找一个值，小于这个值的地方可以连通起来，最后能连通到最后的点的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">swimInWater</span>(<span class="params">self, grid: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">temp, x,y</span>):</span><br><span class="line">            <span class="keyword">if</span> x&lt;<span class="number">0</span> <span class="keyword">or</span> x&gt;=<span class="built_in">len</span>(temp) <span class="keyword">or</span> y&lt;<span class="number">0</span> <span class="keyword">or</span> y&gt;=<span class="built_in">len</span>(temp[<span class="number">0</span>]):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> temp[x][y]==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> x==<span class="built_in">len</span>(temp)-<span class="number">1</span> <span class="keyword">and</span> y==<span class="built_in">len</span>(temp[<span class="number">0</span>])-<span class="number">1</span> <span class="keyword">and</span> temp[x][y]==<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            temp[x][y] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i,j <span class="keyword">in</span> [[x+<span class="number">1</span>,y],[x,y+<span class="number">1</span>],[x-<span class="number">1</span>,y],[x,y-<span class="number">1</span>]]:</span><br><span class="line">                <span class="keyword">if</span> dfs(temp,i,j):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                <span class="comment">#  dfs(temp,i,j): 直接这么写的话是错误的</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        grid_list = <span class="built_in">sum</span>(grid, [])</span><br><span class="line">        grid_list = <span class="built_in">sum</span>(grid, [])</span><br><span class="line">        low = <span class="built_in">min</span>(grid_list)</span><br><span class="line">        high = <span class="built_in">max</span>(grid_list)</span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            mid = (low + high) &gt;&gt; <span class="number">1</span></span><br><span class="line">            temp = [[<span class="number">1</span> <span class="keyword">if</span> j &lt;= mid <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> i] <span class="keyword">for</span> i <span class="keyword">in</span> grid]</span><br><span class="line">            <span class="keyword">if</span> dfs(temp, <span class="number">0</span>, <span class="number">0</span>):</span><br><span class="line">                high = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> low</span><br></pre></td></tr></table></figure>
<h3 id="绝对差值和">绝对差值和</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/minimum-absolute-sum-difference/submissions/">https://leetcode-cn.com/problems/minimum-absolute-sum-difference/submissions/</a> 题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minAbsoluteSumDiff</span>(<span class="params">self, nums1, nums2</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums1: List[int]</span></span><br><span class="line"><span class="string">        :type nums2: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        temp = [<span class="built_in">abs</span>(nums1[i] - nums2[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums1))]</span><br><span class="line">        abs_sum = <span class="built_in">sum</span>(temp)</span><br><span class="line"></span><br><span class="line">        nums11 = <span class="built_in">sorted</span>(nums1)</span><br><span class="line"></span><br><span class="line">        res = abs_sum</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums2):</span><br><span class="line">            index = bisect.bisect(nums11, j)</span><br><span class="line">            <span class="keyword">if</span> index &lt; <span class="built_in">len</span>(nums11):</span><br><span class="line">                res = <span class="built_in">min</span>(res, abs_sum - <span class="built_in">abs</span>(nums1[i]-nums2[i]) + <span class="built_in">abs</span>(nums11[index]-nums2[i]))</span><br><span class="line">            <span class="keyword">if</span> index &gt; <span class="number">0</span>:</span><br><span class="line">                res = <span class="built_in">min</span>(res, abs_sum - <span class="built_in">abs</span>(nums1[i]-nums2[i]) + <span class="built_in">abs</span>(nums11[index-<span class="number">1</span>]-nums2[i]))</span><br><span class="line">        <span class="keyword">return</span> res% (<span class="number">10</span>**<span class="number">9</span>+<span class="number">7</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里需要注意的是，要查看需要插入数值的位置，查看其前后的位置，是不是有让绝对值更小的值。</p>
<h3 id="交换和-面试题-16-21">交换和[面试题 16.21]</h3>
<p>这道题和上面的是类似的，在得到index后需要判断位置，是不行越界了。题目在 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/sum-swap-lcci/">https://leetcode-cn.com/problems/sum-swap-lcci/</a> 题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findSwapValues</span>(<span class="params">self, array1, array2</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type array1: List[int]</span></span><br><span class="line"><span class="string">        :type array2: List[int]</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">import</span> bisect</span><br><span class="line">        array1.sort()</span><br><span class="line">        array2.sort()</span><br><span class="line"></span><br><span class="line">        sum1 = <span class="built_in">sum</span>(array1)</span><br><span class="line">        sum2 = <span class="built_in">sum</span>(array2)</span><br><span class="line"></span><br><span class="line">        diff = sum2 - sum1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> diff % <span class="number">2</span> !=<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> array1:</span><br><span class="line">            index = bisect.bisect_left(array2, i + diff // <span class="number">2</span>)</span><br><span class="line">            index = <span class="built_in">min</span>(<span class="built_in">len</span>(array2)-<span class="number">1</span>, index) <span class="comment">#dasdsdsa</span></span><br><span class="line">            <span class="keyword">if</span> array2[index] == i + diff // <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">return</span> [i, i + diff // <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>
<h3 id="得到子序列的最少操作次数-1713">得到子序列的最少操作次数[1713]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/minimum-operations-to-make-a-subsequence/">https://leetcode-cn.com/problems/minimum-operations-to-make-a-subsequence/</a> 这里的题目和最长上升子序列的思路基本上是一样的，可以看 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/minimum-operations-to-make-a-subsequence/solution/mo-gu-qie-cha-cong-lcswen-ti-dao-liswen-xist8/">https://leetcode-cn.com/problems/minimum-operations-to-make-a-subsequence/solution/mo-gu-qie-cha-cong-lcswen-ti-dao-liswen-xist8/</a> 的讲解，说的很清楚，总结来说就是一个如下的思路</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minOperations</span>(<span class="params">self, target: <span class="type">List</span>[<span class="built_in">int</span>], arr: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        posTa = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(target):</span><br><span class="line">            posTa[t] = i</span><br><span class="line">        posAr = []</span><br><span class="line">        <span class="keyword">for</span> i, a <span class="keyword">in</span> <span class="built_in">enumerate</span>(arr):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">in</span> posTa:</span><br><span class="line">                posAr.append(posTa[a])</span><br><span class="line">        <span class="comment"># 算出来出现的索引就可以了，然后就是求解了</span></span><br><span class="line">        stk = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> posAr:</span><br><span class="line">            <span class="keyword">if</span> stk <span class="keyword">and</span> x &lt;= stk[-<span class="number">1</span>]:</span><br><span class="line">                idx = bisect_left(stk, x)</span><br><span class="line">                stk[idx] = x</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stk.append(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(target) - <span class="built_in">len</span>(stk)</span><br></pre></td></tr></table></figure>
<h2 id="矩阵">矩阵</h2>
<h3 id="二维数组中的查找-LCR-121">二维数组中的查找[LCR.121]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof/">https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof/</a> 题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findNumberIn2DArray</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        i, j = <span class="built_in">len</span>(matrix) - <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &gt;= <span class="number">0</span> <span class="keyword">and</span> j &lt; <span class="built_in">len</span>(matrix[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">if</span> matrix[i][j] &gt; target: i -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> matrix[i][j] &lt; target: j += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>有个人总结的比较好，题解在 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof/solution/yu-niang-niang-04er-wei-shu-zu-zhong-de-vpcs9/">https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof/solution/yu-niang-niang-04er-wei-shu-zu-zhong-de-vpcs9/</a> 讲解了多个方法</p>
<h3 id="搜索二维矩阵-74">搜索二维矩阵[74]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/search-a-2d-matrix/">https://leetcode-cn.com/problems/search-a-2d-matrix/</a> 和上面的不一样，这个矩阵的展开收拾递增的，因此可以使用查找的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">searchMatrix</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        low = <span class="number">0</span></span><br><span class="line">        m = <span class="built_in">len</span>(matrix)</span><br><span class="line">        n = <span class="built_in">len</span>(matrix[<span class="number">0</span>])</span><br><span class="line">        high = m * n - <span class="number">1</span> <span class="comment"># 如果这里改成了m*n的话，那么后面的最好用low&lt;high</span></span><br><span class="line">        <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">            mid = low + (high-low)//<span class="number">2</span></span><br><span class="line">            row = mid//n</span><br><span class="line">            col = mid%n</span><br><span class="line">            <span class="keyword">if</span> matrix[row][col] &gt; target:</span><br><span class="line">                high = mid - <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> matrix[row][col] &lt; target:</span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="统计有序矩阵中的负数-1351">统计有序矩阵中的负数[1351]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/count-negative-numbers-in-a-sorted-matrix/">https://leetcode-cn.com/problems/count-negative-numbers-in-a-sorted-matrix/</a> 题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countNegatives</span>(<span class="params">self, grid: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        end = <span class="built_in">len</span>(grid[<span class="number">0</span>])</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(grid):</span><br><span class="line">            <span class="keyword">if</span> grid[i][-<span class="number">1</span>]&lt;<span class="number">0</span>:</span><br><span class="line">                index = bisect.bisect_right([-j <span class="keyword">for</span> j <span class="keyword">in</span> grid[i]],<span class="number">0</span>)</span><br><span class="line">                res += <span class="built_in">len</span>(grid[<span class="number">0</span>]) - index</span><br><span class="line">            i = i + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="螺旋矩阵-54">螺旋矩阵[54]</h3>
<p>题号为54，位于<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/spiral-matrix/description/">https://leetcode.cn/problems/spiral-matrix/description/</a> 题解如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spiralOrder</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(matrix)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> matrix[<span class="number">0</span>]</span><br><span class="line">        directions = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, -<span class="number">1</span>), (-<span class="number">1</span>, <span class="number">0</span>)]</span><br><span class="line">        row, col, direction_index = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        res = []</span><br><span class="line">        m = <span class="built_in">len</span>(matrix)</span><br><span class="line">        n = <span class="built_in">len</span>(matrix[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m * n):</span><br><span class="line">            res.append(matrix[row][col])</span><br><span class="line">            matrix[row][col] = -<span class="number">999999999</span></span><br><span class="line">            next_row = row + directions[direction_index % <span class="number">4</span>][<span class="number">0</span>]</span><br><span class="line">            next_col = col + directions[direction_index % <span class="number">4</span>][<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> next_row &lt; <span class="number">0</span> <span class="keyword">or</span> next_row &gt;= m <span class="keyword">or</span> next_col &lt; <span class="number">0</span> <span class="keyword">or</span> next_col &gt;= n <span class="keyword">or</span> matrix[next_row][next_col] == -<span class="number">999999999</span>:</span><br><span class="line">                direction_index = direction_index + <span class="number">1</span></span><br><span class="line">                next_row = row + directions[direction_index % <span class="number">4</span>][<span class="number">0</span>]</span><br><span class="line">                next_col = col + directions[direction_index % <span class="number">4</span>][<span class="number">1</span>]</span><br><span class="line">            row, col = next_row, next_col</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>上下两道题用的是同样的思路，这样比较好统一</p>
<h3 id="螺旋矩阵II-59">螺旋矩阵II[59]</h3>
<p>题号为59，位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/spiral-matrix-ii/description/">https://leetcode.cn/problems/spiral-matrix-ii/description/</a>  题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generateMatrix</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        directions = [(<span class="number">0</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">0</span>),(<span class="number">0</span>,-<span class="number">1</span>),(-<span class="number">1</span>,<span class="number">0</span>)]</span><br><span class="line">        matrix = [[-<span class="number">1</span>]*n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        row, col, direc_index = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n*n):</span><br><span class="line">            matrix[row][col] = i + <span class="number">1</span></span><br><span class="line">            next_row = row + directions[direc_index%<span class="number">4</span>][<span class="number">0</span>]</span><br><span class="line">            next_col = row + directions[direc_index%<span class="number">4</span>][<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> next_row &lt; <span class="number">0</span> <span class="keyword">or</span> next_row &gt;= n <span class="keyword">or</span> next_col &lt; <span class="number">0</span> <span class="keyword">or</span> next_row &gt;= n <span class="keyword">or</span> matrix[next_row][next_col] &gt; -<span class="number">1</span>:</span><br><span class="line">                direc_index = direc_index + <span class="number">1</span></span><br><span class="line">                next_row = row + directions[direc_index%<span class="number">4</span>][<span class="number">0</span>]</span><br><span class="line">                next_col = row + directions[direc_index%<span class="number">4</span>][<span class="number">1</span>]</span><br><span class="line">            row, col = next_row, next_col</span><br><span class="line">        <span class="keyword">return</span> matrix</span><br></pre></td></tr></table></figure>
<p>注意具体的思路是先便利，然后换方向</p>
<h3 id="螺旋矩阵III-885">螺旋矩阵III[885]</h3>
<p>题号为885，位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/spiral-matrix-iii/description/">https://leetcode.cn/problems/spiral-matrix-iii/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spiralMatrixIII</span>(<span class="params">self, rows: <span class="built_in">int</span>, cols: <span class="built_in">int</span>, rStart: <span class="built_in">int</span>, cStart: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]: </span><br><span class="line">        visited_flag = [[<span class="literal">False</span>] * cols <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(rows)]</span><br><span class="line">        directions = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, -<span class="number">1</span>), (-<span class="number">1</span>, <span class="number">0</span>)]</span><br><span class="line">        direction_index = -<span class="number">1</span></span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        init_lens = <span class="number">0</span></span><br><span class="line">        res = []</span><br><span class="line">        res.append([rStart, cStart])</span><br><span class="line">        <span class="keyword">while</span> cnt + <span class="number">1</span> &lt; rows * cols:</span><br><span class="line">            init_lens = init_lens + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            direction_index = direction_index + <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j1 <span class="keyword">in</span> <span class="built_in">range</span>(init_lens):</span><br><span class="line">                next_r = rStart + directions[direction_index % <span class="number">4</span>][<span class="number">0</span>]</span><br><span class="line">                next_c = cStart + directions[direction_index % <span class="number">4</span>][<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> next_r &lt; <span class="number">0</span> <span class="keyword">or</span> next_r &gt;= rows <span class="keyword">or</span> next_c &lt; <span class="number">0</span> <span class="keyword">or</span> next_c &gt;= cols <span class="keyword">or</span> visited_flag[next_r][next_c]:</span><br><span class="line">                    rStart = next_r</span><br><span class="line">                    cStart = next_c</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res.append([next_r, next_c])</span><br><span class="line">                    rStart = next_r</span><br><span class="line">                    cStart = next_c</span><br><span class="line">                    visited_flag[next_r][next_c] = <span class="literal">True</span></span><br><span class="line">                    cnt = cnt + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            direction_index = direction_index + <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j2 <span class="keyword">in</span> <span class="built_in">range</span>(init_lens):</span><br><span class="line">                next_r = rStart + directions[direction_index % <span class="number">4</span>][<span class="number">0</span>]</span><br><span class="line">                next_c = cStart + directions[direction_index % <span class="number">4</span>][<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> next_r &lt; <span class="number">0</span> <span class="keyword">or</span> next_r &gt;= rows <span class="keyword">or</span> next_c &lt; <span class="number">0</span> <span class="keyword">or</span> next_c &gt;= cols <span class="keyword">or</span> visited_flag[next_r][next_c]:</span><br><span class="line">                    rStart = next_r</span><br><span class="line">                    cStart = next_c</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res.append([next_r, next_c])</span><br><span class="line">                    rStart = next_r</span><br><span class="line">                    cStart = next_c</span><br><span class="line">                    visited_flag[next_r][next_c] = <span class="literal">True</span></span><br><span class="line">                    cnt = cnt + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>主要思路就是分析题目，其实每个固定的长度比如走1格子，其实是分为两个角度来走的，那么整体上就是1步往右，1步往下，2步往左，2步往上。依次这样来再结合判断条件。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/deep_learning/ch2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/deep_learning/ch2/" class="post-title-link" itemprop="url">机器学习基础面试题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-24 10:13:37" itemprop="dateModified" datetime="2024-03-24T10:13:37+08:00">2024-03-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">算法面试</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>机器学习基础</h1>
<p>​	机器学习起源于上世纪50年代，1959年在IBM工作的Arthur Samuel设计了一个下棋程序，这个程序具有学习的能力，它可以在不断的对弈中提高自己。由此提出了“机器学习”这个概念，它是一个结合了多个学科如概率论，优化理论，统计等，最终在计算机上实现自我获取新知识，学习改善自己的这样一个研究领域。机器学习是人工智能的一个子集，目前已经发展出许多有用的方法，比如支持向量机，回归，决策树，随机森林，强化方法，集成学习，深度学习等等，一定程度上可以帮助人们完成一些数据预测，自动化，自动决策，最优化等初步替代脑力的任务。本章我们主要介绍下机器学习的基本概念、监督学习、分类算法、逻辑回归、代价函数、损失函数、LDA、PCA、决策树、支持向量机、EM算法、聚类和降维以及模型评估有哪些方法、指标等等。</p>
<h2 id="2-1-基本概念">2.1 基本概念</h2>
<h3 id="2-1-1-大话理解机器学习本质">2.1.1 大话理解机器学习本质</h3>
<p>​	机器学习(Machine Learning, ML)，顾名思义，让机器去学习。这里，机器指的是计算机，是算法运行的物理载体，你也可以把各种算法本身当做一个有输入和输出的机器。那么到底让计算机去学习什么呢？对于一个任务及其表现的度量方法，设计一种算法，让算法能够提取中数据所蕴含的规律，这就叫机器学习。如果输入机器的数据是带有标签的，就称作有监督学习。如果数据是无标签的，就是无监督学习。</p>
<h3 id="2-1-2-什么是神经网络">2.1.2 什么是神经网络</h3>
<p>​	神经网络就是按照一定规则将多个神经元连接起来的网络。不同的神经网络，具有不同的连接规则。例如全连接(Full Connected, FC)神经网络，它的规则包括：</p>
<p>（1）有三种层：输入层，输出层，隐藏层。</p>
<p>（2）同一层的神经元之间没有连接。</p>
<p>（3）fully connected的含义：第 N 层的每个神经元和第 N-1 层的所有神经元相连，第 N-1 层神经元的输出就是第 N 层神经元的输入。</p>
<p>（4）每个连接都有一个权值。</p>
<p><strong>神经网络架构</strong><br>
​	图2-1就是一个神经网络系统，它由很多层组成。输入层负责接收信息，比如一只猫的图片。输出层是计算机对这个输入信息的判断结果，它是不是猫。隐藏层就是对输入信息的传递和加工处理。<br>
<img src="2.5.1.png" alt="图2-2 神经网络系统"></p>
<p>​								图2-1 神经网络系统</p>
<h3 id="2-1-3-各种常见算法图示">2.1.3 各种常见算法图示</h3>
<p>​	日常使用机器学习的任务中，我们经常会遇见各种算法，图2-2是各种常见算法的图示。</p>
<table>
<thead>
<tr>
<th style="text-align:center">回归算法</th>
<th style="text-align:center">聚类算法</th>
<th style="text-align:center">正则化方法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="2.1/1.jpg" alt></td>
<td style="text-align:center"><img src="2.1/2.jpg" alt></td>
<td style="text-align:center"><img src="2.1/3.jpg" alt></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">决策树学习</th>
<th style="text-align:center">贝叶斯方法</th>
<th style="text-align:center">基于核的算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="2.2.4.png" alt></td>
<td style="text-align:center"><img src="2.1/5.jpg" alt></td>
<td style="text-align:center"><img src="2.1/6.jpg" alt></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">聚类算法</th>
<th style="text-align:center">关联规则学习</th>
<th style="text-align:center">人工神经网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="2.1/7.jpg" alt></td>
<td style="text-align:center"><img src="2.2.8.png" alt></td>
<td style="text-align:center"><img src="2.2.09.png" alt></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">深度学习</th>
<th style="text-align:center">降低维度算法</th>
<th style="text-align:center">集成算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="2.2.10.png" alt></td>
<td style="text-align:center"><img src="2.2.11.png" alt></td>
<td style="text-align:center"><img src="2.2.12.png" alt></td>
</tr>
</tbody>
</table>
<p>​										图2-2 各种常见算法图示</p>
<h3 id="2-1-4-计算图的导数计算">2.1.4 计算图的导数计算</h3>
<p>​	计算图导数计算是反向传播，利用链式法则和隐式函数求导。</p>
<p>​	假设  $z = f(u,v)$  在点  $(u,v)$  处偏导连续， $(u,v)$ 是关于  $t$  的函数，在  $t$  点可导，求  $z$  在  $t$  点的导数。</p>
<p>根据链式法则有</p>
 $$
\frac{dz}{dt}=\frac{\partial z}{\partial u}.\frac{du}{dt}+\frac{\partial z}{\partial v}
				.\frac{dv}{dt}
$$ 
<p>​	链式法则用文字描述:“由两个函数凑起来的复合函数，其导数等于里边函数代入外边函数的值之导数，乘以里边函数的导数。<br>
​	为了便于理解，下面举例说明：</p>
 $$
f(x)=x^2,g(x)=2x+1
$$ 
<p>​	则:</p>
 $$
{f[g(x)]}'=2[g(x)] \times g'(x)=2[2x+1] \times 2=8x+4
$$ 
<h3 id="2-1-5-理解局部最优与全局最优">2.1.5 理解局部最优与全局最优</h3>
<p>​	笑谈局部最优和全局最优</p>
<blockquote>
<p>​	柏拉图有一天问老师苏格拉底什么是爱情？苏格拉底叫他到麦田走一次，摘一颗最大的麦穗回来，不许回头，只可摘一次。柏拉图空着手出来了，他的理由是，看见不错的，却不知道是不是最好的，一次次侥幸，走到尽头时，才发现还不如前面的，于是放弃。苏格拉底告诉他：“这就是爱情。”这故事让我们明白了一个道理，因为生命的一些不确定性，所以全局最优解是很难寻找到的，或者说根本就不存在，我们应该设置一些限定条件，然后在这个范围内寻找最优解，也就是局部最优解——有所斩获总比空手而归强，哪怕这种斩获只是一次有趣的经历。<br>
​	柏拉图有一天又问什么是婚姻？苏格拉底叫他到树林走一次,选一棵最好的树做圣诞树，也是不许回头，只许选一次。这次他一身疲惫地拖了一棵看起来直挺、翠绿，却有点稀疏的杉树回来，他的理由是，有了上回的教训，好不容易看见一棵看似不错的，又发现时间、体力已经快不够用了，也不管是不是最好的，就拿回来了。苏格拉底告诉他：“这就是婚姻。”</p>
</blockquote>
<p>​	优化问题一般分为局部最优和全局最优。其中，</p>
<p>（1）局部最优，就是在函数值空间的一个有限区域内寻找最小值；而全局最优，是在函数值空间整个区域寻找最小值问题。</p>
<p>（2）函数局部最小点是它的函数值小于或等于附近点的点，但是有可能大于较远距离的点。</p>
<p>（3）全局最小点是那种它的函数值小于或等于所有的可行点。</p>
<h3 id="2-1-5-大数据与深度学习之间的关系">2.1.5 大数据与深度学习之间的关系</h3>
<p>首先来看大数据、机器学习及数据挖掘三者简单的定义：</p>
<p><strong>大数据</strong>通常被定义为“超出常用软件工具捕获，管理和处理能力”的数据集。<br>
<strong>机器学习</strong>关心的问题是如何构建计算机程序使用经验自动改进。<br>
<strong>数据挖掘</strong>是从数据中提取模式的特定算法的应用，在数据挖掘中，重点在于算法的应用，而不是算法本身。</p>
<p><strong>机器学习和数据挖掘</strong>之间的关系如下：<br>
数据挖掘是一个过程，在此过程中机器学习算法被用作提取数据集中的潜在有价值模式的工具。<br>
大数据与深度学习关系总结如下：</p>
<p>（1）深度学习是一种模拟大脑的行为。可以从所学习对象的机制以及行为等等很多相关联的方面进行学习，模仿类型行为以及思维。</p>
<p>（2）深度学习对于大数据的发展有帮助。深度学习对于大数据技术开发的每一个阶段均有帮助，不管是数据的分析还是挖掘还是建模，只有深度学习，这些工作才会有可能一一得到实现。</p>
<p>（3）深度学习转变了解决问题的思维。很多时候发现问题到解决问题，走一步看一步不是一个主要的解决问题的方式了，在深度学习的基础上，要求我们从开始到最后都要基于一个目标，为了需要优化的那个最终目标去进行处理数据以及将数据放入到数据应用平台上去，这就是端到端（End to End）。</p>
<p>（4）大数据的深度学习需要一个框架。在大数据方面的深度学习都是从基础的角度出发的，深度学习需要一个框架或者一个系统。总而言之，将你的大数据通过深度分析变为现实，这就是深度学习和大数据的最直接关系。</p>
<h2 id="2-2-机器学习学习方式">2.2 机器学习学习方式</h2>
<p>​	根据数据类型的不同，对一个问题的建模有不同的方式。依据不同的学习方式和输入数据，机器学习主要分为以下四种学习方式。</p>
<h3 id="2-2-1-监督学习">2.2.1 监督学习</h3>
<p>​	特点：监督学习是使用已知正确答案的示例来训练网络。已知数据和其一一对应的标签，训练一个预测模型，将输入数据映射到标签的过程。</p>
<p>​	常见应用场景：监督式学习的常见应用场景如分类问题和回归问题。</p>
<p>​	算法举例：常见的有监督机器学习算法包括支持向量机(Support Vector Machine, SVM)，朴素贝叶斯(Naive Bayes)，逻辑回归(Logistic Regression)，K近邻(K-Nearest Neighborhood, KNN)，决策树(Decision Tree)，随机森林(Random Forest)，AdaBoost以及线性判别分析(Linear Discriminant Analysis, LDA)等。深度学习(Deep Learning)也是大多数以监督学习的方式呈现。</p>
<h3 id="2-2-2-非监督式学习">2.2.2 非监督式学习</h3>
<p>​	定义：在非监督式学习中，数据并不被特别标识，适用于你具有数据集但无标签的情况。学习模型是为了推断出数据的一些内在结构。</p>
<p>​	常见应用场景：常见的应用场景包括关联规则的学习以及聚类等。</p>
<p>​	算法举例：常见算法包括Apriori算法以及k-Means算法。</p>
<h3 id="2-2-3-半监督式学习">2.2.3 半监督式学习</h3>
<p>​	特点：在此学习方式下，输入数据部分被标记，部分没有被标记，这种学习模型可以用来进行预测。</p>
<p>​	常见应用场景：应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，通过对已标记数据建模，在此基础上，对未标记数据进行预测。</p>
<p>​	算法举例：常见算法如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。</p>
<h3 id="2-2-4-弱监督学习">2.2.4 弱监督学习</h3>
<p>​	特点：弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或包含多种情况（没有标记，有一个标记，和有多个标记）的多个元素。 数据集的标签是不可靠的，这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。</p>
<p>​	算法举例：举例，给出一张包含气球的图片，需要得出气球在图片中的位置及气球和背景的分割线，这就是已知弱标签学习强标签的问题。</p>
<p>​	在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。</p>
<h3 id="2-2-5-监督学习有哪些步骤">2.2.5 监督学习有哪些步骤</h3>
<p>​	监督学习是使用已知正确答案的示例来训练网络，每组训练数据有一个明确的标识或结果。想象一下，我们可以训练一个网络，让其从照片库中（其中包含气球的照片）识别出气球的照片。以下就是我们在这个假设场景中所要采取的步骤。</p>
<p><strong>步骤1：数据集的创建和分类</strong><br>
​	首先，浏览你的照片（数据集），确定所有包含气球的照片，并对其进行标注。然后，将所有照片分为训练集和验证集。目标就是在深度网络中找一函数，这个函数输入是任意一张照片，当照片中包含气球时，输出1，否则输出0。</p>
<p><strong>步骤2：数据增强（Data Augmentation）</strong><br>
​	当原始数据搜集和标注完毕，一般搜集的数据并不一定包含目标在各种扰动下的信息。数据的好坏对于机器学习模型的预测能力至关重要，因此一般会进行数据增强。对于图像数据来说，数据增强一般包括，图像旋转，平移，颜色变换，裁剪，仿射变换等。</p>
<p><strong>步骤3：特征工程（Feature Engineering）</strong><br>
​	一般来讲，特征工程包含特征提取和特征选择。常见的手工特征(Hand-Crafted Feature)有尺度不变特征变换(Scale-Invariant Feature Transform, SIFT)，方向梯度直方图(Histogram of Oriented Gradient, HOG)等。由于手工特征是启发式的，其算法设计背后的出发点不同，将这些特征组合在一起的时候有可能会产生冲突，如何将组合特征的效能发挥出来，使原始数据在特征空间中的判别性最大化，就需要用到特征选择的方法。在深度学习方法大获成功之后，人们很大一部分不再关注特征工程本身。因为，最常用到的卷积神经网络(Convolutional Neural Networks, CNNs)本身就是一种特征提取和选择的引擎。研究者提出的不同的网络结构、正则化、归一化方法实际上就是深度学习背景下的特征工程。</p>
<p><strong>步骤4：构建预测模型和损失</strong><br>
​	将原始数据映射到特征空间之后，也就意味着我们得到了比较合理的输入。下一步就是构建合适的预测模型得到对应输入的输出。而如何保证模型的输出和输入标签的一致性，就需要构建模型预测和标签之间的损失函数，常见的损失函数(Loss Function)有交叉熵、均方差等。通过优化方法不断迭代，使模型从最初的初始化状态一步步变化为有预测能力的模型的过程，实际上就是学习的过程。</p>
<p><strong>步骤5：训练</strong><br>
​	选择合适的模型和超参数进行初始化，其中超参数比如支持向量机中核函数、误差项惩罚权重等。当模型初始化参数设定好后，将制作好的特征数据输入到模型，通过合适的优化方法不断缩小输出与标签之间的差距，当迭代过程到了截止条件，就可以得到训练好的模型。优化方法最常见的就是梯度下降法及其变种，使用梯度下降法的前提是优化目标函数对于模型是可导的。</p>
<p><strong>步骤6：验证和模型选择</strong><br>
​	训练完训练集图片后，需要进行模型测试。利用验证集来验证模型是否可以准确地挑选出含有气球在内的照片。<br>
​	在此过程中，通常会通过调整和模型相关的各种事物（超参数）来重复步骤2和3，诸如里面有多少个节点，有多少层，使用怎样的激活函数和损失函数，如何在反向传播阶段积极有效地训练权值等等。</p>
<p><strong>步骤7：测试及应用</strong><br>
​	当有了一个准确的模型，就可以将该模型部署到你的应用程序中。你可以将预测功能发布为API（Application Programming Interface, 应用程序编程接口）调用，并且你可以从软件中调用该API，从而进行推理并给出相应的结果。</p>
<h2 id="2-8-分类算法">2.8 分类算法</h2>
<p>​	分类算法和回归算法是对真实世界不同建模的方法。分类模型是认为模型的输出是离散的，例如大自然的生物被划分为不同的种类，是离散的。回归模型的输出是连续的，例如人的身高变化过程是一个连续过程，而不是离散的。</p>
<p>​	因此，在实际建模过程时，采用分类模型还是回归模型，取决于你对任务（真实世界）的分析和理解。</p>
<h3 id="2-8-1-常用分类算法的优缺点？">2.8.1 常用分类算法的优缺点？</h3>
<p>​	接下来我们介绍常用分类算法的优缺点，如表2-1所示。</p>
<p>​									表2-1 常用分类算法的优缺点</p>
<table>
<thead>
<tr>
<th style="text-align:left">算法</th>
<th style="text-align:left">优点</th>
<th style="text-align:left">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Bayes 贝叶斯分类法</td>
<td style="text-align:left">1）所需估计的参数少，对于缺失数据不敏感。<br>2）有着坚实的数学基础，以及稳定的分类效率。</td>
<td style="text-align:left">1）需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。<br>2）需要知道先验概率。<br>3）分类决策存在错误率。</td>
</tr>
<tr>
<td style="text-align:left">Decision Tree决策树</td>
<td style="text-align:left">1）不需要任何领域知识或参数假设。<br>2）适合高维数据。<br>3）简单易于理解。<br>4）短时间内处理大量数据，得到可行且效果较好的结果。<br>5）能够同时处理数据型和常规性属性。</td>
<td style="text-align:left">1）对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<br>2）易于过拟合。<br>3）忽略属性之间的相关性。<br>4）不支持在线学习。</td>
</tr>
<tr>
<td style="text-align:left">SVM支持向量机</td>
<td style="text-align:left">1）可以解决小样本下机器学习的问题。<br>2）提高泛化性能。<br>3）可以解决高维、非线性问题。超高维文本分类仍受欢迎。<br>4）避免神经网络结构选择和局部极小的问题。</td>
<td style="text-align:left">1）对缺失数据敏感。<br>2）内存消耗大，难以解释。<br>3）运行和调参略烦人。</td>
</tr>
<tr>
<td style="text-align:left">KNN K近邻</td>
<td style="text-align:left">1）思想简单，理论成熟，既可以用来做分类也可以用来做回归； <br>2）可用于非线性分类；<br> 3）训练时间复杂度为O(n)； <br>4）准确度高，对数据没有假设，对outlier不敏感；</td>
<td style="text-align:left">1）计算量太大。<br>2）对于样本分类不均衡的问题，会产生误判。<br>3）需要大量的内存。<br>4）输出的可解释性不强。</td>
</tr>
<tr>
<td style="text-align:left">Logistic Regression逻辑回归</td>
<td style="text-align:left">1）速度快。<br>2）简单易于理解，直接看到各个特征的权重。<br>3）能容易地更新模型吸收新的数据。<br>4）如果想要一个概率框架，动态调整分类阀值。</td>
<td style="text-align:left">特征处理复杂。需要归一化和较多的特征工程。</td>
</tr>
<tr>
<td style="text-align:left">Neural Network 神经网络</td>
<td style="text-align:left">1）分类准确率高。<br>2）并行处理能力强。<br>3）分布式存储和学习能力强。<br>4）鲁棒性较强，不易受噪声影响。</td>
<td style="text-align:left">1）需要大量参数（网络拓扑、阀值、阈值）。<br>2）结果难以解释。<br>3）训练时间过长。</td>
</tr>
<tr>
<td style="text-align:left">Adaboosting</td>
<td style="text-align:left">1）adaboost是一种有很高精度的分类器。<br>2）可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br>3）当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。<br>4）简单，不用做特征筛选。<br>5）不用担心overfitting。</td>
<td style="text-align:left">对outlier比较敏感</td>
</tr>
</tbody>
</table>
<h3 id="2-8-2-分类算法的评估方法">2.8.2 分类算法的评估方法</h3>
<p>​	分类评估方法主要功能是用来评估分类算法的好坏，而评估一个分类器算法的好坏又包括许多项指标。了解各种评估方法，在实际应用中选择正确的评估方法是十分重要的。</p>
<ul>
<li>
<p><strong>几个常用术语</strong><br>
​	这里首先介绍几个常见的模型评价术语，现在假设我们的分类目标只有两类，计为正例（positive）和负例（negative）分别是：</p>
<ol>
<li>True positives(TP):  被正确地划分为正例的个数，即实际为正例且被分类器划分为正例的实例数；</li>
<li>False positives(FP): 被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数；</li>
<li>False negatives(FN):被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数；</li>
<li>True negatives(TN): 被正确地划分为负例的个数，即实际为负例且被分类器划分为负例的实例数。</li>
</ol>
<p>​									表2-2 四个术语的混淆矩阵</p>
</li>
</ul>
<p><img src="2.9/1.png" alt="图2-3 术语的混淆矩阵"></p>
<p>表2-2是这四个术语的混淆矩阵，做以下说明：<br>
1）P=TP+FN表示实际为正例的样本个数。<br>
2）True、False描述的是分类器是否判断正确。<br>
3）Positive、Negative是分类器的分类结果，如果正例计为1、负例计为-1，即positive=1、negative=-1。用1表示True，-1表示False，那么实际的类标=TF*PN，TF为true或false，PN为positive或negative。<br>
4）例如True positives(TP)的实际类标=1*1=1为正例，False positives(FP)的实际类标=(-1)*1=-1为负例，False negatives(FN)的实际类标=(-1)*(-1)=1为正例，True negatives(TN)的实际类标=1*(-1)=-1为负例。</p>
<ul>
<li>
<p><strong>评价指标</strong></p>
<ol>
<li>
<p>正确率（accuracy）<br>
正确率是我们最常见的评价指标，accuracy = (TP+TN)/(P+N)，正确率是被分对的样本数在所有样本数中的占比，通常来说，正确率越高，分类器越好。</p>
</li>
<li>
<p>错误率（error rate)<br>
错误率则与正确率相反，描述被分类器错分的比例，error rate = (FP+FN)/(P+N)，对某一个实例来说，分对与分错是互斥事件，所以accuracy =1 -  error rate。</p>
</li>
<li>
<p>灵敏度（sensitivity）<br>
sensitivity = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力。</p>
</li>
<li>
<p>特异性（specificity)<br>
specificity = TN/N，表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。</p>
</li>
<li>
<p>精度（precision）<br>
precision=TP/(TP+FP)，精度是精确性的度量，表示被分为正例的示例中实际为正例的比例。</p>
</li>
<li>
<p>召回率（recall）<br>
召回率是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P=sensitivity，可以看到召回率与灵敏度是一样的。</p>
</li>
<li>
<p>其他评价指标<br>
计算速度：分类器训练和预测需要的时间；<br>
鲁棒性：处理缺失值和异常值的能力；<br>
可扩展性：处理大数据集的能力；<br>
可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子。</p>
</li>
<li>
<p>精度和召回率反映了分类器分类性能的两个方面。如果综合考虑查准率与查全率，可以得到新的评价指标F1-score，也称为综合分类率： $F1=\frac{2 \times precision \times recall}{precision + recall}​$ 。</p>
<p>为了综合多个类别的分类情况，评测系统整体性能，经常采用的还有微平均F1（micro-averaging）和宏平均F1（macro-averaging ）两种指标。</p>
<p>（1）宏平均F1与微平均F1是以两种不同的平均方式求的全局F1指标。</p>
<p>（2）宏平均F1的计算方法先对每个类别单独计算F1值，再取这些F1值的算术平均值作为全局指标。</p>
<p>（3）微平均F1的计算方法是先累加计算各个类别的a、b、c、d的值，再由这些值求出F1值。</p>
<p>（4）由两种平均F1的计算方式不难看出，宏平均F1平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均F1平等考虑文档集中的每一个文档，所以它的值受到常见类别的影响比较大。</p>
</li>
</ol>
</li>
<li>
<p><strong>ROC曲线和PR曲线</strong></p>
<pre><code>  如图2-3，ROC曲线是（Receiver Operating Characteristic Curve，受试者工作特征曲线）的简称，是以灵敏度（真阳性率）为纵坐标，以1减去特异性（假阳性率）为横坐标绘制的性能评价曲线。可以将不同模型对同一数据集的ROC曲线绘制在同一笛卡尔坐标系中，ROC曲线越靠近左上角，说明其对应模型越可靠。也可以通过ROC曲线下面的面积（Area Under Curve, AUC）来评价模型，AUC越大，模型越可靠。
</code></pre>
</li>
</ul>
<p><img src="2.7.3.png" alt></p>
<p>​	                                                                         图2-3 ROC曲线</p>
<p>​	PR曲线是Precision Recall Curve的简称，描述的是precision和recall之间的关系，以recall为横坐标，precision为纵坐标绘制的曲线。该曲线的所对应的面积AUC实际上是目标检测中常用的评价指标平均精度（Average Precision, AP）。AP越高，说明模型性能越好。</p>
<h3 id="2-8-3-正确率能很好的评估分类算法吗">2.8.3 正确率能很好的评估分类算法吗</h3>
<p>​	不同算法有不同特点，在不同数据集上有不同的表现效果，根据特定的任务选择不同的算法。如何评价分类算法的好坏，要做具体任务具体分析。对于决策树，主要用正确率去评估，但是其他算法，只用正确率能很好的评估吗？<br>
​	答案是否定的。<br>
​	正确率确实是一个很直观很好的评价指标，但是有时候正确率高并不能完全代表一个算法就好。比如对某个地区进行地震预测，地震分类属性分为0：不发生地震、1发生地震。我们都知道，不发生的概率是极大的，对于分类器而言，如果分类器不加思考，对每一个测试样例的类别都划分为0，达到99%的正确率，但是，问题来了，如果真的发生地震时，这个分类器毫无察觉，那带来的后果将是巨大的。很显然，99%正确率的分类器并不是我们想要的。出现这种现象的原因主要是数据分布不均衡，类别为1的数据太少，错分了类别1但达到了很高的正确率缺忽视了研究者本身最为关注的情况。</p>
<h3 id="2-8-4-什么样的分类器是最好的">2.8.4 什么样的分类器是最好的</h3>
<p>​	对某一个任务，某个具体的分类器不可能同时满足或提高所有上面介绍的指标。<br>
​	如果一个分类器能正确分对所有的实例，那么各项指标都已经达到最优，但这样的分类器往往不存在。比如之前说的地震预测，既然不能百分百预测地震的发生，但实际情况中能容忍一定程度的误报。假设在1000次预测中，共有5次预测发生了地震，真实情况中有一次发生了地震，其他4次则为误报。正确率由原来的999/1000=99.9下降为996/1000=99.6。召回率由0/1=0%上升为1/1=100%。对此解释为，虽然预测失误了4次，但真的地震发生前，分类器能预测对，没有错过，这样的分类器实际意义更为重大，正是我们想要的。在这种情况下，在一定正确率前提下，要求分类器的召回率尽量高。</p>
<h2 id="2-9-逻辑回归">2.9 逻辑回归</h2>
<h3 id="2-9-1-回归划分">2.9.1 回归划分</h3>
<p>广义线性模型家族里，依据因变量不同，可以有如下划分：</p>
<p>（1）如果是连续的，就是多重线性回归。</p>
<p>（2）如果是二项分布，就是逻辑回归。</p>
<p>（3）如果是泊松（Poisson）分布，就是泊松回归。</p>
<p>（4）如果是负二项分布，就是负二项回归。</p>
<p>（5）逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的逻辑回归。</p>
<h3 id="2-9-2-逻辑回归适用性">2.9.2 逻辑回归适用性</h3>
<p>逻辑回归可用于以下几个方面：</p>
<p>（1）用于概率预测。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。</p>
<p>（2）用于分类。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。</p>
<p>（3）寻找危险因素。寻找某一疾病的危险因素等。</p>
<p>（4）仅能用于线性问题。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。</p>
<p>（5）各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。</p>
<h3 id="2-9-3-生成模型和判别模型的区别">2.9.3 生成模型和判别模型的区别</h3>
<p>生成模型：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)（贝叶斯概率）。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。典型的生成模型有朴素贝叶斯，隐马尔科夫模型等</p>
<p>判别模型：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。这些模型的特点都是输入属性X可以直接得到后验概率P(Y|X)，输出条件概率最大的作为最终的类别（对于二分类任务来说，实际得到一个score，当score大于threshold时则为正类，否则为负类）。</p>
<p>举例：</p>
<p>判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。</p>
<p>生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。</p>
<p>联系和区别：</p>
<pre><code>生成方法的特点：上面说到，生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布P(Y,X)，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。

判别方法的特点：判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。
</code></pre>
<p>​	最后，由生成模型可以得到判别模型，但由判别模型得不到生成模型。</p>
<h3 id="2-9-4-逻辑回归与朴素贝叶斯有什么区别">2.9.4 逻辑回归与朴素贝叶斯有什么区别</h3>
<p>逻辑回归与朴素贝叶斯区别有以下几个方面：</p>
<p>（1）逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。</p>
<p>（2）朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，两种概率哲学间的区别。</p>
<p>（3）朴素贝叶斯需要条件独立假设。</p>
<p>（4）逻辑回归需要求特征参数间是线性的。</p>
<h3 id="2-9-5-线性回归与逻辑回归的区别">2.9.5 线性回归与逻辑回归的区别</h3>
<p>线性回归与逻辑回归的区别如下描述：</p>
<p>（1）线性回归的样本的输出，都是连续值， $ y\in (-\infty ,+\infty )$ ，而逻辑回归中 $y\in (0,1)$ ，只能取0和1。</p>
<p>（2）对于拟合函数也有本质上的差别：</p>
<p>​	线性回归： $f(x)=\theta ^{T}x=\theta _{1}x _{1}+\theta _{2}x _{2}+...+\theta _{n}x _{n}$</p>
<p>​	逻辑回归： $f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)$ ，其中， $g(z)=\frac{1}{1+e^{-z}}$</p>
<p>​	可以看出，线性回归的拟合函数，是对f(x)的输出变量y的拟合，而逻辑回归的拟合函数是对为1类样本的概率的拟合。</p>
<p>​	那么，为什么要以1类样本的概率进行拟合呢，为什么可以这样拟合呢？</p>
<p>​	 $\theta ^{T}x=0$ 就相当于是1类和0类的决策边界：</p>
<p>​	当 $\theta ^{T}x>0$ ，则y&gt;0.5；若 $\theta ^{T}x\rightarrow +\infty $ ，则 $y \rightarrow  1 $ ，即y为1类;</p>
<p>​	当 $\theta ^{T}x<0$ 0 ，则y&lt;0.5；若 $\theta ^{t}x\rightarrow -\infty $ ，则 $y \rightarrow ，即y为0类;< p>
<p>这个时候就能看出区别，在线性回归中 $\theta ^{T}x$ 为预测值的拟合函数；而在逻辑回归中 $\theta ^{T}x$ 为决策边界。下表2-3为线性回归和逻辑回归的区别。</p>
<p>​									表2-3 线性回归和逻辑回归的区别</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">线性回归</th>
<th style="text-align:center">逻辑回归</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">目的</td>
<td style="text-align:center">预测</td>
<td style="text-align:center">分类</td>
</tr>
<tr>
<td style="text-align:center">$y^{(i)}$</td>
<td style="text-align:center">未知</td>
<td style="text-align:center">（0,1）</td>
</tr>
<tr>
<td style="text-align:center">函数</td>
<td style="text-align:center">拟合函数</td>
<td style="text-align:center">预测函数</td>
</tr>
<tr>
<td style="text-align:center">参数计算方式</td>
<td style="text-align:center">最小二乘法</td>
<td style="text-align:center">极大似然估计</td>
</tr>
</tbody>
</table>
<p>下面具体解释一下：</p>
<ol>
<li>拟合函数和预测函数什么关系呢？简单来说就是将拟合函数做了一个逻辑函数的转换，转换后使得 $y^{(i)} \in (0,1)$ ;</li>
<li>最小二乘和最大似然估计可以相互替代吗？回答当然是不行了。我们来看看两者依仗的原理：最大似然估计是计算使得数据出现的可能性最大的参数，依仗的自然是Probability。而最小二乘是计算误差损失。</li>
</ol>
<h2 id="2-10-代价函数">2.10 代价函数</h2>
<h3 id="2-10-1-为什么需要代价函数">2.10.1 为什么需要代价函数</h3>
<ol>
<li>为了得到训练逻辑回归模型的参数，需要一个代价函数，通过训练代价函数来得到参数。</li>
<li>用于找到最优解的目的函数。</li>
</ol>
<h3 id="2-10-2-代价函数作用原理">2.10.2 代价函数作用原理</h3>
<p>​	在回归问题中，通过代价函数来求解最优解，常用的是平方误差代价函数。假设函数图像如图2-4所示，当参数发生变化时，假设函数状态也会随着变化。</p>
<p><img src="2.16/1.jpg" alt></p>
<p>​										图2-4   $h(x) = A + Bx$ 函数示意图</p>
<p>​	想要拟合图中的离散点，我们需要尽可能找到最优的 $A$ 和 $B$ 来使这条直线更能代表所有数据。如何找到最优解呢，这就需要使用代价函数来求解，以平方误差代价函数为例，假设函数为 $h(x)=\theta_0x$ 。<br>
​	<strong>平方误差代价函数的主要思想</strong>就是将实际数据给出的值与拟合出的线的对应值做差，求出拟合出的直线与实际的差距。在实际应用中，为了避免因个别极端数据产生的影响，采用类似方差再取二分之一的方式来减小个别数据的影响。因此，引出代价函数：</p>
 $$
J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2
$$ 
<p>​	<strong>最优解即为代价函数的最小值</strong> $\min J(\theta_0, \theta_1)$ 。如果是1个参数，代价函数一般通过二维曲线便可直观看出。如果是2个参数，代价函数通过三维图像可看出效果，参数越多，越复杂。<br>
当参数为2个时，代价函数是三维图像，如下图2-5所示。</p>
<p><img src="2.16/2.jpg" alt></p>
<p>​										图2-5  代价函数三维图像</p>
<h3 id="2-10-3-为什么代价函数要非负">2.10.3 为什么代价函数要非负</h3>
<p>​	目标函数存在一个下界，在优化过程当中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就能证明是收敛有效的。<br>
​	只要设计的目标函数有下界，基本上都可以，代价函数非负更为方便。</p>
<h3 id="2-10-4-常见代价函数">2.10.4 常见代价函数</h3>
<p>（1）<strong>二次代价函数（quadratic cost）</strong>：</p>
 $$
J = \frac{1}{2n}\sum_x\Vert y(x)-a^L(x)\Vert^2
$$ 
<p>​	其中， $J$ 表示代价函数， $x$ 表示样本， $y$ 表示实际值， $a$ 表示输出值， $n$ 表示样本的总数。使用一个样本为例简单说明，此时二次代价函数为：</p>
 $$
J = \frac{(y-a)^2}{2}
$$ 
<p>​	假如使用梯度下降法（Gradient descent）来调整权值参数的大小，权值 $w$ 和偏置 $b$ 的梯度推导如下：</p>
 $$
\frac{\partial J}{\partial w}=(y-a)\sigma'(z)x\;,
\frac{\partial J}{\partial b}=(y-a)\sigma'(z)
$$ 
<p>其中， $z​$ 表示神经元的输入， $\sigma​$ 表示激活函数。权值 $w​$ 和偏置 $b​$ 的梯度跟激活函数的梯度成正比，激活函数的梯度越大，权值 $w​$ 和偏置 $b​$ 的大小调整得越快，训练收敛得就越快。</p>
<p><em>注</em>：神经网络常用的激活函数为sigmoid函数，该函数的曲线如下图2-6所示：</p>
<p><img src="2.18/1.jpg" alt></p>
<p>​												图2-6 sigmoid函数曲线</p>
<p>如上图所示，对0.88和0.98两个点进行比较：<br>
​	假设目标是收敛到1.0。0.88离目标1.0比较远，梯度比较大，权值调整比较大。0.98离目标1.0比较近，梯度比较小，权值调整比较小。调整方案合理。<br>
​	假如目标是收敛到0。0.88离目标0比较近，梯度比较大，权值调整比较大。0.98离目标0比较远，梯度比较小，权值调整比较小。调整方案不合理。<br>
​	原因：在使用sigmoid函数的情况下, 初始的代价（误差）越大，导致训练越慢。</p>
<p>（2）<strong>交叉熵代价函数（cross-entropy）</strong>：</p>
 $$
J = -\frac{1}{n}\sum_x[y\ln a + (1-y)\ln{(1-a)}]
$$ 
<p>其中， $J$ 表示代价函数， $x$ 表示样本， $y$ 表示实际值， $a$ 表示输出值， $n$ 表示样本的总数。<br>
权值 $w$ 和偏置 $b​$ 的梯度推导如下：</p>
 $$
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)
$$ 
<p>当误差越大时，梯度就越大，权值 $w$ 和偏置 $b$ 调整就越快，训练的速度也就越快。<br>
<strong>二次代价函数适合输出神经元是线性的情况，交叉熵代价函数适合输出神经元是S型函数的情况。</strong></p>
<p>（3）<strong>对数似然代价函数（log-likelihood cost）</strong>：<br>
对数似然函数常用来作为softmax回归的代价函数。深度学习中普遍的做法是将softmax作为最后一层，此时常用的代价函数是对数似然代价函数。<br>
对数似然代价函数与softmax的组合和交叉熵与sigmoid函数的组合非常相似。对数似然代价函数在二分类时可以化简为交叉熵代价函数的形式。<br>
在tensorflow中：<br>
与sigmoid搭配使用的交叉熵函数：<code>tf.nn.sigmoid_cross_entropy_with_logits()</code>。<br>
与softmax搭配使用的交叉熵函数：<code>tf.nn.softmax_cross_entropy_with_logits()</code>。<br>
在pytorch中：<br>
与sigmoid搭配使用的交叉熵函数：<code>torch.nn.BCEWithLogitsLoss()</code>。<br>
与softmax搭配使用的交叉熵函数：<code>torch.nn.CrossEntropyLoss()</code>。</p>
<p>对数似然函数：</p>
<p>​	我们将似然函数作为机器学习模型的损失函数，并且用在分类问题中。这时似然函数是直接作用于模型的输出的（损失函数就是为了衡量当前参数下model的预测值predict距离真实值label的大小，所以似然函数用作损失函数时当然也是为了完成该任务），所以对于似然函数来说，这里的样本集就成了label集（而不是机器学习意义上的样本集X了），这里的参数也不是机器学习model 的参数，而是predict值。</p>
<p>其实作为损失函数的似然函数并不关心你当前的机器学习model的参数是怎样的，毕竟它此时所接收的输入只有两部分：<strong>1、predict。2、label 。3、分布模型（predict服从的分布）</strong>。</p>
<p>显然这里的label就是似然函数的观测值，即样本集。<strong>而它眼里的模型，当然就是predict这个随机变量所服从的概率分布模型。它的目的，就是衡量predict背后的模型对于当前观测值的解释程度。而每个样本的predict值，恰恰就是它所服从的分布模型的参数。</strong></p>
<p>比如此时我们的机器学习任务是一个4个类别的分类任务，机器学习model的输出就是当前样本X下的每个类别的概率，如predict=[0.1, 0.1, 0.7, 0.1]，而该样本的标签是类别3，表示成向量就是label=[0, 0, 1, 0]。那么label=[0, 0, 1, 0]就是似然函数眼里的样本，然后我们可以假设predict这个随机变量背后的模型是<strong>单次观测下的多项式分布</strong>，（<strong>因为softmax本身是基于多项式分布的</strong>）。</p>
<p>回顾：</p>
<p>伯努利分布，也叫做（0，1）分布，贝努利分布可以看成是将一枚硬币（只有正反两个面，代表两个类别）向上扔出，出现某个面（类别）的概率情况，因此其概率密度函数为：</p>
 $$
f(x)=p^x(1-p)^{1-x}=
\begin{cases}
p,& x=1\\
q,& x=0
\end{cases}
$$ 
<p>这是理解似然函数做损失函数的关键！另外，贝努利分布的模型参数就是其中一个类别的发生概率。</p>
<p>而二项分布呢，就是将贝努利实验重复n次（各次实验之间是相互独立的）。</p>
<p>而多项式分布呢，就是将二项分布推广到多个面（类别）。</p>
<p><strong>所以，单次观测下的多项式分布就是贝努利分布的多类推广！即：</strong></p>
 $$
f_{mulit}(x;p)=\prod_{i=1}^C p_{i}^{xi}
$$ 
<p>其中，C代表类别数。p代表向量形式的模型参数，即各个类别的发生概率，如p=[0.1, 0.1, 0.7, 0.1]，则p1=0.1, p3=0.7等。即，<strong>多项式分布的模型参数就是各个类别的发生概率！<strong>x代表</strong>one-hot形式</strong>的观测值，如x=类别3，则x=[0, 0, 1, 0]。xi代表x的第i个元素，比如x=类别3时，x1=0，x2=0，x3=1，x4=0。</p>
<p>想一下，机器学习model对某个样本的输出，就代表各个类别发生的概率。但是，对于当前<strong>这一个</strong>样本而言，它肯定只能有<strong>一个类别</strong>，所以这一个样本就可以看成是一次实验（观察），而这次实验（观察）的结果要服从上述各个类别发生的概率，那不就是服从多项式分布嘛！而且是单次观察！各个类别发生的概率predict当然就是这个多项式分布的参数。</p>
<p><strong>总结一下，对于多类分类问题，似然函数就是衡量当前这个以predict为参数的单次观测下的多项式分布模型与样本值label之间的似然度。</strong></p>
<p>所以，根据似然函数的定义，单个样本的似然函数即：</p>
 $$
L = f_{mulit}(label;predict)
$$ 
<p>所以，整个样本集（或者一个batch）的似然函数即：</p>
 $$
L=\prod_{X}f_{multi}(label;predict)= \prod_{X}\prod_{i=1}^{C}predict(i)^{label(i)}
$$ 
<p>所以在累乘号前面加上log函数后，就成了所谓的对数似然函数：</p>
 $$
L=\sum_{X}\sum_{i=1}^{C}label(i)log(predict(i))
$$ 
<p>而最大化对数似然函数就等效于最小化负对数似然函数，所以前面加个负号就和交叉熵的形式相同的了。</p>
<p>交叉熵定义：对于某种分布的随机变量X~p(x), 有一个模型q(x)用于近似p(x)的概率分布，则分布X与模型q之间的交叉熵即：</p>
 $$
H(X,q)=-\sum_{x}p(x)logq(x)
$$ 
<p>这里X的分布模型即样本集label的真实分布模型，这里模型q(x)即想要模拟真实分布模型的机器学习模型。可以说交叉熵是直接衡量两个分布，或者说两个model之间的差异。而似然函数则是解释以model的输出为参数的某分布模型对样本集的解释程度。因此，可以说这两者是“同貌不同源”，但是“殊途同归”啦。</p>
<p>tips：</p>
<p>最大似然估计：</p>
<p>给定一堆数据，假如我们知道它是从某一种分布中随机取出来的，可是我们并不知道这个分布具体的参，即“模型已定，参数未知”。例如，我们知道这个分布是正态分布，但是不知道均值和方差；或者是二项分布，但是不知道均值。最大似然估计（MLE，Maximum Likelihood Estimation）就可以用来估计模型的参数。<strong>MLE的目标是找出一组参数，使得模型产生出观测数据的概率最大。</strong></p>
<h3 id="2-10-5-为什么用交叉熵代替二次代价函数">2.10.5 为什么用交叉熵代替二次代价函数</h3>
<p>（1）<strong>为什么不用二次方代价函数</strong><br>
由上一节可知，权值 $w$ 和偏置 $b$ 的偏导数为 $\frac{\partial J}{\partial w}=(a-y)\sigma'(z)x$ ， $\frac{\partial J}{\partial b}=(a-y)\sigma'(z)$ ， 偏导数受激活函数的导数影响，sigmoid函数导数在输出接近0和1时非常小，会导致一些实例在刚开始训练时学习得非常慢。</p>
<p>（2）<strong>为什么要用交叉熵</strong><br>
交叉熵函数权值 $w$ 和偏置 $b$ 的梯度推导为：</p>
 $$
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)
$$ 
<p>由以上公式可知，权重学习的速度受到 $\sigma{(z)}-y$ 影响，更大的误差，就有更快的学习速度，避免了二次代价函数方程中因 $\sigma'{(z)}$ 导致的学习缓慢的情况。</p>
<h2 id="2-11-损失函数">2.11 损失函数</h2>
<h3 id="2-11-1-什么是损失函数">2.11.1 什么是损失函数</h3>
<p>​	损失函数（Loss Function）又叫做误差函数，用来衡量算法的运行情况，估量模型的预测值与真实值的不一致程度，是一个非负实值函数，通常使用 $
L(Y, f(x))​$ 来表示。损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。</p>
<h3 id="2-11-2-常见的损失函数">2.11.2 常见的损失函数</h3>
<p>​	机器学习通过对算法中的目标函数进行不断求解优化，得到最终想要的结果。分类和回归问题中，通常使用损失函数或代价函数作为目标函数。<br>
​	损失函数用来评价预测值和真实值不一样的程度。通常损失函数越好，模型的性能也越好。<br>
​	损失函数可分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是在经验风险损失函数上加上正则项。<br>
​	下面介绍常用的损失函数：</p>
<p>（1）<strong>0-1损失函数</strong><br>
如果预测值和目标值相等，值为0，如果不相等，值为1。</p>
 $$
L(Y, f(x)) =
\begin{cases}
1,& Y\ne f(x)\\
0,& Y = f(x)
\end{cases}
$$ 
<p>一般的在实际使用中，相等的条件过于严格，可适当放宽条件：</p>
 $$
L(Y, f(x)) =
\begin{cases}
1,& |Y-f(x)|\geqslant T\\
0,& |Y-f(x)|< T
\end{cases}
$$ 
<p>（2）<strong>绝对值损失函数</strong><br>
和0-1损失函数相似，绝对值损失函数表示为：</p>
 $$
L(Y, f(x)) = |Y-f(x)|​
$$ 
<p>（3）<strong>平方损失函数</strong></p>
 $$
L(Y, f(x)) = \sum_N{(Y-f(x))}^2
$$ 
<p>这点可从最小二乘法和欧几里得距离角度理解。最小二乘法的原理是，最优拟合曲线应该使所有点到回归直线的距离和最小。</p>
<p>（4）<strong>对数损失函数</strong></p>
 $$
L(Y, P(Y|X)) = -\log{P(Y|X)}=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M y_{ij}log(p_{ij})
$$ 
<p>​	其中, Y 为输出变量, X为输入变量, L 为损失函数. N为输入样本量, M为可能的类别数,  $y_{ij}$  是一个二值指标, 表示类别 j 是否是输入实例 xi 的真实类别.  $p_{ij}$  为模型或分类器预测输入实例 xi 属于类别 j 的概率.</p>
<p>常见的逻辑回归使用的就是对数损失函数，有很多人认为逻辑回归的损失函数是平方损失，其实不然。逻辑回归它假设样本服从伯努利分布（0-1分布），进而求得满足该分布的似然函数，接着取对数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看，就是对数损失函数。形式上等价于二分类的交叉熵损失函数。</p>
<p>（6）<strong>指数损失函数</strong><br>
指数损失函数的标准形式为：</p>
 $$
L(Y, f(x)) = \exp(-Yf(x))
$$ 
<p>例如AdaBoost就是以指数损失函数为损失函数。</p>
<p>（7）<strong>Hinge损失函数</strong><br>
Hinge损失函数的标准形式如下：</p>
 $$
L(y) = \max{(0, 1-ty)}
$$ 
<p>统一的形式：</p>
 $$
L(Y, f(x)) = \max{(0, Yf(x))}
$$ 
<p>其中y是预测值，范围为(-1,1)，t为目标值，其为-1或1。</p>
<p>在线性支持向量机中，最优化问题可等价于</p>
 $$
\underset{\min}{w,b}\sum_{i=1}^N (1-y_i(wx_i+b))+\lambda\Vert w\Vert ^2
$$ 
<p>上式相似于下式</p>
 $$
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i) + \Vert w\Vert ^2
$$ 
<p>其中 $l(wx_i+by_i)$ 是Hinge损失函数， $\Vert w\Vert ^2$ 可看做为正则化项。</p>
<h3 id="2-11-3-逻辑回归为什么使用对数损失函数">2.11.3 逻辑回归为什么使用对数损失函数</h3>
<p>假设逻辑回归模型</p>
 $$
P(y=1|x;\theta)=\frac{1}{1+e^{-\theta^{T}x}}
$$ 
<p>假设逻辑回归模型的概率分布是伯努利分布，其概率质量函数为：</p>
 $$
P(X=n)=
\begin{cases}
1-p, n=0\\
 p,n=1
\end{cases}
$$ 
<p>其似然函数为：</p>
 $$
L(\theta)=\prod_{i=1}^{m}
P(y=1|x_i)^{y_i}P(y=0|x_i)^{1-y_i}
$$ 
<p>对数似然函数为：</p>
 $$
\ln L(\theta)=\sum_{i=1}^{m}[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln{P(y=0|x_i)}]\\
  =\sum_{i=1}^m[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln(1-P(y=1|x_i))]
$$ 
<p>对数函数在单个数据点上的定义为：</p>
 $$
cost(y,p(y|x))=-y\ln{p(y|x)-(1-y)\ln(1-p(y|x))}
$$ 
<p>则全局样本损失函数为：</p>
 $$
cost(y,p(y|x)) = -\sum_{i=1}^m[y_i\ln p(y_i|x_i)+(1-y_i)\ln(1-p(y_i|x_i))]
$$ 
<p>由此可看出，对数损失函数与极大似然估计的对数似然函数本质上是相同的。所以逻辑回归直接采用对数损失函数。</p>
<h3 id="2-11-4-对数损失函数是如何度量损失的">2.11.4 对数损失函数是如何度量损失的</h3>
<p>​	例如，在高斯分布中，我们需要确定均值和标准差。<br>
​	如何确定这两个参数？最大似然估计是比较常用的方法。最大似然的目标是找到一些参数值，这些参数值对应的分布可以最大化观测到数据的概率。<br>
​	因为需要计算观测到所有数据的全概率，即所有观测到的数据点的联合概率。现考虑如下简化情况：</p>
<p>（1）假设观测到每个数据点的概率和其他数据点的概率是独立的。</p>
<p>（2）取自然对数。<br>
假设观测到单个数据点 $x_i(i=1,2,...n)$ 的概率为：</p>
 $$
P(x_i;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp 
		\left( - \frac{(x_i-\mu)^2}{2\sigma^2} \right)
$$ 
<p>（3）其联合概率为：</p>
 $$
P(x_1,x_2,...,x_n;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp 
		\left( - \frac{(x_1-\mu)^2}{2\sigma^2} \right) \\ \times
		 \frac{1}{\sigma \sqrt{2\pi}}\exp 
		\left( - \frac{(x_2-\mu)^2}{2\sigma^2} \right) \times ... \times
		\frac{1}{\sigma \sqrt{2\pi}}\exp 
		\left( - \frac{(x_n-\mu)^2}{2\sigma^2} \right)
$$ 
<p>​	对上式取自然对数，可得：</p>
 $$
 \ln(P(x_1,x_2,...x_n;\mu,\sigma))=
 		\ln \left(\frac{1}{\sigma \sqrt{2\pi}} \right) 
 		 - \frac{(x_1-\mu)^2}{2\sigma^2}  \\ +
 		 \ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right) 
 		 - \frac{(x_2-\mu)^2}{2\sigma^2} +...+
 		 \ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right) 
 		 - \frac{(x_n-\mu)^2}{2\sigma^2}
$$ 
<p>根据对数定律，上式可以化简为：</p>
 $$
\ln(P(x_1,x_2,...x_n;\mu,\sigma))=-n\ln(\sigma)-\frac{n}{2} \ln(2\pi)\\
     	-\frac{1}{2\sigma^2}[(x_1-\mu)^2+(x_2-\mu)^2+...+(x_n-\mu)^2]
$$ 
<p>然后求导为：</p>
 $$
\frac{\partial\ln(P(x_1,x_2,...,x_n;\mu,\sigma))}{\partial\mu}=
     			\frac{n}{\sigma^2}[\mu - (x_1+x_2+...+x_n)]
$$ 
<p>​     上式左半部分为对数损失函数。损失函数越小越好，因此我们令等式左半的对数损失函数为0，可得：</p>
 $$
\mu=\frac{x_1+x_2+...+x_n}{n}
$$ 
<p>同理，可计算 $\sigma ​$ 。</p>
<h2 id="2-12-梯度下降">2.12 梯度下降</h2>
<h3 id="2-12-1-机器学习中为什么需要梯度下降">2.12.1 机器学习中为什么需要梯度下降</h3>
<p>梯度下降是机器学习中常见优化算法之一，梯度下降法有以下几个作用：</p>
<p>（1）梯度下降是迭代法的一种，可以用于求解最小二乘问题。</p>
<p>（2）在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。</p>
<p>（3）在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。</p>
<p>（4）如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。</p>
<p>（5）在机器学习中，梯度下降法主要有随机梯度下降法和批量梯度下降法。</p>
<h3 id="2-12-2-梯度下降法缺点">2.12.2 梯度下降法缺点</h3>
<p>梯度下降法缺点有以下几点：</p>
<p>（1）靠近极小值时收敛速度减慢。</p>
<p>（2）直线搜索时可能会产生一些问题。</p>
<p>（3）可能会“之字形”地下降。</p>
<p>梯度概念也有需注意的地方：</p>
<p>（1）梯度是一个向量，即有方向有大小。</p>
<p>（2）梯度的方向是最大方向导数的方向。</p>
<p>（3）梯度的值是最大方向导数的值。</p>
<h3 id="2-12-3-梯度下降法直观理解">2.12.3 梯度下降法直观理解</h3>
<p>梯度下降法经典图示如下图2.7所示：</p>
<p><img src="2.25/1.png" alt></p>
<p>​									图2.7 梯度下降法经典图示</p>
<p>​	形象化举例，由上图2.7所示，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。<br>
​	由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<p><strong>核心思想归纳</strong>：</p>
<p>（1）初始化参数，随机选取取值范围内的任意数；</p>
<p>（2）迭代操作：<br>
a）计算当前梯度；<br>
b）修改新的变量；<br>
c）计算朝最陡的下坡方向走一步；<br>
d）判断是否需要终止，如否，返回a）；</p>
<p>（3）得到全局最优解或者接近全局最优解。</p>
<h3 id="2-12-4-梯度下降法算法描述">2.12.4 梯度下降法算法描述</h3>
<p>梯度下降法算法步骤如下：</p>
<p>（1）确定优化模型的假设函数及损失函数。<br>
​	举例，对于线性回归，假设函数为：</p>
 $$
  h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n
$$ 
<p>其中， $\theta_i,x_i(i=0,1,2,...,n)$ 分别为模型参数、每个样本的特征值。<br>
对于假设函数，损失函数为：</p>
 $$
  J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
  	,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2
$$ 
<p>（2）相关参数初始化。<br>
​	主要初始化 ${\theta}_i$ 、算法迭代步长 ${\alpha} $ 、终止距离 ${\zeta} $ 。初始化时可以根据经验初始化，即 ${\theta} $ 初始化为0，步长 ${\alpha} $ 初始化为1。当前步长记为 ${\varphi}_i $ 。当然，也可随机初始化。</p>
<p>（3）迭代计算。</p>
<p>​	1）计算当前位置时损失函数的梯度，对 ${\theta}_i $ ，其梯度表示为：</p>
 $$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
	,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2
$$ 
<p>​	2）计算当前位置下降的距离。</p>
 $$
{\varphi}_i={\alpha} \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)
$$ 
<p>​	3）判断是否终止。<br>
​	确定是否所有 ${\theta}_i$ 梯度下降的距离 ${\varphi}_i$ 都小于终止距离 ${\zeta}$ ，如果都小于 ${\zeta}$ ，则算法终止，当然的值即为最终结果，否则进入下一步。<br>
​	4）更新所有的 ${\theta}_i$ ，更新后的表达式为：</p>
 $$
{\theta}_i={\theta}_i-\alpha \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)
$$ 
 $$
\theta_i=\theta_i - \alpha \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{(j)}_0
	,x^{(j)}_1,...,x^{(j)}_n)-y_j)x^{(j)}_i
$$ 
<p>​	5）令上式 $x^{(j)}_0=1$ ，更新完毕后转入1)。<br>
​	由此，可看出，当前位置的梯度方向由所有样本决定，上式中  $\frac{1}{m}​$ 、 $\alpha \frac{1}{m}​$  的目的是为了便于理解。</p>
<h3 id="2-12-5-如何对梯度下降法进行调优">2.12.5 如何对梯度下降法进行调优</h3>
<p>实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：</p>
<p>（1）<strong>算法迭代步长 $\alpha$ 选择。</strong><br>
在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。</p>
<p>（2）<strong>参数的初始值选择。</strong><br>
初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
<p>（3）<strong>标准化处理。</strong><br>
由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。</p>
<h3 id="2-12-6-随机梯度和批量梯度区别">2.12.6 随机梯度和批量梯度区别</h3>
<p>​	随机梯度下降（SGD）和批量梯度下降（BGD）是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。<br>
下面通过介绍两种梯度下降法的求解思路，对其进行比较。<br>
假设函数为：</p>
 $$
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n
$$ 
<p>损失函数为：</p>
 $$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)^2
$$ 
<p>其中， $m​$ 为样本个数， $j​$ 为参数个数。</p>
<p>1、 <strong>批量梯度下降的求解思路如下：</strong><br>
a) 得到每个 $ \theta ​$ 对应的梯度：</p>
 $$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$ 
<p>b) 由于是求最小化风险函数，所以按每个参数  $ \theta ​$  的梯度负方向更新  $ \theta_i ​$  ：</p>
 $$
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$ 
<p>c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，如果样本数据很大，这种方法迭代速度就很慢。<br>
相比而言，随机梯度下降可避免这种问题。</p>
<p>2、<strong>随机梯度下降的求解思路如下：</strong><br>
a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是训练集中每个样本的粒度。<br>
损失函数可以写成如下这种形式，</p>
 $$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
			,x^{j}_1,...,x^{j}_n))^2 = 
			\frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))
$$ 
<p>b）对每个参数  $ \theta​$  按梯度方向更新  $ \theta​$ ：</p>
 $$
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))
$$ 
<p>c) 随机梯度下降是通过每个样本来迭代更新一次。<br>
随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。</p>
<p><strong>小结：</strong><br>
随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">批量梯度下降</td>
<td style="text-align:left">a）采用所有数据来梯度下降。<br>b）批量梯度下降法在样本量很大的时候，训练速度慢。</td>
</tr>
<tr>
<td style="text-align:center">随机梯度下降</td>
<td style="text-align:left">a）随机梯度下降用一个样本来梯度下降。<br>b）训练速度很快。<br>c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br>d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</td>
</tr>
</tbody>
</table>
<p>下面介绍能结合两种方法优点的小批量梯度下降法。</p>
<p>3、 <strong>小批量（Mini-Batch）梯度下降的求解思路如下</strong><br>
对于总数为 $m$ 个样本的数据，根据样本的数据，选取其中的 $n(1< n< m)$ 个子样本来迭代。其参数 $\theta$ 按梯度方向更新 $\theta_i$ 公式如下：</p>
 $$
\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}
		( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}
$$ 
<h3 id="2-12-7-各种梯度下降法性能比较">2.12.7 各种梯度下降法性能比较</h3>
<p>​	下表简单对比随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和Online GD的区别：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">BGD</th>
<th style="text-align:center">SGD</th>
<th style="text-align:center">Mini-batch GD</th>
<th style="text-align:center">Online GD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">训练集</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">实时更新</td>
</tr>
<tr>
<td style="text-align:center">单次迭代样本数</td>
<td style="text-align:center">整个训练集</td>
<td style="text-align:center">单个样本</td>
<td style="text-align:center">训练集的子集</td>
<td style="text-align:center">根据具体算法定</td>
</tr>
<tr>
<td style="text-align:center">算法复杂度</td>
<td style="text-align:center">高</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">低</td>
</tr>
<tr>
<td style="text-align:center">时效性</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">高</td>
</tr>
<tr>
<td style="text-align:center">收敛性</td>
<td style="text-align:center">稳定</td>
<td style="text-align:center">不稳定</td>
<td style="text-align:center">较稳定</td>
<td style="text-align:center">不稳定</td>
</tr>
</tbody>
</table>
<p>BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下Online GD。</p>
<p>​	Online GD于Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。</p>
<p>​	Online GD在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。</p>
<h2 id="2-13-自然梯度法">2.13 自然梯度法</h2>
<p><strong>（贡献者：郜泉凯－华南理工大学）</strong></p>
<h3 id="2-13-1-为什么我们需要自然梯度">2.13.1 为什么我们需要自然梯度</h3>
<p>传统的梯度下降方法是在欧氏空间进行、并与时序过程结合的优化方法，但这样的更新过程无法度量由于参数变化引起的概率属性的变化（这一点也可以认为是传统梯度下降方法的缺点）。在如强化学习等很多应用领域关注模型输出的概率分布，优化过程常常需要在一定概率属性的约束下完成，这就需要自然梯度。</p>
<h3 id="2-12-2-如何定义自然梯度">2.12.2 如何定义自然梯度</h3>
<p>若度量模型参数变化引起的概率分布变化，常用的“距离”度量是KL散度（Kullback-Leibler divergence）。设模型概率分布为 $p(x;\theta)$ ，其与参数变动后的概率分布间的KL散度为：</p>
 $$
D_{KL}(p(x;\theta)||p(x;\theta+\delta\theta))=\int p(x;\theta)log\frac {p(x;\theta)}{p(x;\theta+\delta\theta)}dx
$$ 
<p>我们令 $f(\theta+\delta\theta)=log p(x;\theta+\delta\theta)$ ，做泰勒展开取二阶近似（忽略高阶余项）得到：</p>
 $$
f(\theta+\delta\theta)\approx f(\theta)+\delta\theta^T\frac{\partial f(\theta)}{\partial\theta}+\frac{1}{2}\delta\theta^T\frac{\partial f(\theta)}{\partial\theta}\frac{\partial f(\theta)^T}{\partial\theta}\delta\theta
$$ 
<p>带入到 $D_{KL}(p(x;\theta)||p(x;\theta+\delta\theta))$ 中可得到：</p>
 $$
\begin{eqnarray}
D_{KL}(p(x;\theta)||p(x;\theta+\delta\theta))&=&\int p(x;\theta)(f(\theta)-f(\theta+\delta\theta))dx\\
&=&-\int p(x;\theta)(\delta\theta^T\frac{\partial f(\theta)}{\partial\theta}+\frac{1}{2}\delta\theta^T\frac{\partial f(\theta)}{\partial\theta}\frac{\partial f(\theta)^T}{\partial\theta}\delta\theta)dx\\
&=&-\delta\theta^T\int p(x;\theta)\frac{\partial logp(x;\theta)}{\partial\theta}dx\\
&-&\frac{1}{2}\delta\theta^T\int p(x;\theta)\frac{\partial f(\theta)}{\partial\theta}\frac{\partial f(\theta)^T}{\partial\theta}dx\delta\theta\\
&=&-\delta\theta^T\int p(x;\theta)\frac{\frac{\partial p(x;\theta)}{\partial\theta}}{p(x;\theta)}dx-\frac{1}{2}\delta\theta^TG\delta\theta\\
&=&-\frac{1}{2}\delta\theta^TG\delta\theta
\end{eqnarray}
$$ 
<p>我们记在KL散度意义下的参数增量为 $\delta\theta_G$ ，接下来我们寻求在 $||\delta\theta_G||^2=\epsilon$ 约束下 $\delta\theta_G$ 的方向，使得目标函数 $J(\theta)$ 下降最快,即 $J(\theta+\delta\theta)-J(\theta)$ 最大。应用拉格朗日乘子法：</p>
 $$
\max_{\delta\theta}J(\theta+\delta\theta)-J(\theta)-\lambda(||\delta\theta_G||^2-\epsilon)
$$ 
<p>应用一阶泰勒展开等价于:</p>
 $$
\max_{\delta\theta}\nabla \delta\theta^T J(\theta)-\frac{1}{2}\lambda\delta\theta^TG\delta\theta
$$ 
<p>对 $\delta\theta$ 求导得 $\nabla J(\theta)-\lambda G\delta\theta=0$ ，即 $\delta\theta=\frac{1}{\lambda}G^{-1}\nabla J(\theta)$ ，其中 $G^{-1}\nabla J(\theta)$ 称为自然梯度，相应的自然梯度下降公式为 $\theta_{k+1}=\theta_k-\alpha_kG^{-1}(\theta_k)\nabla J(\theta_K)$ 。</p>
<h3 id="2-12-3-Fisher信息矩阵的意义">2.12.3 Fisher信息矩阵的意义</h3>
<p>首先我们对一个模型进行建模，成为以 $\theta$ 为参数的概率分布 $p(x;\theta)$ 。为求出一个合理的 $\theta$ 我们需要一个评分函数（score function）： $s(\theta)=\nabla_{\theta}logp(x;\theta)$ ，意为对数似然的梯度，当分数为0时（对数似然梯度为0），对数似然达到极值。对评分函数求关于 $p(x;\theta)$ 数学期望 $p_E$ 不难发现期望为0。接下来求估计误差的界，我们用评分函数的方差来确定，即 $E_{p(x;\theta)}[(s(\theta)-p_E)(s(\theta-p_E)^T)]$ 。带入评分函数的数学表达形式则等价于Fisher信息矩阵 $G(\theta)=\int p(x;\theta)\frac{\partial f(\theta)}{\partial\theta}\frac{\partial f(\theta)^T}{\partial\theta}dx$ 。特别地，Fisher信息矩阵与评分函数 $\nabla_{\theta}logp(x;\theta)$ 的Hessian似然的负数等价。</p>
<p>证明：首先求出评分函数的Hessian矩阵，由梯度的Jacobian决定</p>
 $$
\begin{eqnarray}
H_{logp(x;\theta)}&=&J(\frac{\nabla p(x;\theta)}{p(x;\theta)})\\
&=&\frac{\frac{\partial\nabla p(x;\theta)}{\partial\theta}p(x;\theta)-\nabla p(x;\theta)\nabla p(x;\theta)^T}{p(x;\theta)p(x;\theta)}\\
&=&\frac{H_{p(x;\theta)}p(x;\theta)}{p(x;\theta)p(x;\theta)}-\frac{\nabla p(x;\theta)\nabla p(x;\theta)^T}{p(x;\theta)p(x;\theta)}\\
\end{eqnarray}
$$ 
<p>等式两边同时求关于 $p(x;\theta)$ 的数学期望：</p>
 $$
\begin{eqnarray}
E_{p(x;\theta)}[H_{logp(x;\theta)}] &=& E_{p(x;\theta)}(\frac{H_{p(x;\theta)}p(x;\theta)}{p(x;\theta)p(x;\theta)})-G\\
&=&\int\frac{H_{p(x;\theta)}}{p(x;\theta)}p(x;\theta)dx-G\\
&=&\nabla^2\int p(x;\theta)dx-G\\
&=&-G
\end{eqnarray}
$$ 
<p>而Hessian矩阵刻画着对数似然函数的曲率，所以本质上自然梯度下降法是在一个消除了不同概率分布的曲率后，在同一个“平坦”曲面上进行迭代更新，步长等于原概率分布空间的步长按照曲率折合到新的“平坦曲面”的大小。</p>
<p>值得注意的一点是，一般来说似然函数获取很难，在实际问题中，我们可以用采样的方法从数据集中采样数据，将Fisher信息矩阵原始表达式的积分变为求和来近似估计，这样的方式得到的Fisher信息矩阵称为经验Fisher。</p>
<h2 id="2-14-线性判别分析（LDA）">2.14 线性判别分析（LDA）</h2>
<h3 id="2-14-1-LDA思想总结">2.14.1 LDA思想总结</h3>
<p>​	线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。</p>
<p>LDA分类思想简单总结如下：</p>
<ol>
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。</li>
<li>对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。</li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。</li>
</ol>
<p>如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</p>
<h3 id="2-14-2-图解LDA核心思想">2.14.2 图解LDA核心思想</h3>
<p>​	假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<p><img src="2.29/1.png" alt></p>
<p>左图和右图是两种不同的投影方式。</p>
<p>​	左图思路：让不同类别的平均点距离最远的投影方式。</p>
<p>​	右图思路：让同类别的数据挨得最近的投影方式。</p>
<p>​	从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。</p>
<p>​	以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<h3 id="2-14-3-二类LDA算法原理">2.14.3 二类LDA算法原理</h3>
<p>​	输入：数据集  $D=\{(\boldsymbol x_1,\boldsymbol y_1),(\boldsymbol x_2,\boldsymbol y_2),...,(\boldsymbol x_m,\boldsymbol y_m)\}​$ ，其中样本  $\boldsymbol x_i ​$  是n维向量， $\boldsymbol y_i  \epsilon \{0, 1\}​$ ，降维后的目标维度  $d​$ 。定义</p>
<p>​	 $N_j(j=0,1)$  为第  $j$  类样本个数；</p>
<p>​	 $X_j(j=0,1)$  为第  $j$  类样本的集合；</p>
<p>​	 $u_j(j=0,1)​$  为第  $j​$  类样本的均值向量；</p>
<p>​	 $\sum_j(j=0,1)$  为第  $j$  类样本的协方差矩阵。</p>
<p>​	其中</p>
 $$
u_j = \frac{1}{N_j} \sum_{\boldsymbol x\epsilon X_j}\boldsymbol x(j=0,1)， 
\sum_j = \sum_{\boldsymbol x\epsilon X_j}(\boldsymbol x-u_j)(\boldsymbol x-u_j)^T(j=0,1)
$$ 
<p>​	假设投影直线是向量  $\boldsymbol w$ ，对任意样本  $\boldsymbol x_i$ ，它在直线  $w$ 上的投影为  $\boldsymbol w^Tx_i$ ，两个类别的中心点  $u_0$ ,  $u_1 $ 在直线  $w$  的投影分别为  $\boldsymbol w^Tu_0$  、 $\boldsymbol w^Tu_1$ 。</p>
<p>​	LDA的目标是让两类别的数据中心间的距离  $\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2$  尽量大，与此同时，希望同类样本投影点的协方差 $\boldsymbol w^T \sum_0 \boldsymbol w$ 、 $\boldsymbol w^T \sum_1 \boldsymbol w$  尽量小，最小化  $\boldsymbol w^T \sum_0 \boldsymbol w + \boldsymbol w^T \sum_1 \boldsymbol w​$  。<br>
​	定义<br>
​	类内散度矩阵</p>
 $$
S_w = \sum_0 + \sum_1 = 
	\sum_{\boldsymbol x\epsilon X_0}(\boldsymbol x-u_0)(\boldsymbol x-u_0)^T + 
	\sum_{\boldsymbol x\epsilon X_1}(\boldsymbol x-u_1)(\boldsymbol x-u_1)^T
$$ 
<p>​	类间散度矩阵  $S_b = (u_0 - u_1)(u_0 - u_1)^T$</p>
<p>​	据上分析，优化目标为</p>
 $$
\mathop{\arg\max}_\boldsymbol w J(\boldsymbol w) = \frac{\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2}{\boldsymbol w^T \sum_0\boldsymbol w + \boldsymbol w^T \sum_1\boldsymbol w} = 
\frac{\boldsymbol w^T(u_0-u_1)(u_0-u_1)^T\boldsymbol w}{\boldsymbol w^T(\sum_0 + \sum_1)\boldsymbol w} =
\frac{\boldsymbol w^TS_b\boldsymbol w}{\boldsymbol w^TS_w\boldsymbol w}
$$ 
<p>​	根据广义瑞利商的性质，矩阵  $S^{-1}_{w} S_b$  的最大特征值为  $J(\boldsymbol w)$  的最大值，矩阵  $S^{-1}_{w} S_b$  的最大特征值对应的特征向量即为  $\boldsymbol w$ 。</p>
<h3 id="2-14-4-LDA算法流程总结">2.14.4 LDA算法流程总结</h3>
<p>LDA算法降维流程如下：</p>
<p>​	输入：数据集  $D = \{ (x_1,y_1),(x_2,y_2), ... ,(x_m,y_m) \}$ ，其中样本  $x_i $  是n维向量， $y_i  \epsilon \{C_1, C_2, ..., C_k\}$ ，降维后的目标维度  $d$  。</p>
<p>​	输出：降维后的数据集  $\overline{D} $  。</p>
<p>步骤：</p>
<ol>
<li>计算类内散度矩阵  $S_w$ 。</li>
<li>计算类间散度矩阵  $S_b​$  。</li>
<li>计算矩阵  $S^{-1}_wS_b​$  。</li>
<li>计算矩阵  $S^{-1}_wS_b$  的最大的 d 个特征值。</li>
<li>计算 d 个特征值对应的 d 个特征向量，记投影矩阵为 W 。</li>
<li>转化样本集的每个样本，得到新样本  $P_i = W^Tx_i​$  。</li>
<li>输出新样本集  $\overline{D} = \{ (p_1,y_1),(p_2,y_2),...,(p_m,y_m) \}​$</li>
</ol>
<h3 id="2-14-5-LDA和PCA区别">2.14.5 LDA和PCA区别</h3>
<table>
<thead>
<tr>
<th style="text-align:center">异同点</th>
<th style="text-align:left">LDA</th>
<th style="text-align:left">PCA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">相同点</td>
<td style="text-align:left">1. 两者均可以对数据进行降维；<br>2. 两者在降维时均使用了矩阵特征分解的思想；<br>3. 两者都假设数据符合高斯分布；</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">不同点</td>
<td style="text-align:left">有监督的降维方法；</td>
<td style="text-align:left">无监督的降维方法；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">降维最多降到k-1维；</td>
<td style="text-align:left">降维多少没有限制；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">可以用于降维，还可以用于分类；</td>
<td style="text-align:left">只用于降维；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">选择分类性能最好的投影方向；</td>
<td style="text-align:left">选择样本点投影具有最大方差的方向；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">更明确，更能反映样本间差异；</td>
<td style="text-align:left">目的较为模糊；</td>
</tr>
</tbody>
</table>
<h3 id="2-14-6-LDA优缺点">2.14.6 LDA优缺点</h3>
<table>
<thead>
<tr>
<th style="text-align:center">优缺点</th>
<th style="text-align:left">简要说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">优点</td>
<td style="text-align:left">1. 可以使用类别的先验知识；<br>2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr>
<td style="text-align:center">缺点</td>
<td style="text-align:left">1. LDA不适合对非高斯分布样本进行降维；<br>2. LDA降维最多降到分类数k-1维；<br>3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好；<br>4. LDA可能过度拟合数据。</td>
</tr>
</tbody>
</table>
<h2 id="2-15-主成分分析（PCA）">2.15  主成分分析（PCA）</h2>
<h3 id="2-15-1-主成分分析（PCA）思想总结">2.15.1 主成分分析（PCA）思想总结</h3>
<ol>
<li>PCA就是将高维的数据通过线性变换投影到低维空间上去。</li>
<li>投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。</li>
<li>去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。</li>
<li>去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。</li>
<li>对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。</li>
<li>完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。</li>
<li>之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。</li>
</ol>
<h3 id="2-15-2-图解PCA核心思想">2.15.2 图解PCA核心思想</h3>
<p>​	PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n &lt; m），这n维形成主元，是重构出来最能代表原始数据的正交特征。</p>
<p>​	假设数据集是m个n维， $(\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \cdots, \boldsymbol x^{(m)})$ 。如果 $n=2$ ，需要降维到 $n'=1$ ，现在想找到某一维度方向代表这两个维度的数据。下图有 $u_1, u_2$ 两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？</p>
<p><img src="2.34/1.png" alt></p>
<p>从图可看出， $u_1$ 比 $u_2$ 好，为什么呢？有以下两个主要评价指标：</p>
<ol>
<li>样本点到这个直线的距离足够近。</li>
<li>样本点在这个直线上的投影能尽可能的分开。</li>
</ol>
<p>如果我们需要降维的目标维数是其他任意维，则：</p>
<ol>
<li>样本点到这个超平面的距离足够近。</li>
<li>样本点在这个超平面上的投影能尽可能的分开。</li>
</ol>
<h3 id="2-15-3-PCA算法推理">2.15.3 PCA算法推理</h3>
<p>下面以基于最小投影距离为评价指标推理：</p>
<p>​	假设数据集是m个n维， $(x^{(1)}, x^{(2)},...,x^{(m)})$ ，且数据进行了中心化。经过投影变换得到新坐标为  ${w_1,w_2,...,w_n}$ ，其中  $w$  是标准正交基，即  $\| w \|_2 = 1$ ， $w^T_iw_j = 0$ 。</p>
<p>​	经过降维后，新坐标为  $\{ w_1,w_2,...,w_n \}$ ，其中  $n'$  是降维后的目标维数。样本点  $x^{(i)}$  在新坐标系下的投影为  $z^{(i)} = \left(z^{(i)}_1, z^{(i)}_2, ..., z^{(i)}_{n'}   \right)$ ，其中  $z^{(i)}_j = w^T_j x^{(i)}$  是  $x^{(i)} ​$  在低维坐标系里第 j 维的坐标。</p>
<p>​	如果用  $z^{(i)} $  去恢复  $x^{(i)} $  ，则得到的恢复数据为  $\widehat{x}^{(i)} = \sum^{n'}_{j=1} x^{(i)}_j w_j = Wz^{(i)}$ ，其中  $W$ 为标准正交基组成的矩阵。</p>
<p>​	考虑到整个样本集，样本点到这个超平面的距离足够近，目标变为最小化  $\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2$  。对此式进行推理，可得：</p>
 $$
\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2 = 
	\sum^m_{i=1} \| Wz^{(i)} - x^{(i)} \|^2_2 \\
	= \sum^m_{i=1} \left( Wz^{(i)} \right)^T \left( Wz^{(i)} \right)
	- 2\sum^m_{i=1} \left( Wz^{(i)} \right)^T x^{(i)}
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
	= \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right)
	- 2\sum^m_{i=1} \left( z^{(i)} \right)^T x^{(i)}
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
	= - \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right)
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
	= -tr \left( W^T \left( \sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T \right)W \right)
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
	= -tr \left( W^TXX^TW \right)
	+ \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)}
$$ 
<p>​	在推导过程中，分别用到了  $\overline{x}^{(i)} = Wz^{(i)}$  ，矩阵转置公式  $(AB)^T = B^TA^T$ ， $W^TW = I$ ， $z^{(i)} = W^Tx^{(i)}$  以及矩阵的迹，最后两步是将代数和转为矩阵形式。<br>
​	由于  $W$  的每一个向量  $w_j$  是标准正交基， $\sum^m_{i=1} x^{(i)} \left(  x^{(i)} \right)^T$  是数据集的协方差矩阵， $\sum^m_{i=1} \left(  x^{(i)} \right)^T x^{(i)} $  是一个常量。最小化  $\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2$  又可等价于</p>
 $$
\underbrace{\arg \min}_W - tr \left( W^TXX^TW \right) s.t.W^TW = I
$$ 
<p>利用拉格朗日函数可得到</p>
 $$
J(W) = -tr(W^TXX^TW) + \lambda(W^TW - I)
$$ 
<p>​	对  $W$  求导，可得  $-XX^TW + \lambda W = 0 $  ，也即  $ XX^TW = \lambda W $  。  $ XX^T $  是  $ n' $  个特征向量组成的矩阵， $\lambda$  为 $ XX^T $  的特征值。 $W$  即为我们想要的矩阵。<br>
​	对于原始数据，只需要  $z^{(i)} = W^TX^{(i)}$  ，就可把原始数据集降维到最小投影距离的  $n'$  维数据集。</p>
<p>​	基于最大投影方差的推导，这里就不再赘述，有兴趣的同仁可自行查阅资料。</p>
<h3 id="2-15-4-PCA算法流程总结">2.15.4 PCA算法流程总结</h3>
<p>输入： $n​$  维样本集  $D = \left( x^{(1)},x^{(2)},...,x^{(m)} \right)​$  ，目标降维的维数  $n'​$  。</p>
<p>输出：降维后的新样本集  $D'  = \left( z^{(1)},z^{(2)},...,z^{(m)} \right)$  。</p>
<p>主要步骤如下：</p>
<ol>
<li>对所有的样本进行中心化， $ x^{(i)} = x^{(i)} - \frac{1}{m} \sum^m_{j=1} x^{(j)} $  。</li>
<li>计算样本的协方差矩阵  $XX^T​$  。</li>
<li>对协方差矩阵  $XX^T$  进行特征值分解。</li>
<li>取出最大的  $n' $  个特征值对应的特征向量  $\{ w_1,w_2,...,w_{n'} \}$  。</li>
<li>标准化特征向量，得到特征向量矩阵  $W$  。</li>
<li>转化样本集中的每个样本  $z^{(i)} = W^T x^{(i)}$  。</li>
<li>得到输出矩阵  $D' = \left( z^{(1)},z^{(2)},...,z^{(n)} \right)​$  。<br>
<em>注</em>：在降维时，有时不明确目标维数，而是指定降维到的主成分比重阈值  $k(k \epsilon(0,1])​$  。假设  $n​$  个特征值为  $\lambda_1 \geqslant \lambda_2 \geqslant ... \geqslant \lambda_n​$  ，则  $n'​$  可从  $\sum^{n'}_{i=1} \lambda_i \geqslant k \times \sum^n_{i=1} \lambda_i ​$  得到。</li>
</ol>
<h3 id="2-15-5-PCA算法主要优缺点">2.15.5 PCA算法主要优缺点</h3>
<table>
<thead>
<tr>
<th style="text-align:center">优缺点</th>
<th style="text-align:left">简要说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">优点</td>
<td style="text-align:left">1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　2.各主成分之间正交，可消除原始数据成分间的相互影响的因素。3. 计算方法简单，主要运算是特征值分解，易于实现。</td>
</tr>
<tr>
<td style="text-align:center">缺点</td>
<td style="text-align:left">1.主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。</td>
</tr>
</tbody>
</table>
<h3 id="2-15-6-降维的必要性及目的">2.15.6 降维的必要性及目的</h3>
<p><strong>降维的必要性</strong>：</p>
<ol>
<li>多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。</li>
<li>高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。</li>
<li>过多的变量，对查找规律造成冗余麻烦。</li>
<li>仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。</li>
</ol>
<p><strong>降维的目的</strong>：</p>
<ol>
<li>减少预测变量的个数。</li>
<li>确保这些变量是相互独立的。</li>
<li>提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。</li>
<li>数据在低维下更容易处理、更容易使用。</li>
<li>去除数据噪声。</li>
<li>降低算法运算开销。</li>
</ol>
<h3 id="2-15-7-KPCA与PCA的区别">2.15.7 KPCA与PCA的区别</h3>
<p>​	应用PCA算法前提是假设存在一个线性超平面，进而投影。那如果数据不是线性的呢？该怎么办？这时候就需要KPCA，数据集从  $n$  维映射到线性可分的高维  $N >n$ ，然后再从  $N$  维降维到一个低维度  $n'(n'<n<N)$ 。< p>
<p>​	KPCA用到了核函数思想，使用了核函数的主成分分析一般称为核主成分分析(Kernelized PCA, 简称KPCA）。</p>
<p>假设高维空间数据由  $n​$  维空间的数据通过映射  $\phi​$  产生。</p>
<p>​	 $n$  维空间的特征分解为：</p>
 $$
\sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T W = \lambda W
$$ 
<p>​	其映射为</p>
 $$
\sum^m_{i=1} \phi \left( x^{(i)} \right) \phi \left( x^{(i)} \right)^T W = \lambda W
$$ 
<p>​	通过在高维空间进行协方差矩阵的特征值分解，然后用和PCA一样的方法进行降维。由于KPCA需要核函数的运算，因此它的计算量要比PCA大很多。</p>
<h2 id="2-16-模型评估">2.16 模型评估</h2>
<h3 id="2-16-1-模型评估常用方法？">2.16.1 模型评估常用方法？</h3>
<p>​	一般情况来说，单一评分标准无法完全评估一个机器学习模型。只用good和bad偏离真实场景去评估某个模型，都是一种欠妥的评估方式。下面介绍常用的分类模型和回归模型评估方法。</p>
<p><strong>分类模型常用评估方法：</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">指标</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Accuracy</td>
<td style="text-align:center">准确率</td>
</tr>
<tr>
<td style="text-align:center">Precision</td>
<td style="text-align:center">精准度/查准率</td>
</tr>
<tr>
<td style="text-align:center">Recall</td>
<td style="text-align:center">召回率/查全率</td>
</tr>
<tr>
<td style="text-align:center">P-R曲线</td>
<td style="text-align:center">查准率为纵轴，查全率为横轴，作图</td>
</tr>
<tr>
<td style="text-align:center">F1</td>
<td style="text-align:center">F1值</td>
</tr>
<tr>
<td style="text-align:center">Confusion Matrix</td>
<td style="text-align:center">混淆矩阵</td>
</tr>
<tr>
<td style="text-align:center">ROC</td>
<td style="text-align:center">ROC曲线</td>
</tr>
<tr>
<td style="text-align:center">AUC</td>
<td style="text-align:center">ROC曲线下的面积</td>
</tr>
</tbody>
</table>
<p><strong>回归模型常用评估方法：</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">指标</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Mean Square Error (MSE, RMSE)</td>
<td style="text-align:center">平均方差</td>
</tr>
<tr>
<td style="text-align:center">Absolute Error (MAE, RAE)</td>
<td style="text-align:center">绝对误差</td>
</tr>
<tr>
<td style="text-align:center">R-Squared</td>
<td style="text-align:center">R平方值</td>
</tr>
</tbody>
</table>
<h3 id="2-16-2-误差、偏差和方差有什么区别和联系">2.16.2 误差、偏差和方差有什么区别和联系</h3>
<p>在机器学习中，Bias(偏差)，Error(误差)，和Variance(方差)存在以下区别和联系：</p>
<p>**对于Error **：</p>
<ul>
<li>
<p>误差（error）：一般地，我们把学习器的实际预测输出与样本的真是输出之间的差异称为“误差”。</p>
</li>
<li>
<p>Error = Bias + Variance + Noise，Error反映的是整个模型的准确度。</p>
</li>
</ul>
<p><strong>对于Noise:</strong></p>
<p>噪声：描述了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p>
<p><strong>对于Bias：</strong></p>
<ul>
<li>Bias衡量模型拟合训练数据的能力（训练数据不一定是整个 training dataset，而是只用于训练它的那一部分数据，例如：mini-batch），Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度。</li>
<li>Bias 越小，拟合能力越高（可能产生overfitting）；反之，拟合能力越低（可能产生underfitting）。</li>
<li>偏差越大，越偏离真实数据，如下图第二行所示。</li>
</ul>
<p><strong>对于Variance：</strong></p>
<ul>
<li>
<p>方差公式： $S_{N}^{2}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}$</p>
</li>
<li>
<p>Variance描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，模型的稳定程度越差。</p>
</li>
<li>
<p>Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。</p>
</li>
<li>
<p>Variance越小，模型的泛化的能力越高；反之，模型的泛化的能力越低。</p>
</li>
<li>
<p>如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。 如下图右列所示。</p>
</li>
</ul>
<blockquote>
<p><img src="2.16.20.1.png" alt></p>
</blockquote>
<h3 id="2-16-3-经验误差与泛化误差">2.16.3 经验误差与泛化误差</h3>
<p>经验误差（empirical error）：也叫训练误差（training error），模型在训练集上的误差。</p>
<p>泛化误差（generalization error）：模型在新样本集（测试集）上的误差称为“泛化误差”。</p>
<h3 id="2-16-4-图解欠拟合、过拟合">2.16.4 图解欠拟合、过拟合</h3>
<p>根据不同的坐标方式，欠拟合与过拟合图解不同。</p>
<ol>
<li><strong>横轴为训练样本数量，纵轴为误差</strong></li>
</ol>
<p><img src="2.16.4.1.jpg" alt></p>
<p>如上图所示，我们可以直观看出欠拟合和过拟合的区别：</p>
<p>​	模型欠拟合：在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大；</p>
<p>​	模型过拟合：在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</p>
<p>​	模型正常：在训练集以及测试集上，同时具有相对较低的偏差以及方差。</p>
<ol start="2">
<li><strong>横轴为模型复杂程度，纵轴为误差</strong></li>
</ol>
<p><img src="2.16.4.2.png" alt></p>
<p>​					红线为测试集上的Error,蓝线为训练集上的Error</p>
<p>​	模型欠拟合：模型在点A处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</p>
<p>​	模型过拟合：模型在点C处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</p>
<p>​	模型正常：模型复杂程度控制在点B处为最优。</p>
<ol start="3">
<li><strong>横轴为正则项系数，纵轴为误差</strong></li>
</ol>
<p><img src="2.16.4.3.png" alt></p>
<p>​                                             红线为测试集上的Error,蓝线为训练集上的Error</p>
<p>​	模型欠拟合：模型在点C处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</p>
<p>​	模型过拟合：模型在点A处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。 它通常发生在模型过于复杂的情况下，如参数过多等，会使得模型的预测性能变弱，并且增加数据的波动性。虽然模型在训练时的效果可以表现的很完美，基本上记住了数据的全部特点，但这种模型在未知数据的表现能力会大减折扣，因为简单的模型泛化能力通常都是很弱的。</p>
<p>​	模型正常：模型复杂程度控制在点B处为最优。</p>
<h3 id="2-16-5-如何解决过拟合与欠拟合">2.16.5 如何解决过拟合与欠拟合</h3>
<p><strong>如何解决欠拟合：</strong></p>
<ol>
<li>添加其他特征项。组合、泛化、相关性、上下文特征、平台特征等特征是特征添加的重要手段，有时候特征项不够会导致模型欠拟合。</li>
<li>添加多项式特征。例如将线性模型添加二次项或三次项使模型泛化能力更强。例如，FM（Factorization Machine）模型、FFM（Field-aware Factorization Machine）模型，其实就是线性模型，增加了二阶多项式，保证了模型一定的拟合程度。</li>
<li>可以增加模型的复杂程度。</li>
<li>减小正则化系数。正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。</li>
</ol>
<p><strong>如何解决过拟合：</strong></p>
<ol>
<li>重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清洗数据。</li>
<li>增加训练样本数量。</li>
<li>降低模型复杂程度。</li>
<li>增大正则项系数。</li>
<li>采用dropout方法，dropout方法，通俗的讲就是在训练的时候让神经元以一定的概率不工作。</li>
<li>early stopping。</li>
<li>减少迭代次数。</li>
<li>增大学习率。</li>
<li>添加噪声数据。</li>
<li>树结构中，可以对树进行剪枝。</li>
<li>减少特征项。</li>
</ol>
<p>欠拟合和过拟合这些方法，需要根据实际问题，实际模型，进行选择。</p>
<h3 id="2-16-6-交叉验证的主要作用">2.16.6 交叉验证的主要作用</h3>
<p>​	为了得到更为稳健可靠的模型，对模型的泛化误差进行评估，得到模型泛化误差的近似值。当有多个模型可以选择时，我们通常选择“泛化误差”最小的模型。</p>
<p>​	交叉验证的方法有许多种，但是最常用的是：留一交叉验证、k折交叉验证。</p>
<h3 id="2-16-7-理解k折交叉验证">2.16.7 理解k折交叉验证</h3>
<ol>
<li>将含有N个样本的数据集，分成K份，每份含有N/K个样本。选择其中1份作为测试集，另外K-1份作为训练集，测试集就有K种情况。</li>
<li>在每种情况中，用训练集训练模型，用测试集测试模型，计算模型的泛化误差。</li>
<li>交叉验证重复K次，每份验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测，得到模型最终的泛化误差。</li>
<li>将K种情况下，模型的泛化误差取均值，得到模型最终的泛化误差。</li>
<li>一般 $2\leqslant K \leqslant10$ 。 k折交叉验证的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。</li>
<li>训练集中样本数量要足够多，一般至少大于总样本数的50%。</li>
<li>训练集和测试集必须从完整的数据集中均匀取样。均匀取样的目的是希望减少训练集、测试集与原数据集之间的偏差。当样本数量足够多时，通过随机取样，便可以实现均匀取样的效果。</li>
</ol>
<h3 id="2-16-8-混淆矩阵">2.16.8 混淆矩阵</h3>
<p>第一种混淆矩阵:</p>
<table>
<thead>
<tr>
<th style="text-align:center">真实情况T or F</th>
<th style="text-align:left">预测为正例1，P</th>
<th style="text-align:left">预测为负例0，N</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">本来label标记为1，预测结果真为T、假为F</td>
<td style="text-align:left">TP(预测为1，实际为1)</td>
<td style="text-align:left">FN(预测为0，实际为1)</td>
</tr>
<tr>
<td style="text-align:center">本来label标记为0，预测结果真为T、假为F</td>
<td style="text-align:left">FP(预测为1，实际为0)</td>
<td style="text-align:left">TN(预测为0，实际也为0)</td>
</tr>
</tbody>
</table>
<p>第二种混淆矩阵:</p>
<table>
<thead>
<tr>
<th style="text-align:center">预测情况P or N</th>
<th style="text-align:left">实际label为1,预测对了为T</th>
<th style="text-align:left">实际label为0,预测对了为T</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">预测为正例1，P</td>
<td style="text-align:left">TP(预测为1，实际为1)</td>
<td style="text-align:left">FP(预测为1，实际为0)</td>
</tr>
<tr>
<td style="text-align:center">预测为负例0，N</td>
<td style="text-align:left">FN(预测为0，实际为1)</td>
<td style="text-align:left">TN(预测为0，实际也为0)</td>
</tr>
</tbody>
</table>
<h3 id="2-16-9-错误率及精度">2.16.9 错误率及精度</h3>
<ol>
<li>错误率（Error Rate）：分类错误的样本数占样本总数的比例。</li>
<li>精度（accuracy）：分类正确的样本数占样本总数的比例。</li>
</ol>
<h3 id="2-16-10-查准率与查全率">2.16.10 查准率与查全率</h3>
<p>将算法预测的结果分成四种情况：</p>
<ol>
<li>正确肯定（True Positive,TP）：预测为真，实际为真</li>
<li>正确否定（True Negative,TN）：预测为假，实际为假</li>
<li>错误肯定（False Positive,FP）：预测为真，实际为假</li>
<li>错误否定（False Negative,FN）：预测为假，实际为真</li>
</ol>
<p>则：</p>
<p>查准率（Precision）=TP/（TP+FP）</p>
<p><strong>理解</strong>：预测出为阳性的样本中，正确的有多少。区别准确率（正确预测出的样本，包括正确预测为阳性、阴性，占总样本比例）。<br>
例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。</p>
<p>查全率（Recall）=TP/（TP+FN）</p>
<p><strong>理解</strong>：正确预测为阳性的数量占总样本中阳性数量的比例。<br>
例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p>
<h3 id="2-16-11-ROC与AUC">2.16.11 ROC与AUC</h3>
<p>​	ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。</p>
<p>​	ROC曲线的面积就是AUC（Area Under Curve）。</p>
<p>​	AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。</p>
<p>​	ROC曲线，通过将连续变量设定出多个不同的临界值，从而计算出一系列真正率和假正率，再以假正率为横坐标、真正率为纵坐标绘制成曲线，曲线下面积越大，推断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为假正率和真正率均较高的临界值。</p>
<p>​	对于分类器，或者说分类算法，评价指标主要有Precision，Recall，F-score。下图是一个ROC曲线的示例。</p>
<p><img src="2.40.10/1.png" alt></p>
<p>ROC曲线的横坐标为False Positive Rate（FPR），纵坐标为True Positive Rate（TPR）。其中</p>
 $$
TPR = \frac{TP}{TP+FN} ,FPR = \frac{FP}{FP+TN}
$$ 
<p>​	下面着重介绍ROC曲线图中的四个点和一条线。<br>
​	第一个点(0,1)，即FPR=0, TPR=1，这意味着FN（False Negative）=0，并且FP（False Positive）=0。意味着这是一个完美的分类器，它将所有的样本都正确分类。<br>
​	第二个点(1,0)，即FPR=1，TPR=0，意味着这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。<br>
​	第三个点(0,0)，即FPR=TPR=0，即FP（False Positive）=TP（True Positive）=0，可以发现该分类器预测所有的样本都为负样本（Negative）。<br>
​	第四个点(1,1)，即FPR=TPR=1，分类器实际上预测所有的样本都为正样本。<br>
​	经过以上分析，ROC曲线越接近左上角，该分类器的性能越好。</p>
<p>​	ROC曲线所覆盖的面积称为AUC（Area Under Curve），可以更直观的判断学习器的性能，AUC越大则性能越好。</p>
<h3 id="2-16-12-如何画ROC曲线">2.16.12 如何画ROC曲线</h3>
<p>​	下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。</p>
<p>步骤：<br>
1、假设已经得出一系列样本被划分为正类的概率，按照大小排序。<br>
2、从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。<br>
3、每次选取一个不同的threshold，得到一组FPR和TPR，即ROC曲线上的一点。以此共得到20组FPR和TPR的值。<br>
4、根据3、中的每个坐标点，画图。</p>
<p><img src="2.40.11/1.jpg" alt></p>
<h3 id="2-16-13-如何计算TPR，FPR">2.16.13 如何计算TPR，FPR</h3>
<p>1、分析数据<br>
y_true = [0, 0, 1, 1]；scores = [0.1, 0.4, 0.35, 0.8]；<br>
2、列表</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>预测属于P的概率(score)</th>
<th>真实类别</th>
</tr>
</thead>
<tbody>
<tr>
<td>y[0]</td>
<td>0.1</td>
<td>N</td>
</tr>
<tr>
<td>y[1]</td>
<td>0.4</td>
<td>N</td>
</tr>
<tr>
<td>y[2]</td>
<td>0.35</td>
<td>P</td>
</tr>
<tr>
<td>y[3]</td>
<td>0.8</td>
<td>P</td>
</tr>
</tbody>
</table>
<p>3、将截断点依次取为score值，计算TPR和FPR。<br>
当截断点为0.1时：<br>
说明只要score&gt;=0.1，它的预测类别就是正例。 因为4个样本的score都大于等于0.1，所以，所有样本的预测类别都为P。<br>
scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [1, 1, 1, 1]；<br>
正例与反例信息如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=2</td>
<td>FN=0</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=2</td>
<td>TN=0</td>
</tr>
</tbody>
</table>
<p>由此可得：<br>
TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 1；</p>
<p>当截断点为0.35时：<br>
scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 1, 1];<br>
正例与反例信息如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=2</td>
<td>FN=0</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=1</td>
<td>TN=1</td>
</tr>
</tbody>
</table>
<p>由此可得：<br>
TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 0.5；</p>
<p>当截断点为0.4时：<br>
scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 0, 1]；<br>
正例与反例信息如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=1</td>
<td>FN=1</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=1</td>
<td>TN=1</td>
</tr>
</tbody>
</table>
<p>由此可得：<br>
TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0.5；</p>
<p>当截断点为0.8时：<br>
scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 0, 0, 1]；</p>
<p>正例与反例信息如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=1</td>
<td>FN=1</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=0</td>
<td>TN=2</td>
</tr>
</tbody>
</table>
<p>由此可得：<br>
TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0；</p>
<p>4、根据TPR、FPR值，以FPR为横轴，TPR为纵轴画图。</p>
<h3 id="2-16-14-如何计算AUC">2.16.14 如何计算AUC</h3>
<ul>
<li>将坐标点按照横坐标FPR排序 。</li>
<li>计算第 $i$ 个坐标点和第 $i+1$ 个坐标点的间距 $dx$  。</li>
<li>获取第 $i$ 或者 $i+1$ 个坐标点的纵坐标y。</li>
<li>计算面积微元 $ds=ydx$ 。</li>
<li>对面积微元进行累加，得到AUC。</li>
</ul>
<h3 id="2-16-15-为什么使用Roc和Auc评价分类器">2.16.15 为什么使用Roc和Auc评价分类器</h3>
<p>​	模型有很多评估方法，为什么还要使用ROC和AUC呢？<br>
​	因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。</p>
<h3 id="2-16-16-直观理解AUC">2.16.16 直观理解AUC</h3>
<p>​	下图展现了三种AUC的值：</p>
<p><img src="2.40.15/1.png" alt></p>
<p>​	AUC是衡量二分类模型优劣的一种评价指标，表示正例排在负例前面的概率。其他评价指标有精确度、准确率、召回率，而AUC比这三者更为常用。<br>
​	一般在分类模型中，预测结果都是以概率的形式表现，如果要计算准确率，通常都会手动设置一个阈值来将对应的概率转化成类别，这个阈值也就很大程度上影响了模型准确率的计算。<br>
​	举例：<br>
​	现在假设有一个训练好的二分类器对10个正负样本（正例5个，负例5个）预测，得分按高到低排序得到的最好预测结果为[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]，即5个正例均排在5个负例前面，正例排在负例前面的概率为100%。然后绘制其ROC曲线，由于是10个样本，除去原点我们需要描10个点，如下：</p>
<p><img src="2.16.17-1.png" alt></p>
<p>​	描点方式按照样本预测结果的得分高低从左至右开始遍历。从原点开始，每遇到1便向y轴正方向移动y轴最小步长1个单位，这里是1/5=0.2；每遇到0则向x轴正方向移动x轴最小步长1个单位，这里也是0.2。不难看出，上图的AUC等于1，印证了正例排在负例前面的概率的确为100%。</p>
<p>​	假设预测结果序列为[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]。</p>
<p><img src="2.16.17-2.png" alt></p>
<p>​	计算上图的AUC为0.96与计算正例与排在负例前面的概率0.8 × 1 + 0.2 × 0.8 = 0.96相等，而左上角阴影部分的面积则是负例排在正例前面的概率0.2 × 0.2 = 0.04。</p>
<p>​	假设预测结果序列为[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]。</p>
<p><img src="2.16.17-3.png" alt></p>
<p>​	计算上图的AUC为0.88与计算正例与排在负例前面的概率0.6 × 1 + 0.2 × 0.8 + 0.2 × 0.6 = 0.88相等，左上角阴影部分的面积是负例排在正例前面的概率0.2 × 0.2 × 3 = 0.12。</p>
<h3 id="2-16-17-代价敏感错误率与代价曲线">2.16.17 代价敏感错误率与代价曲线</h3>
<p>不同的错误会产生不同代价。以二分法为例，设置代价矩阵如下：</p>
<p><img src="2-1.png" alt></p>
<p>当判断正确的时候，值为0，不正确的时候，分别为 $Cost_{01}​$ 和 $Cost_{10}​$  。</p>
 $Cost_{10}$ :表示实际为反例但预测成正例的代价。
 $Cost_{01}$ :表示实际为正例但是预测为反例的代价。
<p><strong>代价敏感错误率</strong>=样本中由模型得到的错误值与代价乘积之和 / 总样本。<br>
其数学表达式为：</p>
 $$
E(f;D;cost)=\frac{1}{m}\left( \sum_{x_{i} \in D^{+}}({f(x_i)\neq y_i})\times Cost_{01}+ \sum_{x_{i} \in D^{-}}({f(x_i)\neq y_i})\times Cost_{10}\right)
$$ 
 $D^{+}、D^{-}​$ 分别代表样例集的正例子集和反例子集，x是预测值，y是真实值。
<p><strong>代价曲线</strong>：<br>
在均等代价时，ROC曲线不能直接反应出模型的期望总体代价，而代价曲线可以。<br>
代价曲线横轴为[0,1]的正例函数代价：</p>
 $$
P(+)Cost=\frac{p*Cost_{01}}{p*Cost_{01}+(1-p)*Cost_{10}}
$$ 
<p>其中p是样本为正例的概率。</p>
<p>代价曲线纵轴维[0,1]的归一化代价：</p>
 $$
Cost_{norm}=\frac{FNR*p*Cost_{01}+FNR*(1-p)*Cost_{10}}{p*Cost_{01}+(1-p)*Cost_{10}}
$$ 
<p>其中FPR为假阳率，FNR=1-TPR为假阴率。</p>
<p>注：ROC每个点，对应代价平面上一条线。</p>
<p>例如，ROC上(TPR,FPR),计算出FNR=1-TPR，在代价平面上绘制一条从(0,FPR)到(1,FNR)的线段，面积则为该条件下期望的总体代价。所有线段下界面积，所有条件下学习器的期望总体代价。</p>
<p><img src="2.16.18.1.png" alt></p>
<h3 id="2-16-18-模型有哪些比较检验方法">2.16.18 模型有哪些比较检验方法</h3>
<p>正确性分析：模型稳定性分析，稳健性分析，收敛性分析，变化趋势分析，极值分析等。<br>
有效性分析：误差分析，参数敏感性分析，模型对比检验等。<br>
有用性分析：关键数据求解，极值点，拐点，变化趋势分析，用数据验证动态模拟等。<br>
高效性分析：时空复杂度分析与现有进行比较等。</p>
<h3 id="2-16-19-为什么使用标准差">2.16.19 为什么使用标准差</h3>
<p>方差公式为： $S^2_{N}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}​$</p>
<p>标准差公式为： $S_{N}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}}​$</p>
<p>样本标准差公式为： $S_{N}=\sqrt{\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}}​$</p>
<p>与方差相比，使用标准差来表示数据点的离散程度有3个好处：<br>
1、表示离散程度的数字与样本数据点的数量级一致，更适合对数据样本形成感性认知。</p>
<p>2、表示离散程度的数字单位与样本数据的单位一致，更方便做后续的分析运算。</p>
<p>3、在样本数据大致符合正态分布的情况下，标准差具有方便估算的特性：68%的数据点落在平均值前后1个标准差的范围内、95%的数据点落在平均值前后2个标准差的范围内，而99%的数据点将会落在平均值前后3个标准差的范围内。</p>
<h3 id="2-16-20-类别不平衡产生原因">2.16.20 类别不平衡产生原因</h3>
<p>​	类别不平衡（class-imbalance）是指分类任务中不同类别的训练样例数目差别很大的情况。</p>
<p>产生原因：</p>
<p>​	分类学习算法通常都会假设不同类别的训练样例数目基本相同。如果不同类别的训练样例数目差别很大，则会影响学习结果，测试结果变差。例如二分类问题中有998个反例，正例有2个，那学习方法只需返回一个永远将新样本预测为反例的分类器，就能达到99.8%的精度；然而这样的分类器没有价值。</p>
<h3 id="2-16-21-常见的类别不平衡问题解决方法">2.16.21 常见的类别不平衡问题解决方法</h3>
<p>防止类别不平衡对学习造成的影响，在构建分类模型之前，需要对分类不平衡性问题进行处理。主要解决方法有：</p>
<p>1、扩大数据集</p>
<p>​	增加包含小类样本数据的数据，更多的数据能得到更多的分布信息。</p>
<p>2、对大类数据欠采样</p>
<p>​	减少大类数据样本个数，使与小样本个数接近。<br>
​	缺点：欠采样操作时若随机丢弃大类样本，可能会丢失重要信息。<br>
​	代表算法：EasyEnsemble。其思想是利用集成学习机制，将大类划分为若干个集合供不同的学习器使用。相当于对每个学习器都进行欠采样，但对于全局则不会丢失重要信息。</p>
<p>3、对小类数据过采样</p>
<p>​	过采样：对小类的数据样本进行采样来增加小类的数据样本个数。</p>
<p>​	代表算法：SMOTE和ADASYN。</p>
<p>​	SMOTE：通过对训练集中的小类数据进行插值来产生额外的小类样本数据。</p>
<p>​	新的少数类样本产生的策略：对每个少数类样本a，在a的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。 	<br>
​	ADASYN：根据学习难度的不同，对不同的少数类别的样本使用加权分布，对于难以学习的少数类的样本，产生更多的综合数据。 通过减少类不平衡引入的偏差和将分类决策边界自适应地转移到困难的样本两种手段，改善了数据分布。</p>
<p>4、使用新评价指标</p>
<p>​	如果当前评价指标不适用，则应寻找其他具有说服力的评价指标。比如准确度这个评价指标在类别不均衡的分类任务中并不适用，甚至进行误导。因此在类别不均衡分类任务中，需要使用更有说服力的评价指标来对分类器进行评价。</p>
<p>5、选择新算法</p>
<p>​	不同的算法适用于不同的任务与数据，应该使用不同的算法进行比较。</p>
<p>6、数据代价加权</p>
<p>​	例如当分类任务是识别小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值，从而使得分类器将重点集中在小类样本身上。</p>
<p>7、转化问题思考角度</p>
<p>​	例如在分类问题时，把小类的样本作为异常点，将问题转化为异常点检测或变化趋势检测问题。 异常点检测即是对那些罕见事件进行识别。变化趋势检测区别于异常点检测在于其通过检测不寻常的变化趋势来识别。</p>
<p>8、将问题细化分析</p>
<p>​	对问题进行分析与挖掘，将问题划分成多个更小的问题，看这些小问题是否更容易解决。</p>
<h2 id="2-17-决策树">2.17 决策树</h2>
<h3 id="2-17-1-决策树的基本原理">2.17.1 决策树的基本原理</h3>
<p>​	决策树（Decision Tree）是一种分而治之的决策过程。一个困难的预测问题，通过树的分支节点，被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。将依规则分割数据集的过程不断递归下去（Recursive Partitioning）。随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。当分支节点的深度或者问题的简单程度满足一定的停止规则（Stopping Rule）时, 该分支节点会停止分裂，此为自上而下的停止阈值（Cutoff Threshold）法；有些决策树也使用自下而上的剪枝（Pruning）法。</p>
<h3 id="2-17-2-决策树的三要素？">2.17.2 决策树的三要素？</h3>
<p>​	一棵决策树的生成过程主要分为下3个部分：</p>
<p>​	1、特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。</p>
<p>​	2、决策树生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。树结构来说，递归结构是最容易理解的方式。</p>
<p>​	3、剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。</p>
<h3 id="2-17-3-决策树学习基本算法">2.17.3 决策树学习基本算法</h3>
<p><img src="2-5.png" alt></p>
<h3 id="2-17-4-决策树算法优缺点">2.17.4 决策树算法优缺点</h3>
<p><strong>决策树算法的优点</strong>：</p>
<p>1、决策树算法易理解，机理解释起来简单。</p>
<p>2、决策树算法可以用于小数据集。</p>
<p>3、决策树算法的时间复杂度较小，为用于训练决策树的数据点的对数。</p>
<p>4、相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别。</p>
<p>5、能够处理多输出的问题。</p>
<p>6、对缺失值不敏感。</p>
<p>7、可以处理不相关特征数据。</p>
<p>8、效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。</p>
<p><strong>决策树算法的缺点</strong>：</p>
<p>1、对连续性的字段比较难预测。</p>
<p>2、容易出现过拟合。</p>
<p>3、当类别太多时，错误可能就会增加的比较快。</p>
<p>4、在处理特征关联性比较强的数据时表现得不是太好。</p>
<p>5、对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。</p>
<h3 id="2-17-5-熵的概念以及理解">2.17.5 熵的概念以及理解</h3>
<p>​	熵：度量随机变量的不确定性。<br>
​	定义：假设随机变量X的可能取值有 $x_{1},x_{2},...,x_{n}$ ，对于每一个可能的取值 $x_{i}$ ，其概率为 $P(X=x_{i})=p_{i},i=1,2...,n$ 。随机变量的熵为：</p>
 $$
H(X)=-\sum_{i=1}^{n}p_{i}log_{2}p_{i}
$$ 
<p>​       对于样本集合，假设样本有k个类别，每个类别的概率为 $\frac{|C_{k}|}{|D|}$ ，其中  ${|C_{k}|}{|D|}$ 为类别为k的样本个数， $|D|​$ 为样本总数。样本集合D的熵为：</p>
 $$
H(D)=-\sum_{k=1}^{k}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}
$$ 
<h3 id="2-17-6-信息增益的理解">2.17.6 信息增益的理解</h3>
<p>​	定义：以某特征划分数据集前后的熵的差值。<br>
​	熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。  ​	假设划分前样本集合D的熵为H(D)。使用某个特征A划分数据集D，计算划分后的数据子集的熵为H(D|A)。<br>
​	则信息增益为：</p>
 $$
g(D,A)=H(D)-H(D|A)
$$ 
<p>​	*注：*在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。<br>
​	思想：计算所有特征划分数据集D，得到多个特征划分数据集D的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。<br>
​	另外这里提一下信息增益比相关知识：<br>
​	 $信息增益比=惩罚参数\times信息增益$<br>
​	信息增益比本质：在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。<br>
​	惩罚参数：数据集D以特征A作为随机变量的熵的倒数。</p>
<h3 id="2-17-7-剪枝处理的作用及策略">2.17.7 剪枝处理的作用及策略</h3>
<p>​	剪枝处理是决策树学习算法用来解决过拟合问题的一种办法。</p>
<p>​	在决策树算法中，为了尽可能正确分类训练样本， 节点划分过程不断重复， 有时候会造成决策树分支过多，以至于将训练样本集自身特点当作泛化特点， 而导致过拟合。 因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。</p>
<p>​	剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。</p>
<p>​	预剪枝：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能， 如果不能提升，则停止划分，将当前节点标记为叶结点。</p>
<p>​	后剪枝：生成决策树以后，再自下而上对非叶结点进行考察， 若将此节点标记为叶结点可以带来泛化性能提升，则修改之。</p>
<h2 id="2-18-支持向量机">2.18 支持向量机</h2>
<h3 id="2-18-1-什么是支持向量机">2.18.1 什么是支持向量机</h3>
<p>​	支持向量：在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。</p>
<p>​	支持向量机（Support Vector Machine，SVM）：其含义是通过支持向量运算的分类器。</p>
<p>​	在一个二维环境中，其中点R，S，G点和其它靠近中间黑线的点可以看作为支持向量，它们可以决定分类器，即黑线的具体参数。</p>
<p><img src="2-6.png" alt></p>
<p>​	支持向量机是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是边界最大化，最终转化为一个凸二次规划问题来求解。由简至繁的模型包括：</p>
<p>​	当训练样本线性可分时，通过硬边界（hard margin）最大化，学习一个线性可分支持向量机；</p>
<p>​	当训练样本近似线性可分时，通过软边界（soft margin）最大化，学习一个线性支持向量机；</p>
<p>​	当训练样本线性不可分时，通过核技巧和软边界最大化，学习一个非线性支持向量机；</p>
<h3 id="2-18-2-支持向量机能解决哪些问题">2.18.2 支持向量机能解决哪些问题</h3>
<p><strong>线性分类</strong></p>
<p>​	在训练数据中，每个数据都有n个的属性和一个二分类类别标志，我们可以认为这些数据在一个n维空间里。我们的目标是找到一个n-1维的超平面，这个超平面可以将数据分成两部分，每部分数据都属于同一个类别。</p>
<p>​	这样的超平面有很多，假如我们要找到一个最佳的超平面。此时，增加一个约束条件：要求这个超平面到每边最近数据点的距离是最大的，成为最大边距超平面。这个分类器即为最大边距分类器。</p>
<p><strong>非线性分类</strong></p>
<p>​	SVM的一个优势是支持非线性分类。它结合使用拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件，以及核函数可以生成非线性分类器。</p>
<h3 id="2-18-3-核函数特点及其作用">2.18.3 核函数特点及其作用</h3>
<p>​	引入核函数目的：把原坐标系里线性不可分的数据用核函数Kernel投影到另一个空间，尽量使得数据在新的空间里线性可分。<br>
​	核函数方法的广泛应用，与其特点是分不开的：</p>
<p>1）核函数的引入避免了“维数灾难”，大大减小了计算量。而输入空间的维数n对核函数矩阵无影响。因此，核函数方法可以有效处理高维输入。</p>
<p>2）无需知道非线性变换函数Φ的形式和参数。</p>
<p>3）核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。</p>
<p>4）核函数方法可以和不同的算法相结合，形成多种不同的基于核函数技术的方法，且这两部分的设计可以单独进行，并可以为不同的应用选择不同的核函数和算法。</p>
<h3 id="2-18-4-SVM为什么引入对偶问题">2.18.4 SVM为什么引入对偶问题</h3>
<p>1，对偶问题将原始问题中的约束转为了对偶问题中的等式约束，对偶问题往往更加容易求解。</p>
<p>2，可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的）。</p>
<p>3，在优化理论中，目标函数 f(x) 会有多种形式：如果目标函数和约束条件都为变量 x 的线性函数，称该问题为线性规划；如果目标函数为二次函数，约束条件为线性函数，称该最优化问题为二次规划；如果目标函数或者约束条件均为非线性函数，称该最优化问题为非线性规划。每个线性规划问题都有一个与之对应的对偶问题，对偶问题有非常良好的性质，以下列举几个：</p>
<p>​	a, 对偶问题的对偶是原问题；</p>
<p>​	b, 无论原始问题是否是凸的，对偶问题都是凸优化问题；</p>
<p>​	c, 对偶问题可以给出原始问题一个下界；</p>
<p>​	d, 当满足一定条件时，原始问题与对偶问题的解是完全等价的。</p>
<h3 id="2-18-5-如何理解SVM中的对偶问题">2.18.5 如何理解SVM中的对偶问题</h3>
<p>在硬边界支持向量机中，问题的求解可以转化为凸二次规划问题。</p>
<p>​	假设优化目标为</p>
 $$
\begin{align}
&\min_{\boldsymbol w, b}\frac{1}{2}||\boldsymbol w||^2\\
&s.t. y_i(\boldsymbol w^T\boldsymbol x_i+b)\geqslant 1, i=1,2,\cdots,m.\\
\end{align}  \tag{1}
$$ 
<p><strong>step 1</strong>. 转化问题：</p>
 $$
\min_{\boldsymbol w, b} \max_{\alpha_i \geqslant 0}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}  \tag{2}
$$ 
<p>上式等价于原问题，因为若满足(1)中不等式约束，则(2)式求max时, $\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))$ 必须取0，与(1)等价；若不满足(1)中不等式约束，(2)中求max会得到无穷大。 交换min和max获得其对偶问题:</p>
 $$
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}
$$ 
<p>交换之后的对偶问题和原问题并不相等，上式的解小于等于原问题的解。</p>
<p><strong>step 2</strong>.现在的问题是如何找到问题(1) 的最优值的一个最好的下界?</p>
 $$
\frac{1}{2}||\boldsymbol w||^2 < v\\
1 - y_i(\boldsymbol w^T\boldsymbol x_i+b) \leqslant 0\tag{3}
$$ 
<p>若方程组(3)无解， 则v是问题(1)的一个下界。若(3)有解， 则</p>
 $$
\forall \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} < v
$$ 
<p>由逆否命题得：若</p>
 $$
\exists \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} \geqslant v
$$ 
<p>则(3)无解。</p>
<p>那么v是问题</p>
<p>(1)的一个下界。<br>
要求得一个好的下界，取最大值即可</p>
 $$
\max_{\alpha_i \geqslant 0}  \min_{\boldsymbol w, b} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}
$$ 
<p><strong>step 3</strong>. 令</p>
 $$
L(\boldsymbol w, b,\boldsymbol a) =   \frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))
$$ 
 $p^*$ 为原问题的最小值，对应的 $w,b$ 分别为 $w^*,b^*$ ,则对于任意的 $a>0$ :
 $$
p^* = \frac{1}{2}||\boldsymbol w^*||^2 \geqslant  L(\boldsymbol w^*, b,\boldsymbol a) \geqslant \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)
$$ 
<p>则  $\min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)$ 是问题（1）的一个下界。</p>
<p>此时，取最大值即可求得好的下界，即</p>
 $$
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)
$$ 
<h3 id="2-18-7-常见的核函数有哪些">2.18.7 常见的核函数有哪些</h3>
<table>
<thead>
<tr>
<th>核函数</th>
<th>表达式</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Kernel线性核</td>
<td>$k(x,y)=x^{t}y+c$</td>
<td></td>
</tr>
<tr>
<td>Polynomial Kernel多项式核</td>
<td>$k(x,y)=(ax^{t}y+c)^{d}$</td>
<td>$d\geqslant1$ 为多项式的次数</td>
</tr>
<tr>
<td>Exponential Kernel指数核</td>
<td>$k(x,y)=exp(-\frac{\left \|x-y \right \|}{2\sigma ^{2}})$</td>
<td>$\sigma>0$</td>
</tr>
<tr>
<td>Gaussian Kernel高斯核</td>
<td>$k(x,y)=exp(-\frac{\left \|x-y \right \|^{2}}{2\sigma ^{2}})$</td>
<td>$\sigma$ 为高斯核的带宽， $\sigma>0$ ,</td>
</tr>
<tr>
<td>Laplacian Kernel拉普拉斯核</td>
<td>$k(x,y)=exp(-\frac{\left \|x-y \right \|}{\sigma})$</td>
<td>$\sigma>0$</td>
</tr>
<tr>
<td>ANOVA Kernel</td>
<td>$k(x,y)=exp(-\sigma(x^{k}-y^{k})^{2})^{d}$</td>
<td></td>
</tr>
<tr>
<td>Sigmoid Kernel</td>
<td>$k(x,y)=tanh(ax^{t}y+c)$</td>
<td>$tanh$ 为双曲正切函数， $a>0,c<0$< td>
</0$<></td></tr>
</tbody>
</table>
<h3 id="2-18-9-SVM主要特点">2.18.9 SVM主要特点</h3>
<p>特点：</p>
<p>(1)  SVM方法的理论基础是非线性映射，SVM利用内积核函数代替向高维空间的非线性映射。<br>
(2)  SVM的目标是对特征空间划分得到最优超平面，SVM方法核心是最大化分类边界。<br>
(3)  支持向量是SVM的训练结果，在SVM分类决策中起决定作用的是支持向量。<br>
(4)  SVM是一种有坚实理论基础的新颖的适用小样本学习方法。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。<br>
(5)  SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。<br>
(6)  少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单，而且具有较好的“鲁棒性”。这种鲁棒性主要体现在：<br>
​        ①增、删非支持向量样本对模型没有影响;<br>
​        ②支持向量样本集具有一定的鲁棒性;<br>
​        ③有些成功的应用中，SVM方法对核的选取不敏感<br>
(7)  SVM学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。<br>
(8)  SVM通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。<br>
(9)  SVM在小样本训练集上能够得到比其它算法好很多的结果。SVM优化目标是结构化风险最小，而不是经验风险最小，避免了过拟合问题，通过margin的概念，得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求，有优秀的泛化能力。<br>
(10)  它是一个凸优化问题，因此局部最优解一定是全局最优解的优点。</p>
<h3 id="2-18-10-SVM主要缺点">2.18.10 SVM主要缺点</h3>
<p>(1) SVM算法对大规模训练样本难以实施<br>
​        SVM的空间消耗主要是存储训练样本和核矩阵，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。<br>
​        如果数据量很大，SVM的训练时间就会比较长，如垃圾邮件的分类检测，没有使用SVM分类器，而是使用简单的朴素贝叶斯分类器，或者是使用逻辑回归模型分类。</p>
<p>(2) 用SVM解决多分类问题存在困难</p>
<p>​        经典的支持向量机算法只给出了二类分类的算法，而在实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和SVM决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服SVM固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗糙集理论结合，形成一种优势互补的多类问题的组合分类器。</p>
<p>(3) 对缺失数据敏感，对参数和核函数的选择敏感</p>
<p>​        支持向量机性能的优劣主要取决于核函数的选取，所以对于一个实际问题而言，如何根据实际的数据模型选择合适的核函数从而构造SVM算法。目前比较成熟的核函数及其参数的选择都是人为的，根据经验来选取的，带有一定的随意性。在不同的问题领域，核函数应当具有不同的形式和参数，所以在选取时候应该将领域知识引入进来，但是目前还没有好的方法来解决核函数的选取问题。</p>
<h3 id="2-18-11-逻辑回归与SVM的异同">2.18.11 逻辑回归与SVM的异同</h3>
<p>相同点：</p>
<ul>
<li>LR和SVM都是<strong>分类</strong>算法。</li>
<li>LR和SVM都是<strong>监督学习</strong>算法。</li>
<li>LR和SVM都是<strong>判别模型</strong>。</li>
<li>如果不考虑核函数，LR和SVM都是<strong>线性分类</strong>算法，也就是说他们的分类决策面都是线性的。<br>
说明：LR也是可以用核函数的.但LR通常不采用核函数的方法。（<strong>计算量太大</strong>）</li>
</ul>
<p>不同点：</p>
<p><strong>1、LR采用log损失，SVM采用合页(hinge)损失。</strong><br>
逻辑回归的损失函数：</p>
 $$
J(\theta)=-\frac{1}{m}\sum^m_{i=1}\left[y^{i}logh_{\theta}(x^{i})+ (1-y^{i})log(1-h_{\theta}(x^{i}))\right]
$$ 
<p>支持向量机的目标函数:</p>
 $$
L(w,n,a)=\frac{1}{2}||w||^2-\sum^n_{i=1}\alpha_i \left( y_i(w^Tx_i+b)-1\right)
$$ 
<p>​	逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过<strong>极大似然估计</strong>的方法估计出参数的值。<br>
​	支持向量机基于几何<strong>边界最大化</strong>原理，认为存在最大几何边界的分类面为最优分类面。</p>
<p>2、<strong>LR对异常值敏感，SVM对异常值不敏感</strong>。</p>
<p>​	支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。<br>
​	支持向量机改变非支持向量样本并不会引起决策面的变化。<br>
​	逻辑回归中改变任何样本都会引起决策面的变化。</p>
<p>3、<strong>计算复杂度不同。对于海量数据，SVM的效率较低，LR效率比较高</strong></p>
<p>​	当样本较少，特征维数较低时，SVM和LR的运行时间均比较短，SVM较短一些。准确率的话，LR明显比SVM要高。当样本稍微增加些时，SVM运行时间开始增长，但是准确率赶超了LR。SVM时间虽长，但在可接受范围内。当数据量增长到20000时，特征维数增长到200时，SVM的运行时间剧烈增加，远远超过了LR的运行时间。但是准确率却和LR相差无几。(这其中主要原因是大量非支持向量参与计算，造成SVM的二次规划问题)</p>
<p>4、<strong>对非线性问题的处理方式不同</strong></p>
<p>​	LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过核函数kernel（因为只有支持向量参与核计算，计算复杂度不高）。由于可以利用核函数，SVM则可以通过对偶求解高效处理。LR则在特征空间维度很高时，表现较差。</p>
<p>5、<strong>SVM的损失函数就自带正则</strong>。<br>
​	损失函数中的1/2||w||^2项，这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！**</p>
<p>6、SVM自带<strong>结构风险最小化</strong>，LR则是<strong>经验风险最小化</strong>。</p>
<p>7、SVM会用核函数而LR一般不用核函数。</p>
<h2 id="2-19-贝叶斯分类器">2.19 贝叶斯分类器</h2>
<h3 id="2-19-1-图解极大似然估计">2.19.1 图解极大似然估计</h3>
<p>极大似然估计的原理，用一张图片来说明，如下图所示：</p>
<p><img src="2.19.1.1.png" alt></p>
<p>​	例：有两个外形完全相同的箱子，1号箱有99只白球，1只黑球；2号箱有1只白球，99只黑球。在一次实验中，取出的是黑球，请问是从哪个箱子中取出的？</p>
<p>​	一般的根据经验想法，会猜测这只黑球最像是从2号箱取出，此时描述的“最像”就有“最大似然”的意思，这种想法常称为“最大似然原理”。</p>
<h3 id="2-19-2-极大似然估计原理">2.19.2 极大似然估计原理</h3>
<p>​	总结起来，最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</p>
<p>​	极大似然估计是建立在极大似然原理的基础上的一个统计方法。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p>
<p>​	由于样本集中的样本都是独立同分布，可以只考虑一类样本集 $D$ ，来估计参数向量 $\vec\theta$ 。记已知的样本集为：</p>
 $$
D=\vec x_{1},\vec x_{2},...,\vec x_{n}
$$ 
<p>似然函数（likelihood function）：联合概率密度函数 $p(D|\vec\theta )$ 称为相对于 $\vec x_{1},\vec x_{2},...,\vec x_{n}$ 的 $\vec\theta$ 的似然函数。</p>
 $$
l(\vec\theta )=p(D|\vec\theta ) =p(\vec x_{1},\vec x_{2},...,\vec x_{n}|\vec\theta )=\prod_{i=1}^{n}p(\vec x_{i}|\vec \theta )
$$ 
<p>如果 $\hat{\vec\theta}$ 是参数空间中能使似然函数 $l(\vec\theta)$ 最大的 $\vec\theta$ 值，则 $\hat{\vec\theta}$ 应该是“最可能”的参数值，那么 $\hat{\vec\theta}​$ 就是 $\theta$ 的极大似然估计量。它是样本集的函数，记作：</p>
 $$
\hat{\vec\theta}=d(D)= \mathop {\arg \max}_{\vec\theta} l(\vec\theta )
$$ 
 $\hat{\vec\theta}(\vec x_{1},\vec x_{2},...,\vec x_{n})$ 称为极大似然函数估计值。
<h3 id="2-19-3-贝叶斯分类器基本原理">2.19.3 贝叶斯分类器基本原理</h3>
<p>​	贝叶斯决策论通过<strong>相关概率已知</strong>的情况下利用<strong>误判损失</strong>来选择最优的类别分类。<br>
假设有 $N$ 种可能的分类标记，记为 $Y=\{c_1,c_2,...,c_N\}$ ，那对于样本 $\boldsymbol{x}$ ，它属于哪一类呢？</p>
<p>计算步骤如下：</p>
<p>step 1. 算出样本 $\boldsymbol{x}$ 属于第i个类的概率，即 $P(c_i|x)​$ ；</p>
<p>step 2. 通过比较所有的 $P(c_i|\boldsymbol{x})$ ，得到样本 $\boldsymbol{x}$ 所属的最佳类别。</p>
<p>step 3. 将类别 $c_i$ 和样本 $\boldsymbol{x}$ 代入到贝叶斯公式中，得到：</p>
 $$
P(c_i|\boldsymbol{x})=\frac{P(\boldsymbol{x}|c_i)P(c_i)}{P(\boldsymbol{x})}.
$$ 
<p>​	一般来说， $P(c_i)$ 为先验概率， $P(\boldsymbol{x}|c_i)$ 为条件概率， $P(\boldsymbol{x})$ 是用于归一化的证据因子。对于 $P(c_i)$ 可以通过训练样本中类别为 $c_i$ 的样本所占的比例进行估计；此外，由于只需要找出最大的 $P(\boldsymbol{x}|c_i)$ ，因此我们并不需要计算 $P(\boldsymbol{x})$ 。<br>
​	为了求解条件概率，基于不同假设提出了不同的方法，以下将介绍朴素贝叶斯分类器和半朴素贝叶斯分类器。</p>
<h3 id="2-19-4-朴素贝叶斯分类器">2.19.4 朴素贝叶斯分类器</h3>
<p>​	假设样本 $\boldsymbol{x}$ 包含 $d$ 个属性，即 $\boldsymbol{x}=\{ x_1,x_2,...,x_d\}$ 。于是有：</p>
 $$
P(\boldsymbol{x}|c_i)=P(x_1,x_2,\cdots,x_d|c_i)
$$ 
<p>这个联合概率难以从有限的训练样本中直接估计得到。于是，朴素贝叶斯（Naive Bayesian，简称NB）采用了“属性条件独立性假设”：对已知类别，假设所有属性相互独立。于是有：</p>
 $$
P(x_1,x_2,\cdots,x_d|c_i)=\prod_{j=1}^d P(x_j|c_i)
$$ 
<p>这样的话，我们就可以很容易地推出相应的判定准则了：</p>
 $$
h_{nb}(\boldsymbol{x})=\mathop{\arg \max}_{c_i\in Y} P(c_i)\prod_{j=1}^dP(x_j|c_i)
$$ 
<p><strong>条件概率 $P(x_j|c_i)​$ 的求解</strong></p>
<p>如果 $x_j$ 是标签属性，那么我们可以通过计数的方法估计 $P(x_j|c_i)$</p>
 $$
P(x_j|c_i)=\frac{P(x_j,c_i)}{P(c_i)}\approx\frac{\#(x_j,c_i)}{\#(c_i)}
$$ 
<p>其中， $\#(x_j,c_i)$ 表示在训练样本中 $x_j$ 与 $c_{i}$ 共同出现的次数。</p>
<p>如果 $x_j​$ 是数值属性，通常我们假设类别中 $c_{i}​$ 的所有样本第 $j​$ 个属性的值服从正态分布。我们首先估计这个分布的均值 $μ​$ 和方差 $σ​$ ，然后计算 $x_j​$ 在这个分布中的概率密度 $P(x_j|c_i)​$ 。</p>
<h3 id="2-19-5-举例理解朴素贝叶斯分类器">2.19.5 举例理解朴素贝叶斯分类器</h3>
<p>使用经典的西瓜训练集如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">色泽</th>
<th style="text-align:center">根蒂</th>
<th style="text-align:center">敲声</th>
<th style="text-align:center">纹理</th>
<th style="text-align:center">脐部</th>
<th style="text-align:center">触感</th>
<th style="text-align:center">密度</th>
<th style="text-align:center">含糖率</th>
<th style="text-align:center">好瓜</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.697</td>
<td style="text-align:center">0.460</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.774</td>
<td style="text-align:center">0.376</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.634</td>
<td style="text-align:center">0.264</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.608</td>
<td style="text-align:center">0.318</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.556</td>
<td style="text-align:center">0.215</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.403</td>
<td style="text-align:center">0.237</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.481</td>
<td style="text-align:center">0.149</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.437</td>
<td style="text-align:center">0.211</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.666</td>
<td style="text-align:center">0.091</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">硬挺</td>
<td style="text-align:center">清脆</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.243</td>
<td style="text-align:center">0.267</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">硬挺</td>
<td style="text-align:center">清脆</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.245</td>
<td style="text-align:center">0.057</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.343</td>
<td style="text-align:center">0.099</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.639</td>
<td style="text-align:center">0.161</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.657</td>
<td style="text-align:center">0.198</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">15</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.360</td>
<td style="text-align:center">0.370</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.593</td>
<td style="text-align:center">0.042</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">17</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.719</td>
<td style="text-align:center">0.103</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
<p>对下面的测试例“测1”进行 分类：</p>
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">色泽</th>
<th style="text-align:center">根蒂</th>
<th style="text-align:center">敲声</th>
<th style="text-align:center">纹理</th>
<th style="text-align:center">脐部</th>
<th style="text-align:center">触感</th>
<th style="text-align:center">密度</th>
<th style="text-align:center">含糖率</th>
<th style="text-align:center">好瓜</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">测1</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.697</td>
<td style="text-align:center">0.460</td>
<td style="text-align:center">？</td>
</tr>
</tbody>
</table>
<p>首先，估计类先验概率 $P(c_j)$ ，有</p>
 $$
\begin{align} 
&P(好瓜=是)=\frac{8}{17}=0.471 \newline 
&P(好瓜=否)=\frac{9}{17}=0.529 
\end{align}
$$ 
<p>然后，为每个属性估计条件概率（这里，对于连续属性，假定它们服从正态分布）</p>
 $$
P_{青绿|是}=P（色泽=青绿|好瓜=是）=\frac{3}{8}=0.375
$$ 
 $$
P_{青绿|否}=P（色泽=青绿|好瓜=否）=\frac{3}{9}\approx0.333
$$ 
 $$
P_{蜷缩|是}=P（根蒂=蜷缩|好瓜=是）=\frac{5}{8}=0.625
$$ 
 $$
P_{蜷缩|否}=P（根蒂=蜷缩|好瓜=否）=\frac{3}{9}=0.333
$$ 
 $$
P_{浊响|是}=P（敲声=浊响|好瓜=是）=\frac{6}{8}=0.750
$$ 
 $$
P_{浊响|否}=P（敲声=浊响|好瓜=否）=\frac{4}{9}\approx 0.444
$$ 
 $$
P_{清晰|是}=P（纹理=清晰|好瓜=是）=\frac{7}{8}= 0.875
$$ 
 $$
P_{清晰|否}=P（纹理=清晰|好瓜=否）=\frac{2}{9}\approx 0.222
$$ 
 $$
P_{凹陷|是}=P（脐部=凹陷|好瓜=是）=\frac{6}{8}= 0.750
$$ 
 $$
P_{凹陷|否}=P（脐部=凹陷|好瓜=否）=\frac{2}{9} \approx 0.222
$$ 
 $$
P_{硬滑|是}=P（触感=硬滑|好瓜=是）=\frac{6}{8}= 0.750
$$ 
 $$
P_{硬滑|否}=P（触感=硬滑|好瓜=否）=\frac{6}{9} \approx 0.667
$$ 
 $$
\begin{aligned}
\rho_{密度：0.697|是}&=\rho（密度=0.697|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.129}exp\left( -\frac{(0.697-0.574)^2}{2\times0.129^2}\right) \approx 1.959
\end{aligned}
$$ 
 $$
\begin{aligned}
\rho_{密度：0.697|否}&=\rho（密度=0.697|好瓜=否）\\&=\frac{1}{\sqrt{2 \pi}\times0.195}exp\left( -\frac{(0.697-0.496)^2}{2\times0.195^2}\right) \approx 1.203
\end{aligned}
$$ 
 $$
\begin{aligned}
\rho_{含糖：0.460|是}&=\rho（密度=0.460|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.101}exp\left( -\frac{(0.460-0.279)^2}{2\times0.101^2}\right) \approx 0.788
\end{aligned}
$$ 
 $$
\begin{aligned}
\rho_{含糖：0.460|否}&=\rho（密度=0.460|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.108}exp\left( -\frac{(0.460-0.154)^2}{2\times0.108^2}\right) \approx 0.066
\end{aligned}
$$ 
<p>于是有</p>
 $$
\begin{align} 
P(&好瓜=是)\times P_{青绿|是} \times P_{蜷缩|是} \times P_{浊响|是} \times P_{清晰|是} \times P_{凹陷|是}\newline 
&\times P_{硬滑|是} \times p_{密度：0.697|是} \times p_{含糖：0.460|是} \approx 0.063 \newline\newline 
P(&好瓜=否)\times P_{青绿|否} \times P_{蜷缩|否} \times P_{浊响|否} \times P_{清晰|否} \times P_{凹陷|否}\newline 
&\times P_{硬滑|否} \times p_{密度：0.697|否} \times p_{含糖：0.460|否} \approx 6.80\times 10^{-5} 
\end{align}
$$ 
<p>由于 $0.063>6.80\times 10^{-5}$ ，因此，朴素贝叶斯分类器将测试样本“测1”判别为“好瓜”。</p>
<h3 id="2-19-6-半朴素贝叶斯分类器">2.19.6 半朴素贝叶斯分类器</h3>
<p>​	朴素贝叶斯采用了“属性条件独立性假设”，半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息。<strong>独依赖估计</strong>（One-Dependence Estimator，简称ODE）是半朴素贝叶斯分类器最常用的一种策略。顾名思义，独依赖是假设每个属性在类别之外最多依赖一个其他属性，即：</p>
 $$
P(\boldsymbol{x}|c_i)=\prod_{j=1}^d P(x_j|c_i,{\rm pa}_j)
$$ 
<p>其中 $pa_j$ 为属性 $x_i$ 所依赖的属性，成为 $x_i$ 的父属性。假设父属性 $pa_j$ 已知，那么可以使用下面的公式估计 $P(x_j|c_i,{\rm pa}_j)$</p>
 $$
P(x_j|c_i,{\rm pa}_j)=\frac{P(x_j,c_i,{\rm pa}_j)}{P(c_i,{\rm pa}_j)}
$$ 
<h2 id="2-20-EM算法">2.20 EM算法</h2>
<h3 id="2-20-1-EM算法基本思想">2.20.1 EM算法基本思想</h3>
<p>​	最大期望算法（Expectation-Maximization algorithm, EM），是一类通过迭代进行极大似然估计的优化算法，通常作为牛顿迭代法的替代，用于对包含隐变量或缺失数据的概率模型进行参数估计。</p>
<p>​	最大期望算法基本思想是经过两个步骤交替进行计算：</p>
<p>​	第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值**；**</p>
<p>​	第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。</p>
<p>​	M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。</p>
<h3 id="2-20-2-EM算法推导">2.20.2 EM算法推导</h3>
<p>​	对于 $m$ 个样本观察数据 $x=(x^{1},x^{2},...,x^{m})$ ，现在想找出样本的模型参数 $\theta$ ，其极大化模型分布的对数似然函数为：</p>
 $$
\theta = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta)
$$ 
<p>如果得到的观察数据有未观察到的隐含数据 $z=(z^{(1)},z^{(2)},...z^{(m)})$ ，极大化模型分布的对数似然函数则为：</p>
 $$
\theta =\mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta) = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)  \tag{a}
$$ 
<p>由于上式不能直接求出 $\theta$ ，采用缩放技巧：</p>
 $$
\begin{align} \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)   & = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \\ & \geqslant  \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \end{align}   \tag{1}
$$ 
<p>上式用到了Jensen不等式：</p>
 $$
log\sum\limits_j\lambda_jy_j \geqslant \sum\limits_j\lambda_jlogy_j\;\;,  \lambda_j \geqslant 0, \sum\limits_j\lambda_j =1
$$ 
<p>并且引入了一个未知的新分布 $Q_i(z^{(i)})$ 。</p>
<p>此时，如果需要满足Jensen不等式中的等号，所以有：</p>
 $$
\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} =c, c为常数
$$ 
<p>由于 $Q_i(z^{(i)})$ 是一个分布，所以满足</p>
 $$
\sum\limits_{z}Q_i(z^{(i)}) =1
$$ 
<p>综上，可得：</p>
 $$
Q_i(z^{(i)})  = \frac{P(x^{(i)}， z^{(i)};\theta)}{\sum\limits_{z}P(x^{(i)}, z^{(i)};\theta)} =  \frac{P(x^{(i)}, z^{(i)};\theta)}{P(x^{(i)};\theta)} = P( z^{(i)}|x^{(i)};\theta)
$$ 
<p>如果 $Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)};\theta)$  ，则第(1)式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要最大化下式：</p>
 $$
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)};\theta)}{Q_i(z^{(i)})}
$$ 
<p>简化得：</p>
 $$
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}
$$ 
<p>以上即为EM算法的M步， $\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}​$ 可理解为 $logP(x^{(i)}, z^{(i)};\theta) $ 基于条件概率分布 $Q_i(z^{(i)}) $ 的期望。以上即为EM算法中E步和M步的具体数学含义。</p>
<h3 id="2-20-3-图解EM算法">2.20.3 图解EM算法</h3>
<p>​	考虑上一节中的（a）式，表达式中存在隐变量，直接找到参数估计比较困难，通过EM算法迭代求解下界的最大值到收敛为止。</p>
<p><img src="2.20.1.jpg" alt></p>
<p>​	图片中的紫色部分是我们的目标模型 $p(x|\theta)$ ，该模型复杂，难以求解析解，为了消除隐变量 $z^{(i)}$ 的影响，我们可以选择一个不包含 $z^{(i)}$ 的模型 $r(x|\theta)$ ，使其满足条件 $r(x|\theta) \leqslant p(x|\theta) $ 。</p>
<p>求解步骤如下：</p>
<p>（1）选取 $\theta_1$ ，使得 $r(x|\theta_1) = p(x|\theta_1)$ ，然后对此时的 $r$ 求取最大值，得到极值点 $\theta_2$ ，实现参数的更新。</p>
<p>（2）重复以上过程到收敛为止，在更新过程中始终满足 $r \leqslant p $ .</p>
<h3 id="2-20-4-EM算法流程">2.20.4 EM算法流程</h3>
<p>输入：观察数据 $x=(x^{(1)},x^{(2)},...x^{(m)})$ ，联合分布 $p(x,z ;\theta)$ ，条件分布 $p(z|x; \theta)$ ，最大迭代次数 $J$</p>
<p>1）随机初始化模型参数 $\theta$ 的初值 $\theta^0$ 。</p>
<p>2） $for \ j  \ from \ 1  \ to  \ j$ ：</p>
<p>​	a） E步。计算联合分布的条件概率期望：</p>
 $$
Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}, \theta^{j})
$$ 
 $$
L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}, \theta^{j})log{P(x^{(i)}, z^{(i)};\theta)}
$$ 
<p>​	b） M步。极大化 $L(\theta, \theta^{j})$ ，得到 $\theta^{j+1}$ :</p>
 $$
\theta^{j+1} = \mathop{\arg\max}_\theta L(\theta, \theta^{j})
$$ 
<p>​	c） 如果 $\theta^{j+1}$ 收敛，则算法结束。否则继续回到步骤a）进行E步迭代。</p>
<p>输出：模型参数 $\theta​$ 。</p>
<h2 id="2-21-降维和聚类">2.21 降维和聚类</h2>
<h3 id="2-21-1-图解为什么会产生维数灾难">2.21.1 图解为什么会产生维数灾难</h3>
<p>​	假如数据集包含10张照片，照片中包含三角形和圆两种形状。现在来设计一个分类器进行训练，让这个分类器对其他的照片进行正确分类（假设三角形和圆的总数是无限大），简单的，我们用一个特征进行分类：</p>
<p><img src="2.21.1.1.png" alt></p>
<p>​											图2.21.1.a</p>
<p>​	从上图可看到，如果仅仅只有一个特征进行分类，三角形和圆几乎是均匀分布在这条线段上，很难将10张照片线性分类。那么，增加一个特征后的情况会怎么样：</p>
<p><img src="2.21.1.2.png" alt></p>
<p>​											图2.21.1.b</p>
<p>增加一个特征后，我们发现仍然无法找到一条直线将猫和狗分开。所以，考虑需要再增加一个特征：</p>
<p><img src="2.21.1.3.png" alt></p>
<p>​											图2.21.1.c</p>
<p><img src="2.21.1.4.png" alt></p>
<p>​											图2.21.1.d</p>
<p>​	此时，可以找到一个平面将三角形和圆分开。</p>
<p>​	现在计算一下不同特征数是样本的密度：</p>
<p>​	（1）一个特征时，假设特征空间时长度为5的线段，则样本密度为 $10 \div 5 = 2$ 。</p>
<p>​	（2）两个特征时，特征空间大小为 $ 5\times5 = 25$ ，样本密度为 $10 \div 25 = 0.4$ 。</p>
<p>​	（3）三个特征时，特征空间大小是 $ 5\times5\times5 = 125$ ，样本密度为 $10 \div 125 = 0.08$ 。</p>
<p>​	以此类推，如果继续增加特征数量，样本密度会越来越稀疏，此时，更容易找到一个超平面将训练样本分开。当特征数量增长至无限大时，样本密度就变得非常稀疏。</p>
<p>​	下面看一下将高维空间的分类结果映射到低维空间时，会出现什么情况？</p>
<p><img src="2.21.1.5.png" alt></p>
<p>​										图2.21.1.e</p>
<p>​	上图是将三维特征空间映射到二维特征空间后的结果。尽管在高维特征空间时训练样本线性可分，但是映射到低维空间后，结果正好相反。事实上，增加特征数量使得高维空间线性可分，相当于在低维空间内训练一个复杂的非线性分类器。不过，这个非线性分类器太过“聪明”，仅仅学到了一些特例。如果将其用来辨别那些未曾出现在训练样本中的测试样本时，通常结果不太理想，会造成过拟合问题。</p>
<p><img src="2.21.1.6a.png" alt></p>
<p>​										图2.21.1.f</p>
<p>​	上图所示的只采用2个特征的线性分类器分错了一些训练样本，准确率似乎没有图2.21.1.e的高，但是，采用2个特征的线性分类器的泛化能力比采用3个特征的线性分类器要强。因为，采用2个特征的线性分类器学习到的不只是特例，而是一个整体趋势，对于那些未曾出现过的样本也可以比较好地辨别开来。换句话说，通过减少特征数量，可以避免出现过拟合问题，从而避免“维数灾难”。</p>
<p><img src="2.21.1.6.png" alt></p>
<p>​	上图从另一个角度诠释了“维数灾难”。假设只有一个特征时，特征的值域是0到1，每一个三角形和圆的特征值都是唯一的。如果我们希望训练样本覆盖特征值值域的20%，那么就需要三角形和圆总数的20%。我们增加一个特征后，为了继续覆盖特征值值域的20%就需要三角形和圆总数的45%( $0.452^2\approx0.2$ )。继续增加一个特征后，需要三角形和圆总数的58%( $0.583^3\approx0.2$ )。随着特征数量的增加，为了覆盖特征值值域的20%，就需要更多的训练样本。如果没有足够的训练样本，就可能会出现过拟合问题。</p>
<p>​	通过上述例子，我们可以看到特征数量越多，训练样本就会越稀疏，分类器的参数估计就会越不准确，更加容易出现过拟合问题。“维数灾难”的另一个影响是训练样本的稀疏性并不是均匀分布的。处于中心位置的训练样本比四周的训练样本更加稀疏。</p>
<p><img src="2.21.1.7.png" alt></p>
<p>​	假设有一个二维特征空间，如上图所示的矩形，在矩形内部有一个内切的圆形。由于越接近圆心的样本越稀疏，因此，相比于圆形内的样本，那些位于矩形四角的样本更加难以分类。当维数变大时，特征超空间的容量不变，但单位圆的容量会趋于0，在高维空间中，大多数训练数据驻留在特征超空间的角落。散落在角落的数据要比处于中心的数据难于分类。</p>
<h3 id="2-21-2-怎样避免维数灾难">2.21.2 怎样避免维数灾难</h3>
<p><strong>有待完善！！！</strong></p>
<p>解决维度灾难问题：</p>
<p>主成分分析法PCA，线性判别法LDA</p>
<p>奇异值分解简化数据、拉普拉斯特征映射</p>
<p>Lassio缩减系数法、小波分析法、</p>
<h3 id="2-21-3-聚类和降维有什么区别与联系">2.21.3 聚类和降维有什么区别与联系</h3>
<p>​	聚类用于找寻数据内在的分布结构，既可以作为一个单独的过程，比如异常检测等等。也可作为分类等其他学习任务的前驱过程。聚类是标准的无监督学习。</p>
<p>​	1）在一些推荐系统中需确定新用户的类型，但定义“用户类型”却可能不太容易，此时往往可先对原有的用户数据进行聚类，根据聚类结果将每个簇定义为一个类,然后再基于这些类训练分类模型,用于判别新用户的类型。</p>
<p><img src="2.21.3.1.png" alt></p>
<p>​	2）而降维则是为了缓解维数灾难的一个重要方法，就是通过某种数学变换将原始高维属性空间转变为一个低维“子空间”。其基于的假设就是，虽然人们平时观测到的数据样本虽然是高维的，但是实际上真正与学习任务相关的是个低维度的分布。从而通过最主要的几个特征维度就可以实现对数据的描述，对于后续的分类很有帮助。比如对于Kaggle（数据分析竞赛平台之一）上的泰坦尼克号生还问题。通过给定一个乘客的许多特征如年龄、姓名、性别、票价等，来判断其是否能在海难中生还。这就需要首先进行特征筛选，从而能够找出主要的特征，让学习到的模型有更好的泛化性。</p>
<p>​	聚类和降维都可以作为分类等问题的预处理步骤。</p>
<p><img src="2-19.jpg" alt></p>
<p>​	但是他们虽然都能实现对数据的约减。但是二者适用的对象不同，聚类针对的是数据点，而降维则是对于数据的特征。另外它们有着很多种实现方法。聚类中常用的有K-means、层次聚类、基于密度的聚类等；降维中常用的则PCA、Isomap、LLE等。</p>
<h3 id="2-21-4-有哪些聚类算法优劣衡量标准">2.21.4 有哪些聚类算法优劣衡量标准</h3>
<p>不同聚类算法有不同的优劣和不同的适用条件。可从以下方面进行衡量判断：<br>
1、算法的处理能力：处理大的数据集的能力，即算法复杂度；处理数据噪声的能力；处理任意形状，包括有间隙的嵌套的数据的能力；<br>
2、算法是否需要预设条件：是否需要预先知道聚类个数，是否需要用户给出领域知识；</p>
<p>​    3、算法的数据输入属性：算法处理的结果与数据输入的顺序是否相关，也就是说算法是否独立于数据输入顺序；算法处理有很多属性数据的能力，也就是对数据维数是否敏感，对数据的类型有无要求。</p>
<h3 id="2-21-5-聚类和分类有什么区别">2.21.5 聚类和分类有什么区别</h3>
<p>**聚类（Clustering） **<br>
聚类，简单地说就是把相似的东西分到一组，聚类的时候，我们并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。一个聚类算法通常只需要知道如何计算相似度就可以开始工作了，因此聚类通常并不需要使用训练数据进行学习，在机器学习中属于无监督学习。</p>
<p>**分类（Classification） **</p>
<p>​     分类，对于一个分类器，通常需要你告诉它“这个东西被分为某某类”。一般情况下，一个分类器会从它得到的训练集中进行学习，从而具备对未知数据进行分类的能力，在机器学习中属于监督学习。</p>
<h3 id="2-21-6-不同聚类算法特点性能比较">2.21.6 不同聚类算法特点性能比较</h3>
<table>
<thead>
<tr>
<th style="text-align:center">算法名称</th>
<th style="text-align:center">可伸缩性</th>
<th style="text-align:center">适合的数据类型</th>
<th style="text-align:center">高维性</th>
<th style="text-align:center">异常数据抗干扰性</th>
<th style="text-align:center">聚类形状</th>
<th style="text-align:center">算法效率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">WAVECLUSTER</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">很高</td>
</tr>
<tr>
<td style="text-align:center">ROCK</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">混合型</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">BIRCH</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">球形</td>
<td style="text-align:center">很高</td>
</tr>
<tr>
<td style="text-align:center">CURE</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">较高</td>
</tr>
<tr>
<td style="text-align:center">K-PROTOTYPES</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">混合型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">DENCLUE</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">较高</td>
</tr>
<tr>
<td style="text-align:center">OPTIGRID</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">CLIQUE</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">较低</td>
</tr>
<tr>
<td style="text-align:center">DBSCAN</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">CLARANS</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">球形</td>
<td style="text-align:center">较低</td>
</tr>
</tbody>
</table>
<h3 id="2-21-7-四种常用聚类方法之比较">2.21.7 四种常用聚类方法之比较</h3>
<p>​	聚类就是按照某个特定标准把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。<br>
​	主要的聚类算法可以划分为如下几类：划分方法、层次方法、基于密度的方法、基于网格的方法以及基于模型的方法。下面主要对k-means聚类算法、凝聚型层次聚类算法、神经网络聚类算法之SOM,以及模糊聚类的FCM算法通过通用测试数据集进行聚类效果的比较和分析。</p>
<h3 id="2-21-8-k-means聚类算法">2.21.8 k-means聚类算法</h3>
<p>k-means是划分方法中较经典的聚类算法之一。由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。目前，许多算法均围绕着该算法进行扩展和改进。<br>
k-means算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。k-means算法的处理过程如下：首先，随机地 选择k个对象，每个对象初始地代表了一个簇的平均值或中心;对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇;然后重新计算每个簇的平均值。 这个过程不断重复，直到准则函数收敛。通常，采用平方误差准则，其定义如下：</p>
 $$
E=\sum_{i=1}^{k}\sum_{p\in C_i}\left\|p-m_i\right\|^2
$$ 
<p>这里E是数据中所有对象的平方误差的总和，p是空间中的点， $m_i$ 是簇 $C_i$ 的平均值[9]。该目标函数使生成的簇尽可能紧凑独立，使用的距离度量是欧几里得距离，当然也可以用其他距离度量。</p>
<p><strong>算法流程</strong>：<br>
​    输入：包含n个对象的数据和簇的数目k；<br>
​    输出：n个对象到k个簇，使平方误差准则最小。<br>
​    步骤：<br>
　　(1) 任意选择k个对象作为初始的簇中心；<br>
　　(2) 根据簇中对象的平均值，将每个对象(重新)赋予最类似的簇；<br>
　　(3) 更新簇的平均值，即计算每个簇中对象的平均值；<br>
　　(4) 重复步骤(2)、(3)直到簇中心不再变化；</p>
<h3 id="2-21-9-层次聚类算法">2.21.9 层次聚类算法</h3>
<p>​    根据层次分解的顺序是自底向上的还是自上向下的，层次聚类算法分为凝聚的层次聚类算法和分裂的层次聚类算法。<br>
　凝聚型层次聚类的策略是先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有对象都在一个簇中，或者某个终结条件被满足。绝大多数层次聚类属于凝聚型层次聚类，它们只是在簇间相似度的定义上有所不同。</p>
<p><strong>算法流程</strong>：</p>
<p>注：以采用最小距离的凝聚层次聚类算法为例：</p>
<p>(1) 将每个对象看作一类，计算两两之间的最小距离；<br>
　(2) 将距离最小的两个类合并成一个新类；<br>
　(3) 重新计算新类与所有类之间的距离；<br>
　(4) 重复(2)、(3)，直到所有类最后合并成一类。</p>
<h3 id="2-21-10-SOM聚类算法">2.21.10 SOM聚类算法</h3>
<p>​	SOM神经网络[11]是由芬兰神经网络专家Kohonen教授提出的，该算法假设在输入对象中存在一些拓扑结构或顺序，可以实现从输入空间(n维)到输出平面(2维)的降维映射，其映射具有拓扑特征保持性质,与实际的大脑处理有很强的理论联系。</p>
<p>​	SOM网络包含输入层和输出层。输入层对应一个高维的输入向量，输出层由一系列组织在2维网格上的有序节点构成，输入节点与输出节点通过权重向量连接。 学习过程中，找到与之距离最短的输出层单元，即获胜单元，对其更新。同时，将邻近区域的权值更新，使输出节点保持输入向量的拓扑特征。</p>
<p><strong>算法流程</strong>：</p>
<p>​	(1) 网络初始化，对输出层每个节点权重赋初值；<br>
​	(2) 从输入样本中随机选取输入向量并且归一化，找到与输入向量距离最小的权重向量；<br>
​	(3) 定义获胜单元，在获胜单元的邻近区域调整权重使其向输入向量靠拢；<br>
​	(4) 提供新样本、进行训练；<br>
​	(5) 收缩邻域半径、减小学习率、重复，直到小于允许值，输出聚类结果。</p>
<h3 id="2-21-11-FCM聚类算法">2.21.11 FCM聚类算法</h3>
<p>​	1965年美国加州大学柏克莱分校的扎德教授第一次提出了‘集合’的概念。经过十多年的发展，模糊集合理论渐渐被应用到各个实际应用方面。为克服非此即彼的分类缺点，出现了以模糊集合论为数学基础的聚类分析。用模糊数学的方法进行聚类分析，就是模糊聚类分析[12]。<br>
​	FCM算法是一种以隶属度来确定每个数据点属于某个聚类程度的算法。该聚类算法是传统硬聚类算法的一种改进。<br>
​	设数据集 $X={x_1,x_2,...,x_n}$ ,它的模糊 $c$ 划分可用模糊矩阵 $U=[u_{ij}]$ 表示，矩阵 $U$ 的元素 $u_{ij}$ 表示第 $j(j=1,2,...,n)$ 个数据点属于第 $i(i=1,2,...,c)$ 类的隶属度， $u_{ij}$ 满足如下条件：</p>
 $$
\begin{equation}
\left\{
\begin{array}{lr}
\sum_{i=1}^c u_{ij}=1 \quad\forall~j
\\u_{ij}\in[0,1] \quad\forall ~i,j
\\\sum_{j=1}^c u_{ij}>0 \quad\forall ~i
\end{array}
\right.
\end{equation}
$$ 
<p>目前被广泛使用的聚类准则是取类内加权误差平方和的极小值。即：</p>
 $$
(min)J_m(U,V)=\sum^n_{j=1}\sum^c_{i=1}u^m_{ij}d^2_{ij}(x_j,v_i)
$$ 
<p>其中 $V$ 为聚类中心， $m$ 为加权指数， $d_{ij}(x_j,v_i)=||v_i-x_j||$ 。</p>
<p><strong>算法流程</strong>：</p>
<p>(1) 标准化数据矩阵；<br>
　(2) 建立模糊相似矩阵，初始化隶属矩阵；<br>
　(3) 算法开始迭代，直到目标函数收敛到极小值；<br>
　(4) 根据迭代结果，由最后的隶属矩阵确定数据所属的类，显示最后的聚类结果。</p>
<h3 id="2-21-12-四种聚类算法试验">2.21.12 四种聚类算法试验</h3>
<p>​	选取专门用于测试分类、聚类算法的国际通用的UCI数据库中的IRIS数据集，IRIS数据集包含150个样本数据，分别取自三种不同 的莺尾属植物setosa、versicolor和virginica的花朵样本,每个数据含有4个属性，即萼片长度、萼片宽度、花瓣长度、花瓣宽度，单位为cm。 在数据集上执行不同的聚类算法，可以得到不同精度的聚类结果。基于前面描述的各算法原理及流程，可初步得如下聚类结果。</p>
<table>
<thead>
<tr>
<th>聚类方法</th>
<th>聚错样本数</th>
<th>运行时间/s</th>
<th>平均准确率/（%）</th>
</tr>
</thead>
<tbody>
<tr>
<td>K-means</td>
<td>17</td>
<td>0.146001</td>
<td>89</td>
</tr>
<tr>
<td>层次聚类</td>
<td>51</td>
<td>0.128744</td>
<td>66</td>
</tr>
<tr>
<td>SOM</td>
<td>22</td>
<td>5.267283</td>
<td>86</td>
</tr>
<tr>
<td>FCM</td>
<td>12</td>
<td>0.470417</td>
<td>92</td>
</tr>
</tbody>
</table>
<p><strong>注</strong>：</p>
<p>(1) 聚错样本数：总的聚错的样本数，即各类中聚错的样本数的和；<br>
(2) 运行时间：即聚类整个过程所耗费的时间，单位为s；<br>
(3) 平均准确度：设原数据集有k个类,用 $c_i$ 表示第i类， $n_i$ 为 $c_i$ 中样本的个数， $m_i$ 为聚类正确的个数,则 $m_i/n_i$ 为 第i类中的精度，则平均精度为： $avg=\frac{1}{k}\sum_{i=1}^{k}\frac{m_{i}}{n_{i}}$ 。</p>
<h2 id="参考文献">参考文献</h2>
<p>[1]   Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.<br>
[2]   周志华. 机器学习[M].清华大学出版社, 2016.<br>
[3]   Michael A. Nielsen. “Neural Networks and Deep Learning”, Determination Press, 2015.<br>
[4]   Suryansh S. Gradient Descent: All You Need to Know, 2018.<br>
[5]   刘建平. 梯度下降小结,EM算法的推导, 2018<br>
[6]   杨小兵．聚类分析中若干关键技术的研究[D]． 杭州：浙江大学, 2005.<br>
[7]   XU Rui, Donald Wunsch 1 1． survey of clustering algorithm[J]．IEEE．Transactions on Neural Networks, 2005, 16(3)：645-67 8.<br>
[8]   YI Hong, SAM K． Learning assignment order of instances for the constrained k-means clustering algorithm[J]．IEEE Transactions on Systems, Man, and Cybernetics, Part B：Cybernetics,2009,39 (2)：568-574.<br>
[9]   贺玲, 吴玲达, 蔡益朝．数据挖掘中的聚类算法综述[J]．计算机应用研究, 2007, 24(1):10-13．<br>
[10]  孙吉贵, 刘杰, 赵连宇．聚类算法研究[J]．软件学报, 2008, 19(1)：48-61．<br>
[11]  孔英会, 苑津莎, 张铁峰等．基于数据流管理技术的配变负荷分类方法研究．中国国际供电会议, CICED2006．<br>
[12]  马晓艳, 唐雁．层次聚类算法研究[J]．计算机科学, 2008, 34(7)：34-36．<br>
[13]  FISHER R A． Iris Plants Database <a target="_blank" rel="noopener" href="https://www.ics.uci.edu/vmlearn/MLRepository.html">https://www.ics.uci.edu/vmlearn/MLRepository.html</a>, Authorized license．<br>
[14]  Quinlan J R. Induction of decision trees[J]. Machine learning, 1986, 1(1): 81-106.<br>
[15]  Breiman L. Random forests[J]. Machine learning, 2001, 45(1): 5-32.</p>
</n<N)$></p></0$></p>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/deep_learning/ch9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/deep_learning/ch9/" class="post-title-link" itemprop="url">图像分割面试题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-24 10:13:57" itemprop="dateModified" datetime="2024-03-24T10:13:57+08:00">2024-03-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">算法面试</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>图像分割</h1>
<h2 id="9-1-图像分割算法分类？">9.1 图像分割算法分类？</h2>
<p>图像分割是预测图像中每一个像素所属的类别或者物体。基于深度学习的图像分割算法主要分为两类：</p>
<p><strong>1.语义分割</strong></p>
<p>为图像中的每个像素分配一个类别，如把画面中的所有物体都指出它们各自的类别。</p>
<p><img src="Semantic-01.png" alt></p>
<p><strong>2.实例分割</strong></p>
<p>与语义分割不同，实例分割只对特定物体进行类别分配，这一点与目标检测有点相似，但目标检测输出的是边界框和类别，而实例分割输出的是掩膜（mask）和类别。</p>
<p><img src="Instance-01.png" alt></p>
<h2 id="9-2-传统的基于CNN的分割方法缺点？">9.2 传统的基于CNN的分割方法缺点？</h2>
<p>传统的基于CNN的分割方法：为了对一个像素分类，使用该像素周围的一个图像块作为CNN的输入，用于训练与预测，这种方法主要有几个缺点：<br>
1）存储开销大，例如，对每个像素使用15 * 15的图像块，然后不断滑动窗口，将图像块输入到CNN中进行类别判断，因此，需要的存储空间随滑动窗口的次数和大小急剧上升；<br>
2）效率低下，相邻像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算有很大程度上的重复；<br>
3）像素块的大小限制了感受区域的大小，通常像素块的大小比整幅图像的大小小很多，只能提取一些局部特征，从而导致分类性能受到限制。<br>
而全卷积网络(FCN)则是从抽象的特征中恢复出每个像素所属的类别。即从图像级别的分类进一步延伸到像素级别的分类。</p>
<h2 id="9-3-FCN">9.3 FCN</h2>
<h3 id="9-3-1-FCN改变了什么">9.3.1 FCN改变了什么?</h3>
<p>​	对于一般的分类CNN网络，如VGG和Resnet，都会在网络的最后加入一些全连接层，经过softmax后就可以获得类别概率信息。但是这个概率信息是1维的，即只能标识整个图片的类别，不能标识每个像素点的类别，所以这种全连接方法不适用于图像分割。<br>
​	而FCN提出可以把后面几个全连接都换成卷积，这样就可以获得一张2维的feature map，后接softmax层获得每个像素点的分类信息，从而解决了分割问题，如图4。</p>
<p><img src="figure_9.1.1_2.jpg" alt></p>
<center>图 4</center>
### 9.3.2 FCN网络结构？ 
<p>​	FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全联接层＋softmax输出）不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。<br>
​	下图是语义分割所采用的全卷积网络(FCN)的结构示意图：</p>
<p><img src="figure_9.1.2_1.jpg" alt></p>
<h3 id="9-3-3-全卷积网络举例？">9.3.3 全卷积网络举例？</h3>
<p>​	通常CNN网络在卷积层之后会接上若干个全连接层, 将卷积层产生的特征图(feature map)映射成一个固定长度的特征向量。以AlexNet为代表的经典CNN结构适合于图像级的分类和回归任务，因为它们最后都得到整个输入图像的一个概率向量。</p>
<p><img src="figure_9.1.3_1.jpg" alt></p>
<p>  <br>
如上图所示：<br>
  <br>
（1）在CNN中, 猫的图片输入到AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高, 用来做分类任务。<br>
  <br>
（2）FCN与CNN的区别在于把CNN最后的全连接层转换成卷积层，输出的是一张已经带有标签的图片, 而这个图片就可以做语义分割。<br>
  <br>
（3）CNN的强大之处在于它的多层结构能自动学习特征，并且可以学习到多个层次的特征: 较浅的卷积层感知域较小，学习到一些局部区域的特征；较深的卷积层具有较大的感知域，能够学习到更加抽象一些的特征。高层的抽象特征对物体的大小、位置和方向等敏感性更低，从而有助于识别性能的提高, 所以我们常常可以将卷积层看作是特征提取器。</p>
<h3 id="9-2-4-全连接层和卷积层如何相互转化？">9.2.4 全连接层和卷积层如何相互转化？</h3>
<p>  <br>
<strong>两者相互转换的可能性：</strong><br>
  <br>
全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：<br>
  <br>
（1）对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块，其余部分都是零。而在其中大部分块中，元素都是相等的。<br>
  <br>
（2）任何全连接层都可以被转化为卷积层。比如VGG16中第一个全连接层是25088 * 4096的数据尺寸，将它转化为512 * 7 * 7 * 4096的数据尺寸，即一个K=4096的全连接层，输入数据体的尺寸是7 * 7 * 512，这个全连接层可以被等效地看做一个F=7, P=0, S=1, K=4096 的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致7 * 7, 这样输出就变为1 * 1 * 4096, 本质上和全连接层的输出是一样的。<br>
  <br>
<strong>输出激活数据体深度是由卷积核的数目决定的(K=4096)。</strong><br>
  <br>
在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是227x227x3的图像，一系列的卷积层和下采样层将图像数据变为尺寸为7x7x512的激活数据体, AlexNet的处理方式为使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：<br>
  <br>
（1）第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7,K=4096，这样输出数据体就为[1x1x4096]。<br>
  <br>
（2）第二个全连接层，令其滤波器尺寸为F=1,K=4096，这样输出数据体为[1x1x4096]。<br>
  <br>
（3）最后一个全连接层也做类似的，令其F=1,K=1000，最终输出为[1x1x1000]。</p>
<h3 id="9-2-5-为什么传统CNN的输入图片是固定大小？">9.2.5 为什么传统CNN的输入图片是固定大小？</h3>
<p>  <br>
对于CNN，一幅输入图片在经过卷积和pooling层时，这些层是不关心图片大小的。比如对于一个卷积层，outputsize = (inputsize - kernelsize) / stride + 1，它并不关心inputsize多大，对于一个inputsize大小的输入feature map，滑窗卷积，输出outputsize大小的feature map即可。pooling层同理。但是在进入全连接层时，feature map（假设大小为n×n）要拉成一条向量，而向量中每个元素（共n×n个）作为一个结点都要与下一个层的所有结点（假设4096个）全连接，这里的权值个数是4096×n×n，而我们知道神经网络结构一旦确定，它的权值个数都是固定的，所以这个n不能变化，n是conv5的outputsize，所以层层向回看，每个outputsize都要固定，那每个inputsize都要固定，因此输入图片大小要固定。</p>
<h3 id="9-2-6-把全连接层的权重W重塑成卷积层的滤波器有什么好处？">9.2.6 把全连接层的权重W重塑成卷积层的滤波器有什么好处？</h3>
<p>  <br>
这样的转化可以在单个向前传播的过程中, 使得卷积网络在一张更大的输入图片上滑动，从而得到多个输出(可以理解为一个label map)。<br>
  <br>
比如: 我们想让224×224尺寸的浮窗，以步长为32在384×384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6×6个位置的类别得分, 那么通过将全连接层转化为卷积层之后的运算过程为:<br>
  <br>
如果224×224的输入图片经过卷积层和下采样层之后得到了[7x7x512]的数组，那么，384×384的大图片直接经过同样的卷积层和下采样层之后会得到[12x12x512]的数组, 然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出((12 – 7)/1 + 1 = 6), 这个结果正是浮窗在原图经停的6×6个位置的得分。<br>
  <br>
一个确定的CNN网络结构之所以要固定输入图片大小，是因为全连接层权值数固定，而该权值数和feature map大小有关, 但是FCN在CNN的基础上把1000个结点的全连接层改为含有1000个1×1卷积核的卷积层，经过这一层，还是得到二维的feature map，同样我们也不关心这个feature map大小, 所以对于输入图片的size并没有限制。<br>
  <br>
如下图所示，FCN将传统CNN中的全连接层转化成卷积层，对应CNN网络FCN把最后三层全连接层转换成为三层卷积层:</p>
<p><img src="figure_9.1.7_1.png" alt></p>
<center>一个分类网络</center>
![](ch9/figure_9.1.7_2.png)
<center>变为全卷积网络</center>
![](ch9/figure_9.1.7_3.png)
<center>End-to-end, pixels-to pixels网络</center>
![](ch9/figure_9.1.7_4.jpg)
<p>（1）全连接层转化为全卷积层 : 在传统的CNN结构中，前5层是卷积层，第6层和第7层分别是一个长度为4096的一维向量，第8层是长度为1000的一维向量，分别对应1000个不同类别的概率。FCN将这3层表示为卷积层，卷积核的大小 (通道数，宽，高) 分别为 (4096,1,1)、(4096,1,1)、(1000,1,1)。看上去数字上并没有什么差别，但是卷积跟全连接是不一样的概念和计算过程，使用的是之前CNN已经训练好的权值和偏置，但是不一样的在于权值和偏置是有自己的范围，属于自己的一个卷积核。<br>
  <br>
（2）CNN中输入的图像大小是统一固定成227x227大小的图像，第一层pooling后为55x55，第二层pooling后图像大小为27x27，第五层pooling后的图像大小为13x13, 而FCN输入的图像是H * W大小，第一层pooling后变为原图大小的1/2，第二层变为原图大小的1/4，第五层变为原图大小的1/8，第八层变为原图大小的1/16。<br>
  <br>
（3）经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到H/32 * W/32的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是我们最重要的高维特征图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大几次到原图像的大小。<br>
  <br>
相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算优化模型，然后再对36个位置做预测，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。</p>
<h3 id="9-2-7-反卷积层理解">9.2.7 反卷积层理解</h3>
<p>  <br>
Upsampling的操作可以看成是反卷积(deconvolutional)，卷积运算的参数和CNN的参数一样是在训练FCN模型的过程中通过bp算法学习得到。反卷积层也是卷积层，不关心input大小，滑窗卷积后输出output。deconv并不是真正的deconvolution（卷积的逆变换），最近比较公认的叫法应该是transposed convolution，deconv的前向传播就是conv的反向传播。<br>
  <br>
反卷积参数: 利用卷积过程filter的转置（实际上就是水平和竖直方向上翻转filter）作为计算卷积前的特征图。<br>
  <br>
反卷积的运算如下所示:<br>
  <br>
蓝色是反卷积层的input，绿色是反卷积层的outputFull padding, transposed Full padding, transposed。</p>
<p><img src="figure_9.1.8_1.png" alt></p>
<center>上图中的反卷积，input是2×2, output是4×4。     Zero padding, non-unit strides, transposed。</center>
![](ch9/figure_9.1.8_2.png)
<center>上图中的反卷积，input feature map是3×3, 转化后是5×5, output是5×5</center>
### 9.2.8 跳级(skip)结构
<p>  <br>
对CNN的结果做处理，得到了dense prediction，而作者在试验中发现，得到的分割结果比较粗糙，所以考虑加入更多前层的细节信息，也就是把倒数第几层的输出和最后的输出做一个fusion，实际上也就是加和：</p>
<p><img src="figure_9.1.9_1.png" alt><br>
  <br>
实验表明，这样的分割结果更细致更准确。在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。</p>
<h3 id="9-2-9-模型训练">9.2.9 模型训练</h3>
<p>  <br>
（1）用AlexNet，VGG16或者GoogleNet训练好的模型做初始化，在这个基础上做fine-tuning，全部都fine-tuning，只需在末尾加上upsampling，参数的学习还是利用CNN本身的反向传播原理。<br>
  <br>
（2）采用whole image做训练，不进行patchwise sampling。实验证明直接用全图已经很effective and efficient。<br>
  <br>
（3）对class score的卷积层做全零初始化。随机初始化在性能和收敛上没有优势。<br>
<em>举例：</em><br>
  <br>
<em>FCN例子: 输入可为任意尺寸图像彩色图像；输出与输入尺寸相同，深度为：20类目标+背景=21，模型基于AlexNet。</em><br>
  <br>
<em>蓝色：卷积层。</em><br>
  <br>
<em>绿色：Max Pooling层。</em><br>
  <br>
<em>黄色: 求和运算, 使用逐数据相加，把三个不同深度的预测结果进行融合：较浅的结果更为精细，较深的结果更为鲁棒。</em><br>
  <br>
<em>灰色: 裁剪, 在融合之前，使用裁剪层统一两者大小, 最后裁剪成和输入相同尺寸输出。</em><br>
  <br>
<em>对于不同尺寸的输入图像，各层数据的尺寸（height，width）相应变化，深度（channel）不变。</em></p>
<p><img src="figure_9.1.10_1.png" alt><br>
  <br>
（1）全卷积层部分进行特征提取, 提取卷积层（3个蓝色层）的输出来作为预测21个类别的特征。</p>
<p>  <br>
（2）图中虚线内是反卷积层的运算, 反卷积层（3个橙色层）可以把输入数据尺寸放大。和卷积层一样，升采样的具体参数经过训练确定。</p>
<p>    </p>
<ol>
<li>以经典的AlexNet分类网络为初始化。最后两级是全连接（红色），参数弃去不用。</li>
</ol>
<p><img src="figure_9.1.10_2.png" alt><br>
    <br>
2) 从特征小图（）预测分割小图（），之后直接升采样为大图。</p>
<p><img src="figure_9.1.10_3.png" alt></p>
<center>反卷积（橙色）的步长为32，这个网络称为FCN-32s</center>  
&emsp;&emsp;&emsp;&emsp;
3) 升采样分为两次完成（橙色×2）, 在第二次升采样前，把第4个pooling层（绿色）的预测结果（蓝色）融合进来。使用跳级结构提升精确性。
<p><img src="figure_9.1.10_4.png" alt></p>
<center>第二次反卷积步长为16，这个网络称为FCN-16s</center>
&emsp;&emsp;&emsp;&emsp;
4) 升采样分为三次完成（橙色×3）, 进一步融合了第3个pooling层的预测结果。
<p><img src="figure_9.1.10_5.png" alt></p>
<center>第三次反卷积步长为8，记为FCN-8s</center>
其他参数:   
&emsp;&emsp;
minibatch：20张图片。  
&emsp;&emsp;
learning rate：0.001。  
&emsp;&emsp;
初始化：分类网络之外的卷积层参数初始化为0。  
&emsp;&emsp;
反卷积参数初始化为bilinear插值。  
&emsp;&emsp;
最后一层反卷积固定位bilinear插值不做学习。
<p><img src="figure_9.1.10_6.png" alt></p>
<h3 id="9-2-10-FCN缺点">9.2.10 FCN缺点</h3>
<p>  <br>
（1）得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。<br>
  <br>
（2）对各个像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。</p>
<h2 id="9-3-U-Net">9.3 U-Net</h2>
<p>  <br>
卷积网络被大规模应用在分类任务中，输出的结果是整个图像的类标签。然而，在许多视觉任务，尤其是生物医学图像处理领域，目标输出应该包括目标类别的位置，并且每个像素都应该有类标签。另外，在生物医学图像往往缺少训练图片。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。这个网络有两个优点：<br>
第一，输出结果可以定位出目标类别的位置；<br>
第二，由于输入的训练数据是patches，这样就相当于进行了数据增广，解决了生物医学图像数量少的问题。<br>
  <br>
但是，这个方法也有两个很明显缺点。<br>
  <br>
第一，它很慢，因为这个网络必须训练每个patch，并且因为patch间的重叠有很多的冗余(冗余会造成什么影响呢？卷积核里面的W，就是提取特征的权重，两个块如果重叠的部分太多，这个权重会被同一些特征训练两次，造成资源的浪费，减慢训练时间和效率，虽然说会有一些冗余，训练集大了，准确率不就高了吗？可是你这个是相同的图片啊，重叠的东西都是相同的，举个例子，我用一张相同的图片训练20次，按照这个意思也是增大了训练集啊，可是会出现什么结果呢，很显然，会导致过拟合，也就是对你这个图片识别很准，别的图片就不一定了)。<br>
  <br>
第二，定位准确性和获取上下文信息不可兼得。大的patches需要更多的max-pooling层这样减小了定位准确性(为什么？因为你是对以这个像素为中心的点进行分类，如果patch太大，最后经过全连接层的前一层大小肯定是不变的，如果你patch大就需要更多的pooling达到这个大小，而pooling层越多，丢失信息的信息也越多；小的patches只能看到很小的局部信息，包含的背景信息不够。<br>
  <br>
这篇论文建立了一个更好全卷积方法。我们定义和扩展了这个方法它使用更少的训练图片但产生更精确的分割。</p>
<p><img src="figure_9.2_1.png" alt></p>
<p>  <br>
(1)	使用全卷积神经网络。(全卷积神经网络就是卷积取代了全连接层，全连接层必须固定图像大小而卷积不用，所以这个策略使得，你可以输入任意尺寸的图片，而且输出也是图片，所以这是一个端到端的网络。)<br>
  <br>
(2)	左边的网络是收缩路径：使用卷积和maxpooling。<br>
  <br>
(3)	右边的网络是扩张路径:使用上采样产生的特征图与左侧收缩路径对应层产生的特征图进行concatenate操作。（pooling层会丢失图像信息和降低图像分辨率且是不可逆的操作，对图像分割任务有一些影响，对图像分类任务的影响不大，为什么要做上采样？因为上采样可以补足一些图片的信息，但是信息补充的肯定不完全，所以还需要与左边的分辨率比较高的图片相连接起来（直接复制过来再裁剪到与上采样图片一样大小），这就相当于在高分辨率和更抽象特征当中做一个折衷，因为随着卷积次数增多，提取的特征也更加有效，更加抽象，上采样的图片是经历多次卷积后的图片，肯定是比较高效和抽象的图片，然后把它与左边不怎么抽象但更高分辨率的特征图片进行连接）。<br>
  <br>
(4)	最后再经过两次反卷积操作，生成特征图，再用两个1X1的卷积做分类得到最后的两张heatmap,例如第一张表示的是第一类的得分，第二张表示第二类的得分heatmap,然后作为softmax函数的输入，算出概率比较大的softmax类，选择它作为输入给交叉熵进行反向传播训练。</p>
<p>下面是U-Net模型的代码实现：（贡献者：黄钦建－华南理工大学）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">def get_unet():</span><br><span class="line">    inputs = Input((img_rows, img_cols, 1))</span><br><span class="line">    conv1 = Conv2D(32, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(inputs)</span><br><span class="line">    conv1 = Conv2D(32, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(conv1)</span><br><span class="line">    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)</span><br><span class="line">    # pool1 = Dropout(0.25)(pool1)</span><br><span class="line">    # pool1 = BatchNormalization()(pool1)</span><br><span class="line"></span><br><span class="line">    conv2 = Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(pool1)</span><br><span class="line">    conv2 = Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(conv2)</span><br><span class="line">    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)</span><br><span class="line">    # pool2 = Dropout(0.5)(pool2)</span><br><span class="line">    # pool2 = BatchNormalization()(pool2)</span><br><span class="line"></span><br><span class="line">    conv3 = Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(pool2)</span><br><span class="line">    conv3 = Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(conv3)</span><br><span class="line">    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)</span><br><span class="line">    # pool3 = Dropout(0.5)(pool3)</span><br><span class="line">    # pool3 = BatchNormalization()(pool3)</span><br><span class="line"></span><br><span class="line">    conv4 = Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(pool3)</span><br><span class="line">    conv4 = Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(conv4)</span><br><span class="line">    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)</span><br><span class="line">    # pool4 = Dropout(0.5)(pool4)</span><br><span class="line">    # pool4 = BatchNormalization()(pool4)</span><br><span class="line"></span><br><span class="line">    conv5 = Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(pool4)</span><br><span class="line">    conv5 = Conv2D(512, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(conv5)</span><br><span class="line"></span><br><span class="line">    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(</span><br><span class="line">        2, 2), padding=&#x27;same&#x27;)(conv5), conv4], axis=3)</span><br><span class="line">    # up6 = Dropout(0.5)(up6)</span><br><span class="line">    # up6 = BatchNormalization()(up6)</span><br><span class="line">    conv6 = Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(up6)</span><br><span class="line">    conv6 = Conv2D(256, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(conv6)</span><br><span class="line"></span><br><span class="line">    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(</span><br><span class="line">        2, 2), padding=&#x27;same&#x27;)(conv6), conv3], axis=3)</span><br><span class="line">    # up7 = Dropout(0.5)(up7)</span><br><span class="line">    # up7 = BatchNormalization()(up7)</span><br><span class="line">    conv7 = Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(up7)</span><br><span class="line">    conv7 = Conv2D(128, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(conv7)</span><br><span class="line"></span><br><span class="line">    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(</span><br><span class="line">        2, 2), padding=&#x27;same&#x27;)(conv7), conv2], axis=3)</span><br><span class="line">    # up8 = Dropout(0.5)(up8)</span><br><span class="line">    # up8 = BatchNormalization()(up8)</span><br><span class="line">    conv8 = Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(up8)</span><br><span class="line">    conv8 = Conv2D(64, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(conv8)</span><br><span class="line"></span><br><span class="line">    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(</span><br><span class="line">        2, 2), padding=&#x27;same&#x27;)(conv8), conv1], axis=3)</span><br><span class="line">    # up9 = Dropout(0.5)(up9)</span><br><span class="line">    # up9 = BatchNormalization()(up9)</span><br><span class="line">    conv9 = Conv2D(32, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(up9)</span><br><span class="line">    conv9 = Conv2D(32, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(conv9)</span><br><span class="line"></span><br><span class="line">    # conv9 = Dropout(0.5)(conv9)</span><br><span class="line"></span><br><span class="line">    conv10 = Conv2D(1, (1, 1), activation=&#x27;sigmoid&#x27;)(conv9)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=[inputs], outputs=[conv10])</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=Adam(lr=1e-5),</span><br><span class="line">                  loss=dice_coef_loss, metrics=[dice_coef])</span><br><span class="line"></span><br><span class="line">    return model</span><br></pre></td></tr></table></figure>
<h2 id="9-4-SegNet">9.4 SegNet</h2>
<p>  <br>
可训练的图像分割引擎，包含一个encoder网络，一个对应的decoder网络，衔接像素级分类层，解码网络与VGG16的13层卷积层相同。解码网络是将低分辨率的编码特征图映射到全分辨率的特征图。解码网络使用最大池化层的池化索引进行非线性上采样，上采样过程就不需要学习。上采样得到的稀疏图与可训练的滤波器卷积得到致密的特征图。<br>
  <br>
使用池化层索引进行上采样的优势：<br>
  <br>
1）提升边缘刻画度；<br>
  <br>
2）减少训练的参数；<br>
  <br>
3）这种上采样模式可以包含到任何编码-解码网络中。<br>
  <br>
SegNet网络的结构如下图所示：</p>
<p><img src="figure_9.3_1.jpg" alt></p>
<p>  <br>
SegNet网络结构如图1所示，Input为输入图片，Output为输出分割的图像，不同颜色代表不同的分类。语义分割的重要性就在于不仅告诉你图片中某个东西是什么，而且告知你他在图片的位置。我们可以看到是一个对称网络，由中间绿色pooling层与红色upsampling层作为分割，左边是卷积提取高维特征，并通过pooling使图片变小，SegNet作者称为Encoder，右边是反卷积（在这里反卷积与卷积没有区别）与upsampling，通过反卷积使得图像分类后特征得以重现，upsampling使图像变大，SegNet作者称为Decoder，最后通过Softmax，输出不同分类的最大值。这就是大致的SegNet过程，下面对这个过程里面使用到的方法进行介绍。<br>
  <br>
编码网络与滤波器族卷积得到特征图，进行BN，ReLU，最大池化。最大池化是为了获得空间小位移的平移不变。最大池化和下采样损失了边缘细节，因此，在编码过程中保存边缘信息很重要。考虑到内存原因，只保存最大池化索引，如最大特征值的位置。<br>
  <br>
SegNet解码技术如下图所示：</p>
<p><img src="figure_9.3_2.jpg" alt></p>
<p>  <br>
解码网络使用保存的最大池化索引上采样，得到稀疏的特征图，将特征图与可训练的解码滤波器族卷积得到致密的特征图。之后进行BN。高维的特征图输入soft-max层，对每个像素进行分类，得到每个像素属于K类的概率。  图3中右边是FCN的解码技术，FCN对编码的特征图进行降维，降维后输入到解码网络，解码网络中，上采样使用反卷积实现，上采样的特征图与降维的编码图进行element-wise add得到最终的解码特征图。FCN解码模型需要存储编码特征图，在嵌入式设备中内存紧张。<br>
  <br>
SegNet的Encoder过程中，卷积的作用是提取特征，SegNet使用的卷积为same卷积（详见卷积神经网络CNN（1）)，即卷积后不改变图片大小；在Decoder过程中，同样使用same卷积，不过卷积的作用是为upsampling变大的图像丰富信息，使得在Pooling过程丢失的信息可以通过学习在Decoder得到。SegNet中的卷积与传统CNN的卷积并没有区别。</p>
<h2 id="9-5-空洞卷积-Dilated-Convolutions">9.5 空洞卷积(Dilated Convolutions)</h2>
<p>  <br>
在图像分割领域，图像输入到CNN（典型的网络比如FCN[3]）中，FCN先像传统的CNN那样对图像做卷积再pooling，降低图像尺寸的同时增大感受野，但是由于图像分割预测是pixel-wise的输出，所以要将pooling后较小的图像尺寸upsampling到原始的图像尺寸进行预测（upsampling一般采用deconv反卷积操作，deconv可参见知乎答案如何理解深度学习中的deconvolution networks？），之前的pooling操作使得每个pixel预测都能看到较大感受野信息。因此图像分割FCN中有两个关键，一个是pooling减小图像尺寸增大感受野，另一个是upsampling扩大图像尺寸。在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？答案就是dilated conv。<br>
  <br>
以前的CNN主要问题总结：<br>
  <br>
（1）Up-sampling / pooling layer<br>
  <br>
（2）内部数据结构丢失；空间层级化信息丢失。<br>
  <br>
（3）小物体信息无法重建 (假设有四个pooling layer 则 任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。)<br>
  <br>
举例如下：</p>
<p><img src="figure_9.3_3.png" alt></p>
<center>Dilated Convolution with a 3 x 3 kernel and dilation rate 2</center>  
&emsp;&emsp;
下面看一下dilated conv原始论文[4]中的示意图
<p><img src="figure_9.3_4.jpg" alt></p>
<p>  <br>
(a)	图对应3x3的1-dilated conv，和普通的卷积操作一样，(b)图对应3x3的2-dilated conv，实际的卷积kernel size还是3x3，但是空洞为1，也就是对于一个7x7的图像patch，只有9个红色的点和3x3的kernel发生卷积操作，其余的点略过。也可以理解为kernel的size为7x7，但是只有图中的9个点的权重不为0，其余都为0。 可以看到虽然kernel size只有3x3，但是这个卷积的感受野已经增大到了7x7（如果考虑到这个2-dilated conv的前一层是一个1-dilated conv的话，那么每个红点就是1-dilated的卷积输出，所以感受野为3x3，所以1-dilated和2-dilated合起来就能达到7x7的conv）,©图是4-dilated conv操作，同理跟在两个1-dilated和2-dilated conv的后面，能达到15x15的感受野。对比传统的conv操作，3层3x3的卷积加起来，stride为1的话，只能达到(kernel-1) * layer+1=7的感受野，也就是和层数layer成线性关系，而dilated conv的感受野是指数级的增长。<br>
  <br>
dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的sequence信息依赖的问题中，都能很好的应用dilated conv，比如图像分割、语音合成WaveNet、机器翻译ByteNet中。</p>
<h2 id="9-6-RefineNet">9.6 RefineNet</h2>
<p>  <br>
网络结构：<br>
  <br>
RefineNet block的作用就是把不同resolution level的feature map进行融合。网络结构如下：</p>
<p><img src="figure_9.4_1.png" alt><br>
  <br>
最左边一栏就是FCN的encoder部分(文中是用的ResNet)，先把pretrained ResNet按feature map的分辨率分成四个ResNet blocks，然后向右把四个blocks分别作为4个path通过RefineNet block进行融合refine，最后得到一个refined feature map(接softmax再双线性插值输出)。<br>
注意除了RefineNet-4，所有的RefineNet block都是二输入的，用于融合不同level做refine，而单输入的RefineNet-4可以看作是先对ResNet的一个task adaptation。</p>
<p>  <br>
<strong>RefineNet Block</strong><br>
  <br>
接下来仔细看一下RefineNet block，可以看到主要组成部分是Residual convolution unit, Multi-resolution fusion, Chained residual pooling, Output convolutions. 切记这个block作用是融合多个level的feature map输出单个level的feature map，但具体的实现应该是和输入个数、shape无关的。</p>
<p><img src="figure_9.4_2.png" alt></p>
<p>  <br>
Residual convolution unit就是普通的去除了BN的residual unit；</p>
<p>  <br>
Multi-resolution fusion是先对多输入的feature map都用一个卷积层进行adaptation(都化到最小的feature map的shape)，再上采样再做element-wise的相加。注意如果是像RefineNet-4那样的单输入block这一部分就直接pass了；</p>
<p>  <br>
Chained residual pooling中的ReLU对接下来池化的有效性很重要，还可以使模型对学习率的变化没这么敏感。这个链式结构能从很大范围区域上获取背景context。另外，这个结构中大量使用了identity mapping这样的连接，无论长距离或者短距离的，这样的结构允许梯度从一个block直接向其他任一block传播。</p>
<p>  <br>
Output convolutions就是输出前再加一个RCU。</p>
<h2 id="9-7-PSPNet">9.7 PSPNet</h2>
<p>  <br>
场景解析对于无限制的开放词汇和不同场景来说是具有挑战性的.本文使用文中的pyramid pooling module实现基于不同区域的上下文集成，提出了PSPNet，实现利用上下文信息的能力进行场景解析。<br>
  <br>
作者认为，FCN存在的主要问题是没有采取合适的策略来用全局的信息，本文的做法就是借鉴SPPNet来设计了PSPNet解决这个问题。<br>
  <br>
很多State-of-the-art的场景解析框架都是基于FCN的.基于CNN的方法能够增强动态物体的理解，但是在无限制词汇和不同场景中仍然面临挑战.举个例子，如下图.</p>
<p><img src="figure_9.6_1.png" alt><br>
  <br>
FCN认为右侧框中是汽车，但是实际上是船，如果参考上下文的先验知识，就会发现左边是一个船屋，进而推断是框中是船.FCN存在的主要问题就是不能利用好全局的场景线索。</p>
<p>  <br>
对于尤其复杂的场景理解，之前都是采用空间金字塔池化来做的，和之前方法不同（为什么不同，需要参考一下经典的金字塔算法），本文提出了pyramid scene parsing network(PSPNet)。<br>
  <br>
本文的主要贡献如下:<br>
  <br>
(1)	提出了PSPNet在基于FCN的框架中集成困难的上下文特征<br>
  <br>
(2)	通过基于深度监督误差开发了针对ResNet的高效优化策略<br>
  <br>
(3)	构建了一个用于state-of-the-art的场景解析和语义分割的实践系统（具体是什么？）<br>
  <br>
通过观察FCN的结果，发现了如下问题：<br>
  <br>
(1)	关系不匹配（Mismatched Relationship）<br>
  <br>
(2)	易混淆的类别（Confusion Categories）<br>
  <br>
(3)	不显眼的类别（Inconspicuous Classes）<br>
  <br>
总结以上结果发现，以上问题部分或者全部与上下文关系和全局信息有关系，因此本文提出了PSPNet.框架如下:</p>
<p><img src="figure_9.6_2.png" alt><br>
  <br>
并且加入额外的深度监督 Loss</p>
<p><img src="figure_9.6_3.png" alt></p>
<h2 id="9-8-DeepLab系列">9.8 DeepLab系列</h2>
<h3 id="9-8-1-DeepLabv1">9.8.1 DeepLabv1</h3>
<p>  <br>
DeepLab 是结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法。<br>
  <br>
在实验中发现 DCNNs 做语义分割时精准度不够的问题，根本原因是 DCNNs 的高级特征的平移不变性，即高层次特征映射，根源于重复的池化和下采样。<br>
  <br>
针对信号下采样或池化降低分辨率，DeepLab 是采用的 atrous（带孔）算法扩展感受野，获取更多的上下文信息。<br>
  <br>
分类器获取以对象中心的决策是需要空间变换的不变性，这天然地限制了 DCNN 的定位精度，DeepLab 采用完全连接的条件随机场（CRF）提高模型捕获细节的能力。<br>
  <br>
除空洞卷积和 CRFs 之外，论文使用的 tricks 还有 Multi-Scale features。其实就是 U-Net 和 FPN 的思想，在输入图像和前四个最大池化层的输出上附加了两层的 MLP，第一层是 128 个 3×3 卷积，第二层是 128 个 1×1 卷积。最终输出的特征与主干网的最后一层特征图融合，特征图增加 5×128=640 个通道。<br>
  <br>
实验表示多尺度有助于提升预测结果，但是效果不如 CRF 明显。<br>
  <br>
论文模型基于 VGG16，在 Titan GPU 上运行速度达到了 8FPS，全连接 CRF 平均推断需要 0.5s ，在 PASCAL VOC-2012 达到 71.6% IOU accuracy。</p>
<h3 id="9-8-2-DeepLabv2">9.8.2 DeepLabv2</h3>
<p>  <br>
DeepLabv2 是相对于 DeepLabv1 基础上的优化。DeepLabv1 在三个方向努力解决，但是问题依然存在：特征分辨率的降低、物体存在多尺度，DCNN 的平移不变性。<br>
  <br>
因 DCNN 连续池化和下采样造成分辨率降低，DeepLabv2 在最后几个最大池化层中去除下采样，取而代之的是使用空洞卷积，以更高的采样密度计算特征映射。<br>
  <br>
物体存在多尺度的问题，DeepLabv1 中是用多个 MLP 结合多尺度特征解决，虽然可以提供系统的性能，但是增加特征计算量和存储空间。<br>
  <br>
论文受到 Spatial Pyramid Pooling (SPP) 的启发，提出了一个类似的结构，在给定的输入上以不同采样率的空洞卷积并行采样，相当于以多个比例捕捉图像的上下文，称为 ASPP (atrous spatial pyramid pooling) 模块。<br>
  <br>
DCNN 的分类不变形影响空间精度。DeepLabv2 是采样全连接的 CRF 在增强模型捕捉细节的能力。<br>
  <br>
论文模型基于 ResNet，在 NVidia Titan X GPU 上运行速度达到了 8FPS，全连接 CRF 平均推断需要 0.5s ，在耗时方面和 DeepLabv1 无差异，但在 PASCAL VOC-2012 达到 79.7 mIOU。</p>
<h3 id="9-8-3-DeepLabv3">9.8.3 DeepLabv3</h3>
<p>  <br>
好的论文不止说明怎么做，还告诉为什么。DeepLab 延续到 DeepLabv3 系列，依然是在空洞卷积做文章，但是探讨不同结构的方向。<br>
  <br>
DeepLabv3 论文比较了多种捕获多尺度信息的方式：</p>
<p><img src="figure_9.6_4.png" alt></p>
<p>  <br>
1.Image Pyramid：将输入图片放缩成不同比例，分别应用在 DCNN 上，将预测结果融合得到最终输出。<br>
  <br>
2.Encoder-Decoder：利用 Encoder 阶段的多尺度特征，运用到 Decoder 阶段上恢复空间分辨率，代表工作有 FCN、SegNet、PSPNet 等工。<br>
  <br>
3.Deeper w. Atrous Convolution：在原始模型的顶端增加额外的模块，例如 DenseCRF，捕捉像素间长距离信息。<br>
  <br>
4.Spatial Pyramid Pooling：空间金字塔池化具有不同采样率和多种视野的卷积核，能够以多尺度捕捉对象。<br>
  <br>
DeepLabv1-v2 都是使用带孔卷积提取密集特征来进行语义分割。但是为了解决分割对象的多尺度问题，DeepLabv3 设计采用多比例的带孔卷积级联或并行来捕获多尺度背景。<br>
  <br>
此外，DeepLabv3 将修改之前提出的带孔空间金字塔池化模块，该模块用于探索多尺度卷积特征，将全局背景基于图像层次进行编码获得特征，取得 state-of-art 性能，在 PASCAL VOC-2012 达到 86.9 mIOU。</p>
<h3 id="9-8-4-DeepLabv3">9.8.4 DeepLabv3+</h3>
<p>  <br>
语义分割关注的问题:<br>
  <br>
1、 实例对象多尺度问题。<br>
  <br>
2、 因为深度网络存在stride=2的层，会导致feature分辨率下降，从而导致预测精度降低，而造成的边界信息丢失问题。<br>
  <br>
deeplab V3新设计的aspp结构解决了问题1，deeplab v3+主要目的在于解决问题2。<br>
  <br>
问题2 可以使用空洞卷积替代更多的pooling层来获取分辨率更高的feature。但是feature分辨率更高会极大增加运算量。以deeplab v3使用的resnet101为例，stride=16将造成后面9层feature变大，后面9层的计算量变为原来的2*2=4倍大。stride=8则更为恐怖，后面78层的计算量都会变大很多。<br>
  <br>
解决方案：1、编解码器结构；2 Modified Aligned Xception</p>
<p><img src="figure_9.6_5.png" alt></p>
<p>  <br>
在deeplabv3基础上加入解码器。A是aspp结构，其中8x的上采样可以看做是一个解码器。B是编解码结构，它集合了高层和底层的特征。C就是本文采取的结构。<br>
  <br>
方法：<br>
  <br>
（1）Encoder-Decoder with Atrous Convolution</p>
<p><img src="figure_9.6_6.png" alt></p>
<p>  <br>
编码器采用deeplabv3。<br>
  <br>
解码器部分：先从低层级选一个feature，将低层级的feature用1 * 1的卷积进行通道压缩（原本为256通道，或者512通道），目的在于减少低层级的比重。作者认为编码器得到的feature具有更丰富的信息，所以编码器的feature应该有更高的比重。 这样做有利于训练。<br>
  <br>
再将编码器的输出上采样，使其分辨率与低层级feature一致。举个例子，如果采用resnet conv2 输出的feature，则这里要* 4上采样。将两种feature连接后，再进行一次3 * 3的卷积（细化作用），然后再次上采样就得到了像素级的预测。后面的实验结果表明这种结构在 stride=16 时既有很高的精度速度又很快。stride=8相对来说只获得了一点点精度的提升，但增加了很多的计算量。<br>
  <br>
（2）Modified Aligned Xception<br>
  <br>
Xception主要采用了deepwish seperable convolution来替换原来的卷积层。简单的说就是这种结构能在更少参数更少计算量的情况下学到同样的信息。这边则是考虑将原来的resnet-101骨架网换成xception。</p>
<p><img src="figure_9.6_7.png" alt></p>
<p>  <br>
<strong>红色部分为修改</strong><br>
  <br>
更多层：重复8次改为16次（基于MSRA目标检测的工作）。<br>
  <br>
将原来简单的pool层改成了stride为2的deepwish seperable convolution。<br>
  <br>
额外的RELU层和归一化操作添加在每个 3 × 3 depthwise convolution之后（原来只在1 * 1卷积之后）</p>
<h2 id="9-9-Mask-R-CNN">9.9 Mask-R-CNN</h2>
<h3 id="9-9-1-Mask-RCNN-的网络结构示意图">9.9.1 Mask-RCNN 的网络结构示意图</h3>
<p><img src="figure_9.8_1.png" alt></p>
<p>  <br>
其中黑色部分为原来的Faster-RCNN，红色部分为在Faster网络上的修改：<br>
  <br>
1）将ROI Pooling层替换成了ROIAlign；<br>
  <br>
2）添加并列的FCN层（Mask层）；<br>
  <br>
先来概述一下Mask-RCNN的几个特点（来自于Paper<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN</a>的Abstract）：<br>
  <br>
1）在边框识别的基础上添加分支网络，用于语义Mask识别；<br>
  <br>
2）训练简单，相对于Faster仅增加一个小的Overhead，可以跑到5FPS；<br>
  <br>
3）可以方便的扩展到其他任务，比如人的姿态估计等；<br>
  <br>
4）不借助Trick，在每个任务上，效果优于目前所有的 single-model entries；包括 COCO 2016 的Winners。</p>
<h3 id="9-9-2-RCNN行人检测框架">9.9.2 RCNN行人检测框架</h3>
<p>  <br>
来看下后面两种RCNN方法与Mask结合的示意图:</p>
<p><img src="figure_9.8_2.png" alt><br>
  <br>
图中灰色部分是原来的RCNN结合ResNet or FPN的网络，下面黑色部分为新添加的并联Mask层，这个图本身与上面的图也没有什么区别，旨在说明作者所提出的Mask RCNN方法的泛化适应能力：可以和多种RCNN框架结合，表现都不错。</p>
<h3 id="9-9-3-Mask-RCNN-技术要点">9.9.3 Mask-RCNN 技术要点</h3>
<p>  <br>
<strong>1.技术要点1 - 强化的基础网络</strong><br>
  <br>
通过ResNeXt-101+FPN用作特征提取网络，达到state-of-the-art的效果。<br>
  <br>
<strong>2.技术要点2 - ROIAlign</strong><br>
  <br>
采用ROIAlign替代RoiPooling（改进池化操作）。引入了一个插值过程，先通过双线性插值到14<em>14，再pooling到7</em>7，很大程度上解决了仅通过Pooling直接采样带来的Misalignment对齐问题。<br>
  <br>
PS： 虽然 Misalignment 在分类问题上影响并不大，但在 Pixel 级别的 Mask 上会存在较大误差。<br>
  <br>
后面我们把结果对比贴出来（Table2 c &amp; d），能够看到 ROIAlign 带来较大的改进，可以看到，Stride 越大改进越明显。<br>
  <br>
<strong>3.技术要点3 - Loss Function</strong><br>
  <br>
每个ROIAlign对应K * m^2维度的输出。K对应类别个数，即输出K个mask，m对应池化分辨率（7 * 7）。Loss函数定义：</p>
 $$
Lmask(Cls_k)=Sigmoid(Cls_k)
$$ 
<p>  </p>
 $Lmask(Cls_k) = Sigmoid (Cls_k)$ ，平均二值交叉熵 （average binary cross-entropy）Loss，通过逐像素的 Sigmoid 计算得到。  
<p>  <br>
Why K个mask？通过对每个 Class 对应一个Mask可以有效避免类间竞争（其他Class不贡献Loss）。</p>
<p><img src="figure_9.8_3.png" alt><br>
  <br>
通过结果对比来看（Table2 b），也就是作者所说的 Decouple 解耦，要比多分类的Softmax效果好很多。<br>
  <br>
另外，作者给出了很多实验分割效果，就不都列了，只贴一张和FCIS的对比图（FCIS出现了Overlap的问题）</p>
<p><img src="figure_9.8_4.png" alt></p>
<h2 id="9-10-CNN在基于弱监督学习的图像分割中的应用">9.10 CNN在基于弱监督学习的图像分割中的应用</h2>
<p>  <br>
答案来源：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23811946">CNN在基于弱监督学习的图像分割中的应用</a></p>
<p>  <br>
最近基于深度学习的图像分割技术一般依赖于卷积神经网络CNN的训练，训练过程中需要非常大量的标记图像，即一般要求训练图像中都要有精确的分割结果。<br>
  <br>
对于图像分割而言，要得到大量的完整标记过的图像非常困难，比如在ImageNet数据集上，有1400万张图有类别标记，有50万张图给出了bounding box,但是只有4460张图像有像素级别的分割结果。对训练图像中的每个像素做标记非常耗时，特别是对医学图像而言，完成对一个三维的CT或者MRI图像中各组织的标记过程需要数小时。<br>
  <br>
如果学习算法能通过对一些初略标记过的数据集的学习就能完成好的分割结果，那么对训练数据的标记过程就很简单，这可以大大降低花在训练数据标记上的时间。这些初略标记可以是：<br>
  <br>
1、只给出一张图像里面包含哪些物体，<br>
  <br>
2、给出某个物体的边界框，<br>
  <br>
3、对图像中的物体区域做部分像素的标记，例如画一些线条、涂鸦等（scribbles)。</p>
<h3 id="9-10-1-Scribble标记">9.10.1 Scribble标记</h3>
<p>  <br>
论文地址：<a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_ScribbleSup_Scribble-Supervised_Convolutional_CVPR_2016_paper.pdf">ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation (CVPR 2016)</a><br>
  <br>
香港中文大学的Di Lin提出了一个基于Scribble标记的弱监督学习方法。Scribble是一个很方便使用的标记方法，因此被用得比较广泛。如下图，只需要画五条线就能完成对一副图像的标记工作。</p>
<p><img src="figure_9.9_1.png" alt><br>
  <br>
ScribbleSup分为两步，第一步将像素的类别信息从scribbles传播到其他未标记的像素，自动完成所有的训练图像的标记工作； 第二步使用这些标记图像训练CNN。在第一步中，该方法先生成super-pxels, 然后基于graph cut的方法对所有的super-pixel进行标记。</p>
<p><img src="figure_9.9_2.png" alt></p>
<p>  <br>
Graph Cut的能量函数为：</p>
 $$
\sum_{i}\psi _i\left(y_i|X,S\right)+\sum_{i,j}\psi_{ij}\left(y_i,y_j,X\right)
$$ 
<p>  <br>
在这个graph中，每个super-pixel是graph中的一个节点，相接壤的super-pixel之间有一条连接的边。这个能量函数中的一元项包括两种情况，一个是来自于scribble的，一个是来自CNN对该super-pixel预测的概率。整个最优化过程实际上是求graph cut能量函数和CNN参数联合最优值的过程：</p>
 $$
\sum_{i}\psi _i^{scr}\left(y_i|X,S\right)+\sum _i-logP\left(y_i| X,\theta\right)+\sum_{i,j}\psi _{ij}\left(y_i,y_j|X\right)
$$ 
<p>  <br>
上式的最优化是通过交替求  $Y$  和  $\theta$  的最优值来实现的。文章中发现通过三次迭代就能得到比较好的结果。</p>
<p><img src="figure_9.9_3.png" alt></p>
<h3 id="9-10-2-图像级别标记">9.10.2 图像级别标记</h3>
<p>  <br>
论文地址：<a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pathak_Constrained_Convolutional_Neural_ICCV_2015_paper.pdf">Constrained Convolutional Neural Networks for Weakly Supervised Segmentation （ICCV 2015）</a><br>
  <br>
UC Berkeley的Deepak Pathak使用了一个具有图像级别标记的训练数据来做弱监督学习。训练数据中只给出图像中包含某种物体，但是没有其位置信息和所包含的像素信息。该文章的方法将image tags转化为对CNN输出的label分布的限制条件，因此称为 Constrained convolutional neural network (CCNN).</p>
<p><img src="figure_9.9_4.png" alt><br>
  </p>
<p>该方法把训练过程看作是有线性限制条件的最优化过程：</p>
 $$
\underset{\theta ,P}{minimize}\qquad D(P(X)||Q(X|\theta ))\\
subject\to\qquad A\overrightarrow{P} \geqslant \overrightarrow{b},\sum_{X}^{ }P(X)=1
$$ 
<p>  </p>
<p>其中的线性限制条件来自于训练数据上的标记，例如一幅图像中前景类别像素个数期望值的上界或者下界（物体大小）、某个类别的像素个数在某图像中为0，或者至少为1等。该目标函数可以转化为为一个loss function，然后通过SGD进行训练。</p>
<p><img src="figure_9.9_5.png" alt><br>
  <br>
实验中发现单纯使用Image tags作为限制条件得到的分割结果还比较差，在PASCAL VOC 2012 test数据集上得到的mIoU为35.6%，加上物体大小的限制条件后能达到45.1%，如果再使用bounding box做限制，可以达到54%。FCN-8s可以达到62.2%，可见弱监督学习要取得好的结果还是比较难。</p>
<h3 id="9-10-3-DeepLab-bounding-box-image-level-labels">9.10.3 DeepLab+bounding box+image-level labels**</h3>
<p>  <br>
论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1502.02734.pdf">Weakly-and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation</a><br>
  <br>
Google的George Papandreou 和UCLA的Liang-Chieh Chen等在DeepLab的基础上进一步研究了使用bounding box和image-level labels作为标记的训练数据。使用了期望值最大化算法（EM）来估计未标记的像素的类别和CNN的参数。</p>
<p><img src="figure_9.9_6.png" alt><br>
  <br>
对于image-level标记的数据，我们可以观测到图像的像素值和图像级别的标记 ,但是不知道每个像素的标号,因此把 $y$ 当做隐变量。使用如下的概率图模式：</p>
 $$
P\left ( x,y,z;\theta \right ) = P\left ( x \right )\left (\prod_{m=1}^{M} P\left ( y_m|x;\theta \right )\right )P\left ( z|y \right )
$$ 
<p>  <br>
这篇论文是通过EM算法来学习模型的参数 $\theta$ ，具体推导过程可参考原论文。</p>
<p><img src="figure_9.9_7.png" alt><br>
  <br>
对于给出bounding box标记的训练图像，该方法先使用CRF对该训练图像做自动分割，然后在分割的基础上做全监督学习。通过实验发现，单纯使用图像级别的标记得到的分割效果较差，但是使用bounding box的训练数据可以得到较好的结果，在VOC2012 test数据集上得到mIoU 62.2%。另外如果使用少量的全标记图像和大量的弱标记图像进行结合，可以得到与全监督学习(70.3%)接近的分割结果(69.0%)。</p>
<h3 id="9-10-4-统一的框架">9.10.4 统一的框架</h3>
<p>  <br>
论文地址：<a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xu_Learning_to_Segment_2015_CVPR_paper.pdf">Learning to Segment Under Various Forms of Weak Supervision (CVPR 2015)</a></p>
<p>  <br>
Wisconsin-Madison大学的Jia Xu提出了一个统一的框架来处理各种不同类型的弱标记：图像级别的标记、bounding box和部分像素标记如scribbles。该方法把所有的训练图像分成共计 $n$ 个super-pixel，对每个super-pixel提取一个 $d$ 维特征向量。因为不知道每个super-pixel所属的类别，相当于无监督学习，因此该方法对所有的super-pixel做聚类，使用的是最大间隔聚类方法(max-margin clustering, MMC),该过程的最优化目标函数是：</p>
 $$
\underset{W,H}{min} \qquad  \frac{1}{2}tr\left ( W^TW \right ) + \lambda\sum_{p=1}^{n}\sum_{c=1}^{C}\xi \left ( w_c;x_p;h_p^c \right)
$$ 
<p>  <br>
在这个目标函数的基础上，根据不同的弱标记方式，可以给出不同的限制条件，因此该方法就是在相应的限制条件下求最大间隔聚类。</p>
<p><img src="figure_9.9_8.png" alt></p>
<p>  <br>
该方法在Siftflow数据集上得到了比较好的结果，比state-of-the-art的结果提高了10%以上。</p>
<p>  <br>
小结：在弱标记的数据集上训练图像分割算法可以减少对大量全标记数据的依赖，在大多数应用中会更加贴合实际情况。弱标记可以是图像级别的标记、边框和部分像素的标记等。训练的方法一般看做是限制条件下的最优化方法。另外EM算法可以用于CNN参数和像素类别的联合求优。</p>
<h3 id="9-10-5-弱监督分割最新进展（贡献者：明奇-北京理工大学）">9.10.5 弱监督分割最新进展（贡献者：明奇-北京理工大学）</h3>
<ul>
<li><strong>bbox监督</strong></li>
</ul>
<ol>
<li>
<p>Learning to Segment via Cut-and-Paste（ECCV 2018）</p>
<p>利用GAN对抗学习的思想，在cut-paste思想指导下利用bbox弱监督进行实例分割。<br>
<img src="9.10.5-1.png" alt><br>
采用对抗学习的思想，网络主体分为两大部分：mask生成器和合成图像判别器。具体过程为：（1）在图像上截取gt，经过特征提取后预测一个bbox内gt的mask；（2）在原图上随机cut一个背景图像，将bbox内按照生成的mask提取出物体分割结果，然后paste到原图裁剪的背景上去；（3）合成的图像经过判别器进行真假判断。<br>
通过生成器生成更好mask来使得判别器更难判别，在对抗学习中提升两者的性能，逐渐获得更好的结果 .</p>
</li>
<li>
<p>Simple Does It: Weakly Supervised Instance and Semantic Segmentation（CVPR2017）<br>
本文做的是bbox弱监督语义/实例分割任务，能达到全监督分割效果(DeepLabv1)的95%。主要工作为：讨论了使用弱监督语义标签进行迭代训练的方法，以及其限制和不足之处；证明了通过类似GrabCut的算法能通过bbox生成分割训练标签方法的可行性，可以避免像上面的迭代方法重新调整网络训练策略；在VOC数据集上逼近监督学习的分割任务效果。<br>
作者的启发是：将bbox level的mask送入网络训练后得到分割mask的比输入的bbox mask要好（这是很好的insight）。因此启发的操作是：将bbox level标注作为初始mask输入优化，每次得到的标注作为gt进行下一轮的迭代，从而不断获得更好的效果。效果图如下：<br>
<img src="9.10.5-3.png" alt><br>
在此基础上，再加上优化的GrabCut+算法，以及部分区域的筛选，以及BSDS500的边界预测信息整合到一起，能够达到很好的弱监督迭代分割效果。</p>
</li>
</ol>
<ul>
<li><strong>分类监督</strong></li>
</ul>
<ol>
<li>
<p>Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations(CVPR2019)<br>
使用分类标注作为弱监督信息，在CAM提取到特征的基础上，进一步设计IRNet学习额外的特征约束，从而到达更好的弱监督实例分割效果。为了解决CAM应用到实例分割的上述局限，设计IRNet。其组成为两部分：（1）不分类别的实例响应图  （2）pairwise semantic affinitie。其中通过不分类别的实例响应图和CAM结合，约束后得到instance-wise CAMS；另一个分支预先预测物体的边界然后得到pairwise semantic affinitie（关于这个的论文参考Related Work的对应部分，有相应的方法，暂时不深究）进行融合和处理得到最终的分割。整体流程如下：<br>
<img src="9.10.5-2.png" alt></p>
</li>
<li>
<p>Weakly Supervised Instance Segmentation using Class Peak Response（CVPR2018）<br>
本文使用图像级的类别标注监督信息，通过探索类别响应峰值使分类网络能够很好地提取实例分割mask。本工作是使用图像级标注进行弱监督实例分割的首个工作。<br>
在分类监督信息之下，CNN网络会产生一个类别响应图，每个位置是类别置信度分数。其局部极大值往往具有实例很强视觉语义线索。首先将类别峰值响应图的信息进行整合，然后反向传播将其映射到物体实例信息量较大的区域如边界。上述从类别极值响应图产生的映射图称为Peak Response Maps (PRMs)，该图提供了实例物体的详细表征，可以很好地用作分割监督信息。<br>
具体流程如图：<br>
<img src="9.10.5-4.png" alt><br>
首先将图片经过正常的分类网络训练，其中在类别预测响应图上提取出局部响应极值点，进行增强卷积后预测出PRM。然后结合多种信息进行推断生成mask。</p>
</li>
<li>
<p>Weakly Supervised Semantic Segmentation Using Superpixel Pooling Network（AAAI 2017）<br>
本文介绍通过类别标注的标签实现弱监督语义分割的方法。该方法在语义分割mask生成和使用生成mask学习分割生成网络之间反复交替。要实现这种交替迭代学习，关键点就是如何利用类别标注得到较准确的初始分割。为了解决这一问题,提出了Superpixel Pooling Network (SPN)，将输入图像的超像素分割结果作为低阶结构的表征，辅助语义分割的推断。<br>
<img src="9.10.5-5.png" alt><br>
首先是SPN生成初始mask，然后用另一个网络DecoupledNet来学习每个像素的mask标注。其中，该分割网络将语义分割任务解耦为分类和分割两个子任务，并且能够从类别标注中学习形状先验知识用于辅助分割。</p>
</li>
</ol>
<h2 id="9-11-DenseNet（贡献者：黄钦建－华南理工大学）">9.11 DenseNet（贡献者：黄钦建－华南理工大学）</h2>
<p>  <br>
这篇论文是CVPR2017年的最佳论文。</p>
<p>  <br>
卷积神经网络结构的设计主要朝着两个方向发展，一个是更宽的网络（代表：GoogleNet、VGG），一个是更深的网络（代表：ResNet）。但是随着层数的加深会出现一个问题——梯度消失，这将会导致网络停止训练。到目前为止解决这个问题的思路基本都是在前后层之间加一个identity connections(short path)。</p>
<p><img src="9-10-3.png" alt></p>
<p>  <br>
由上图中可知Resnet是做值的相加（也就是add操作），通道数是不变的。而DenseNet是做通道的合并（也就是Concatenation操作），就像Inception那样。从这两个公式就可以看出这两个网络的本质不同。此外DensetNet的前面一层输出也是后面所有层的输入，这也不同于ResNet残差网络。</p>
<p><img src="9-10-1.png" alt></p>
<p>  <br>
DenseNet的Block结构如上图所示。</p>
<p>  <br>
1*1卷积核的目的：减少输入的特征图数量，这样既能降维减少计算量，又能融合各个通道的特征。我们将使用BottleNeck Layers的DenseNet表示为DenseNet-B。(在论文的实验里，将1×1×n小卷积里的n设置为4k，k为每个H产生的特征图数量)</p>
<p><img src="9-10-2.png" alt></p>
<p>  <br>
上图是DenseNet网络的整体网络结构示意图。其中1*1卷积核的目的是进一步压缩参数，并且在Transition Layer层有个参数Reduction（范围是0到1），表示将这些输出缩小到原来的多少倍，默认是0.5，这样传给下一个Dense Block的时候channel数量就会减少一半。当Reduction的值小于1的时候，我们就把带有这种层的网络称为DenseNet-C。</p>
<p>  <br>
DenseNet网络的优点包括：</p>
<ul>
<li>减轻了梯度消失</li>
<li>加强了feature的传递</li>
<li>更有效地利用了feature</li>
<li>一定程度上较少了参数数量</li>
<li>一定程度上减轻了过拟合</li>
</ul>
<h2 id="9-12-图像分割的常用数据集">9.12 图像分割的常用数据集</h2>
<h3 id="9-12-1-PASCAL-VOC">9.12.1 PASCAL VOC</h3>
<p>VOC 数据集分为20类，包括背景为21类，分别如下：</p>
<ul>
<li>Person: person</li>
<li>Animal: bird, cat, cow, dog, horse, sheep</li>
<li>Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train</li>
<li>Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor</li>
</ul>
<p>VOC 数据集中用于分割比赛的图片实例如下，包含原图以及图像分类分割和图像物体分割两种图（PNG格式）。图像分类分割是在20种物体中，ground-turth图片上每个物体的轮廓填充都有一个特定的颜色，一共20种颜色。</p>
<p><img src="VOC-01.png" alt></p>
<h3 id="9-12-2-MS-COCO">9.12.2 MS COCO</h3>
<p>MS COCO 是最大图像分割数据集，提供的类别有 80 类，有超过 33 万张图片，其中 20 万张有标注，整个数据集中个体的数目超过 150 万个。MS COCO是目前难度最大，挑战最高的图像分割数据集。</p>
<p><img src="COCO-01.png" alt></p>
<h3 id="9-12-3-Cityscapes">9.12.3 Cityscapes</h3>
<p>Cityscapes 是驾驶领域进行效果和性能测试的图像分割数据集，它包含了5000张精细标注的图像和20000张粗略标注的图像，这些图像包含50个城市的不同场景、不同背景、不同街景，以及30类涵盖地面、建筑、交通标志、自然、天空、人和车辆等的物体标注。Cityscapes评测集有两项任务：像素级（Pixel-level）图像场景分割（以下简称语义分割）与实例级（Instance-level）图像场景分割（以下简称实例分割）。</p>
<p><img src="Cityscapes-01.png" alt></p>
<h2 id="9-13-全景分割（贡献者：北京理工大学–明奇）">9.13 全景分割（贡献者：北京理工大学–明奇）</h2>
<p>全景分割的开山之作：何恺明的<em>Panoptic Segmentation</em></p>
<ol>
<li><strong>Introduction</strong></li>
</ol>
<p>  语义分割通过带孔全卷积网络，根据不同的stuff进行划分；实例分割则是在目标检测的基础上基于检测框进行物体的分割。缺少一种框架可以将两者进行融合实现既能分割背景又能分割实例，而这在自动驾驶和AR技术中大有作为。由此提出的全景分割任务能将两者进行结合。</p>
<p>  全景分割的思路很直观：为图像的每个像素分配语义label和类内实例id，前者用于区分语义信息，后者用于分割实例（因此stuff不具有实例id）。提出全景分割时，只是启发式地将语意分割和实例分割两种任务的输出进行后处理的融合（如NMS），并以此建立PS任务的baseline。为了评价全景分割的质量，提出panoptic quality (PQ) 标准，将背景和物体的评价纳入一个完整的框架下。示意图如下：<br>
<img src="9.13-1.png" alt></p>
<ol start="2">
<li><strong>Panoptic Segmentation</strong></li>
</ol>
<ul>
<li>
<p><strong>Task format</strong><br>
全景分割的标注方法：<br>
像素级的标注，标出类别label和类内实例id。如果某像素的这两个信息都能匹配，则可以将该像素匹配到某个类别和实例中去；类外的像素可以分配空标签，即并不是所有的像素都要有语义类别。</p>
</li>
<li>
<p><strong>Stuff and thing labels</strong><br>
对于stuff和thing（背景填充和物体实例）的标签，交集是空集，并集是所有可能的label空间。这两者是互相独立不相关的（很好理解，像素属于那个类和它属于哪个物体不具有相关性）。</p>
</li>
<li>
<p><strong>Relationship</strong><br>
都是像素级的label，需要为每个像素分配对应的标签。但是实例分割基于region的，允许重叠的segmentation，而全景分割和语义分割一样是像素级的label，不允许重叠标签的出现。</p>
</li>
<li>
<p><strong>Confidence scores</strong><br>
这一点上更像语义分割而不是实例分割，对于PS不需要置信分数评价分割质量。提到这个，作者认为语义分割和全景分割可以直接利用人工标注的label进行对比从而评价当前mask的质量；而实例分割在选择mask时评价的是分类置信度，这个并没有人工标注进行参考，因此难以把握。</p>
</li>
</ul>
<ol start="3">
<li><strong>Panoptic Segmentation Metric</strong><br>
  用于衡量全景分割效果的指标应具有：完备性；可解释性；简洁性。由是提出了PQ指标，可分为两步：分割匹配、在匹配上进行计算PQ。</li>
</ol>
<p>3.1  <strong>Segment Matching</strong><br>
  定义match：预测的segmentation和gt的iou大于0.5，说明两者can match。再结合全景分割的不可重叠性，不难得到：最多只有一个预测的segmentation可以match gt。</p>
<p>3.2  <strong>PQ Computation</strong><br>
  PQ的计算类似mAP，也是类内求取，然后求类间的平均值，以便不敏感类别不平衡。对于每一类，可以根据gt与预测的segmentation分为三类（下图描述）：<br>
<img src="9.13-2.png" alt></p>
<p>TP: 预测为正，实际为正，描述match较好的<br>
FP: 预测为正，实际为负，描述match错的<br>
FN: 预测为负，实际为正，描述没match出来的gt<br>
  通过上述三类可以计算得到PQ值公式：<br>
<img src="9.13-3.png" alt></p>
<p>式中出去FP与FN后，剩下的式子描述的是match的segmentation的平均IoU，加上FP与FN是为了惩罚match失败的分割实例。<br>
有意思的是，对上述式子进行简单的恒等变化：<br>
<img src="9.13-4.png" alt></p>
<p>第一项评价的是match分割的质量，第二项类似于F1得分。因此可以PQ分解为：</p>
 $$PQ=SQ*RQ$$ 
<ul>
<li>
<p><strong>Void labels</strong><br>
gt中可能出现两种像素标注为空的情况：超出类别的像素和模糊不清的像素（难以分类）。在评估结果时，这些空的标签不予以评估。具体而言：<br>
（1）在matching部分，预测出为void的像素会被移出prediction并不参与IoU计算；<br>
（2）matching后，unmatched prediction按照一般情况会计算FP FN，但是对于空标签情况，如果该prediction含有的void像素块超过一定匹配阈值就会被移除，并不算作FP计算得分。</p>
</li>
<li>
<p><strong>Group labels</strong><br>
有时区分相同语义类别的实例个体标注比较困难，因此有提出组标签的标注方法。但对于PQ计算而言：<br>
（1）matching部分不使用组标签，而是严格区分实例<br>
（2）matching后，对于包含一部分相同类别像素点的unmatched predicted segments，这一部分将被去除并不视作false positives</p>
</li>
</ul>
<p>3.3  <strong>Comparison to Existing Metrics</strong></p>
<ul>
<li>
<p><strong>Semantic segmentation metrics</strong><br>
衡量语义分割的标准有像素级精度，平均精度，IoU。但是其只专注于像素级的划分，不能反映物体实例级别的分割性能。</p>
</li>
<li>
<p><strong>Instance segmentation metrics</strong><br>
度量为AP，主要是引入了置信度分数confidence score对检测目标进行打分。（两者不是完全的隔绝，实例分割也有用IoU监督的，而confidence score是否能够反映mask的真实质量也有存疑过，这个标准也不是固定的）</p>
</li>
<li>
<p><strong>Panoptic quality</strong><br>
PQ的度量可以分解成SQ和RQ，SQ反映了语义分割的像素级IoU性能，RQ专注于检测识别的效果，因此将两者统一到一个框架下。</p>
</li>
</ul>
<p>分割效果：<br>
<img src="9.13-5.png" alt></p>
<br>
<br>
<hr>
TODO
<ul>
<li>[ ] 图像分割数据集标注工具</li>
<li>[ ] 图像分割评价标准</li>
<li>[x] 全景分割</li>
<li>[ ] UNet++</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/deep_learning/ch8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/deep_learning/ch8/" class="post-title-link" itemprop="url">目标检测面试题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-24 10:13:55" itemprop="dateModified" datetime="2024-03-24T10:13:55+08:00">2024-03-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">算法面试</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>目标检测</h1>
<h2 id="8-1-基本概念">8.1 基本概念</h2>
<h3 id="8-1-1-什么是目标检测？">8.1.1 什么是目标检测？</h3>
<p>​	目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标（物体），确定它们的类别和位置，是计算机视觉领域的核心问题之一。由于各类物体有不同的外观、形状和姿态，加上成像时光照、遮挡等因素的干扰，目标检测一直是计算机视觉领域最具有挑战性的问题。</p>
<p>​	计算机视觉中关于图像识别有四大类任务：</p>
<p><strong>分类-Classification</strong>：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。</p>
<p><strong>定位-Location</strong>：解决“在哪里？”的问题，即定位出这个目标的的位置。</p>
<p><strong>检测-Detection</strong>：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。</p>
<p><strong>分割-Segmentation</strong>：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。</p>
<p><img src="8.1.1.png" alt="图像识别四大类任务，图像来源于cs231n 2016课件Lecture 8"></p>
<h3 id="8-1-2-目标检测要解决的核心问题？">8.1.2 目标检测要解决的核心问题？</h3>
<p>除了图像分类之外，目标检测要解决的核心问题是：</p>
<p>1.目标可能出现在图像的任何位置。</p>
<p>2.目标有各种不同的大小。</p>
<p>3.目标可能有各种不同的形状。</p>
<h3 id="8-1-3-目标检测算法分类？">8.1.3 目标检测算法分类？</h3>
<p>基于深度学习的目标检测算法主要分为两类：</p>
<p><strong>1.Two stage目标检测算法</strong></p>
<p>​	先进行区域生成（region proposal，RP）（一个有可能包含待检物体的预选框），再通过卷积神经网络进行样本分类。</p>
<p>​	任务：特征提取—&gt;生成RP—&gt;分类/定位回归。</p>
<p>​	常见的two stage目标检测算法有：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN和R-FCN等。</p>
<p><strong>2.One stage目标检测算法</strong></p>
<p>​	不用RP，直接在网络中提取特征来预测物体分类和位置。</p>
<p>​	任务：特征提取—&gt;分类/定位回归。</p>
<p>​	常见的one stage目标检测算法有：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD和RetinaNet等。</p>
<p><img src="8.1.2.png" alt></p>
<h3 id="8-1-4-目标检测有哪些应用？">8.1.4 目标检测有哪些应用？</h3>
<p>​	目标检测具有巨大的实用价值和应用前景。应用领域包括人脸检测、行人检测、车辆检测、飞机航拍或卫星图像中道路的检测、车载摄像机图像中的障碍物检测、医学影像在的病灶检测等。还有在安防领域中，可以实现比如安全帽、安全带等动态检测，移动侦测、区域入侵检测、物品看护等功能。</p>
<h2 id="8-2-Two-Stage目标检测算法">8.2 Two Stage目标检测算法</h2>
<h3 id="8-2-1-R-CNN">8.2.1 R-CNN</h3>
<p><strong>R-CNN有哪些创新点？</strong></p>
<ol>
<li>使用CNN（ConvNet）对 region proposals 计算 feature vectors。从经验驱动特征（SIFT、HOG）到数据驱动特征（CNN feature map），提高特征对样本的表示能力。</li>
<li>采用大样本下（ILSVRC）有监督预训练和小样本（PASCAL）微调（fine-tuning）的方法解决小样本难以训练甚至过拟合等问题。</li>
</ol>
<p>注：ILSVRC其实就是众所周知的ImageNet的挑战赛，数据量极大；PASCAL数据集（包含目标检测和图像分割等），相对较小。</p>
<p><strong>R-CNN 介绍</strong></p>
<p>​	R-CNN作为R-CNN系列的第一代算法，其实没有过多的使用“深度学习”思想，而是将“深度学习”和传统的“计算机视觉”的知识相结合。比如R-CNN pipeline中的第二步和第四步其实就属于传统的“计算机视觉”技术。使用selective search提取region proposals，使用SVM实现分类。</p>
<p><img src="8.2.1-1.png" alt></p>
<p>原论文中R-CNN pipeline只有4个步骤，光看上图无法深刻理解R-CNN处理机制，下面结合图示补充相应文字</p>
<ol>
<li>
<p>预训练模型。选择一个预训练 （pre-trained）神经网络（如AlexNet、VGG）。</p>
</li>
<li>
<p>重新训练全连接层。使用需要检测的目标重新训练（re-train）最后全连接层（connected layer）。</p>
</li>
<li>
<p>提取 proposals并计算CNN 特征。利用选择性搜索（Selective Search）算法提取所有proposals（大约2000幅images），调整（resize/warp）它们成固定大小，以满足 CNN输入要求（因为全连接层的限制），然后将feature map 保存到本地磁盘。</p>
<p><img src="8.1.4.png" alt></p>
</li>
<li>
<p>训练SVM。利用feature map 训练SVM来对目标和背景进行分类（每个类一个二进制SVM）</p>
</li>
<li>
<p>边界框回归（Bounding boxes Regression）。训练将输出一些校正因子的线性回归分类器</p>
</li>
</ol>
<p><img src="8.1.5.png" alt></p>
<p><strong>R-CNN 实验结果</strong></p>
<p>R-CNN在VOC 2007测试集上mAP达到58.5%，打败当时所有的目标检测算法。</p>
<p><img src="8.1.6.png" alt></p>
<h3 id="8-2-2-Fast-R-CNN">8.2.2 Fast R-CNN</h3>
<p><strong>Fast R-CNN有哪些创新点？</strong></p>
<ol>
<li>只对整幅图像进行一次特征提取，避免R-CNN中的冗余特征提取</li>
<li>用RoI pooling层替换最后一层的max pooling层，同时引入建议框数据，提取相应建议框特征</li>
<li>Fast R-CNN网络末尾采用并行的不同的全连接层，可同时输出分类结果和窗口回归结果，实现了end-to-end的多任务训练【建议框提取除外】，也不需要额外的特征存储空间【R-CNN中的特征需要保持到本地，来供SVM和Bounding-box regression进行训练】</li>
<li>采用SVD对Fast R-CNN网络末尾并行的全连接层进行分解，减少计算复杂度，加快检测速度。</li>
</ol>
<p><strong>Fast R-CNN 介绍</strong></p>
<p>​	Fast R-CNN是基于R-CNN和SPPnets进行的改进。SPPnets，其创新点在于计算整幅图像的the shared feature map，然后根据object proposal在shared feature map上映射到对应的feature vector（就是不用重复计算feature map了）。当然，SPPnets也有缺点：和R-CNN一样，训练是多阶段（multiple-stage pipeline）的，速度还是不够&quot;快&quot;，特征还要保存到本地磁盘中。</p>
<p>将候选区域直接应用于特征图，并使用RoI池化将其转化为固定大小的特征图块。以下是Fast R-CNN的流程图</p>
<p><img src="8.2.2-1.png" alt></p>
<p><strong>RoI Pooling层详解</strong></p>
<p>因为Fast R-CNN使用全连接层，所以应用RoI Pooling将不同大小的ROI转换为固定大小。</p>
<p>RoI Pooling 是Pooling层的一种，而且是针对RoI的Pooling，其特点是输入特征图尺寸不固定，但是输出特征图尺寸固定（如7x7）。</p>
<p><strong>什么是RoI呢？</strong></p>
<p>RoI是Region of Interest的简写，一般是指图像上的区域框，但这里指的是由Selective Search提取的候选框。</p>
<p><img src="8.2.2-2.png" alt></p>
<p>往往经过RPN后输出的不止一个矩形框，所以这里我们是对多个RoI进行Pooling。</p>
<p><strong>RoI Pooling的输入</strong></p>
<p>输入有两部分组成：</p>
<ol>
<li>特征图（feature map）：指的是上面所示的特征图，在Fast RCNN中，它位于RoI Pooling之前，在Faster RCNN中，它是与RPN共享那个特征图，通常我们常常称之为“share_conv”；</li>
<li>RoIs，其表示所有RoI的N*5的矩阵。其中N表示RoI的数量，第一列表示图像index，其余四列表示其余的左上角和右下角坐标。</li>
</ol>
<p>在Fast RCNN中，指的是Selective Search的输出；在Faster RCNN中指的是RPN的输出，一堆矩形候选框，形状为1x5x1x1（4个坐标+索引index），其中值得注意的是：坐标的参考系不是针对feature map这张图的，而是针对原图的（神经网络最开始的输入）。其实关于ROI的坐标理解一直很混乱，到底是根据谁的坐标来。其实很好理解，我们已知原图的大小和由Selective Search算法提取的候选框坐标，那么根据&quot;映射关系&quot;可以得出特征图（featurwe map）的大小和候选框在feature map上的映射坐标。至于如何计算，其实就是比值问题，下面会介绍。所以这里把ROI理解为原图上各个候选框（region proposals），也是可以的。</p>
<p>注：说句题外话，由Selective Search算法提取的一系列可能含有object的bounding box，这些通常称为region proposals或者region of interest（ROI）。</p>
<p><strong>RoI的具体操作</strong></p>
<ol>
<li>
<p>根据输入image，将ROI映射到feature map对应位置</p>
<p>注：映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”，得到了feature map上的box坐标</p>
</li>
<li>
<p>将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）</p>
</li>
<li>
<p>对每个sections进行max pooling操作</p>
</li>
</ol>
<p>这样我们就可以从不同大小的方框得到固定大小的相应 的feature maps。值得一提的是，输出的feature maps的大小不取决于ROI和卷积feature maps大小。RoI Pooling 最大的好处就在于极大地提高了处理速度。</p>
<p><strong>RoI Pooling的输出</strong></p>
<p>输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel * w * h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w * h）的矩形框。</p>
<p><strong>RoI Pooling示例</strong></p>
<p><img src="8.1.11.gif" alt></p>
<h3 id="8-2-3-Faster-R-CNN">8.2.3 Faster R-CNN</h3>
<p><strong>Faster R-CNN有哪些创新点？</strong></p>
<p>Fast R-CNN依赖于外部候选区域方法，如选择性搜索。但这些算法在CPU上运行且速度很慢。在测试中，Fast R-CNN需要2.3秒来进行预测，其中2秒用于生成2000个ROI。Faster R-CNN采用与Fast R-CNN相同的设计，只是它用内部深层网络代替了候选区域方法。新的候选区域网络（RPN）在生成ROI时效率更高，并且以每幅图像10毫秒的速度运行。<br>
<img src="8.2.3-1.png" alt></p>
<p>图8.1.13 Faster R-CNN的流程图<br>
Faster R-CNN的流程图与Fast R-CNN相同，采用外部候选区域方法代替了内部深层网络。<br>
<img src="8.2.3-2.png" alt></p>
<p>图8.1.14<br>
<strong>候选区域网络</strong></p>
<p>候选区域网络（RPN）将第一个卷积网络的输出特征图作为输入。它在特征图上滑动一个3×3的卷积核，以使用卷积网络（如下所示的ZF网络）构建与类别无关的候选区域。其他深度网络（如VGG或ResNet）可用于更全面的特征提取，但这需要以速度为代价。ZF网络最后会输出256个值，它们将馈送到两个独立的全连接层，以预测边界框和两个objectness分数，这两个objectness分数度量了边界框是否包含目标。我们其实可以使用回归器计算单个objectness分数，但为简洁起见，Faster R-CNN使用只有两个类别的分类器：即带有目标的类别和不带有目标的类别。<br>
<img src="8.2.3-3.png" alt></p>
<p>图8.1.15<br>
对于特征图中的每一个位置，RPN会做k次预测。因此，RPN将输出4×k个坐标和每个位置上2×k个得分。下图展示了8×8的特征图，且有一个3×3的卷积核执行运算，它最后输出8×8×3个ROI（其中k=3）。下图（右）展示了单个位置的3个候选区域。<br>
<img src="8.2.3-4.png" alt></p>
<p>图8.1.16<br>
假设最好涵盖不同的形状和大小。因此，Faster R-CNN不会创建随机边界框。相反，它会预测一些与左上角名为锚点的参考框相关的偏移量（如x, y）。我们限制这些偏移量的值，因此我们的猜想仍然类似于锚点。<br>
<img src="8.1.17.png" alt></p>
<p>图8.1.17<br>
要对每个位置进行k个预测，我们需要以每个位置为中心的k个锚点。每个预测与特定锚点相关联，但不同位置共享相同形状的锚点。<br>
<img src="8.2.3-6.png" alt></p>
<p>图8.1.18<br>
这些锚点是精心挑选的，因此它们是多样的，且覆盖具有不同比例和宽高比的现实目标。这使得我们可以用更好的猜想来指导初始训练，并允许每个预测专门用于特定的形状。该策略使早期训练更加稳定和简便。<br>
<img src="8.2.3-7.png" alt></p>
<p>图8.1.19<br>
Faster R-CNN使用更多的锚点。它部署9个锚点框：3个不同宽高比的3个不同大小的锚点（Anchor）框。每一个位置使用9个锚点，每个位置会生成2×9个objectness分数和4×9个坐标。</p>
<h3 id="8-2-4-R-FCN">8.2.4 R-FCN</h3>
<p><strong>R-FCN有哪些创新点？</strong></p>
<p>R-FCN 仍属于two-stage 目标检测算法：RPN+R-FCN</p>
<ol>
<li>Fully convolutional</li>
<li>位置敏感得分图（position-sentive score maps）</li>
</ol>
<blockquote>
<p>our region-based detector is <strong>fully convolutional</strong> with almost all computation shared on the entire image. To achieve this goal, we propose <strong>position-sensitive score maps</strong> to address a dilemma between translation-invariance in image classification and translation-variance in object detection.</p>
</blockquote>
<p>R-FCN backbone：ResNet</p>
<p>ResNet-101+R-FCN：83.6% in PASCAL VOC 2007 test datasets</p>
<p>既提高了mAP，又加快了检测速度</p>
<pre><code>假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。现在我们回顾一下所有问题。在Faster R-CNN中，检测器使用了多个全连接层进行预测。如果有2000个ROI，那么成本非常高。R-FCN通过减少每个ROI所需的工作量实现加速。上面基于区域的特征图与ROI是独立的，可以在每个ROI之外单独计算。剩下的工作就比较简单了，因此R-FCN的速度比Faster R-CNN快。
</code></pre>
<p><img src="8.2.4-1.png" alt></p>
<pre><code>图8.2.1 人脸检测
现在我们来看一下5×5的特征图M，内部包含一个蓝色方块。我们将方块平均分成3×3个区域。现在，我们在M中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有黄色的网格单元[2,2]处于激活状态。在左侧创建一个新的特征图，用于检测目标的左上角。
</code></pre>
<p><img src="8.2.4-2.png" alt></p>
<pre><code>图8.2.2 检测示例
我们将方块分成9个部分，由此创建了9个特征图，每个用来检测对应的目标区域。这些特征图叫做位置敏感得分图（position-sensitive score map），因为每个图检测目标的子区域（计算其得分）。
</code></pre>
<p><img src="8.2.4-3.png" alt></p>
<pre><code>图8.2.3生成9个得分图
下图中红色虚线矩形是建议的ROI。我们将其分割成3×3个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角ROI区域包含左眼的概率。我们将结果存储成3×3 vote数组，如下图（右）所示。例如，vote_array[0][0]包含左上角区域是否包含目标对应部分的得分。
</code></pre>
<p><img src="8.2.4-4.png" alt></p>
<pre><code>图8.2.4
将ROI应用到特征图上，输出一个3x3数组。将得分图和ROI映射到vote数组的过程叫做位置敏感ROI池化（position-sensitive ROI-pool）。该过程与前面讨论过的ROI池化非常接近。
</code></pre>
<p><img src="8.2.4-5.png" alt></p>
<pre><code>图8.2.5
将ROI的一部分叠加到对应的得分图上，计算V[i][j]。在计算出位置敏感ROI池化的所有值后，类别得分是其所有元素得分的平均值。
</code></pre>
<p><img src="8.2.6.png" alt></p>
<pre><code>图8.2.6 ROI池化
假如我们有C个类别要检测。我们将其扩展为C+1个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有3×3个得分图，因此一共有(C+1)×3×3个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。以下是数据流图，在本案例中，k=3。
</code></pre>
<p><img src="8.2.7.png" alt></p>
<pre><code>图8.2.7
</code></pre>
<h3 id="8-2-5-FPN">8.2.5 FPN</h3>
<p><strong>FPN有哪些创新点？</strong></p>
<ol>
<li>多层特征</li>
<li>特征融合</li>
</ol>
<p>解决目标检测中的多尺度问题，通过简单的网络连接改变，在基本不增加原有模型计算量的情况下，大幅度提升小物体（small object）检测的性能。</p>
<p>在物体检测里面，有限计算量情况下，网络的深度（对应到感受野）与 stride 通常是一对矛盾的东西，常用的网络结构对应的 stride 一般会比较大（如 32），而图像中的小物体甚至会小于 stride 的大小，造成的结果就是小物体的检测性能急剧下降。传统解决这个问题的思路包括：</p>
<ol>
<li>图像金字塔（image pyramid），即多尺度训练和测试。但该方法计算量大，耗时较久。</li>
<li>特征分层，即每层分别预测对应的scale分辨率的检测结果，如SSD算法。该方法强行让不同层学习同样的语义信息，但实际上不同深度对应于不同层次的语义特征，浅层网络分辨率高，学到更多是细节特征，深层网络分辨率低，学到更多是语义特征。</li>
</ol>
<p>因而，目前多尺度的物体检测主要面临的挑战为：</p>
<ol>
<li>如何学习具有强语义信息的多尺度特征表示？</li>
<li>如何设计通用的特征表示来解决物体检测中的多个子问题？如 object proposal, box localization, instance segmentation.</li>
<li>如何高效计算多尺度的特征表示？</li>
</ol>
<p>FPN网络直接在Faster R-CNN单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义（rich semantic）特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。作者接下来实验了将 FPN 应用在 Faster RCNN 上的性能，在 COCO 上达到了 state-of-the-art 的单模型精度。在RPN上，FPN增加了8.0个点的平均召回率（average recall，AR）；在后面目标检测上，对于COCO数据集，FPN增加了2.3个点的平均精确率（average precision，AP），对于VOC数据集，FPN增加了3.8个点的AP。</p>
<p><img src="FPN-01.png" alt></p>
<p>FPN算法主要由三个模块组成，分别是：</p>
<ol>
<li>Bottom-up pathway（自底向上线路）</li>
<li>Lareral connections（横向链接）</li>
<li>Top-down path（自顶向下线路）</li>
</ol>
<p><img src="FPN-02.png" alt></p>
<p><strong>Bottom-up pathway</strong></p>
<p>FPN是基于Faster R-CNN进行改进，其backbone是ResNet-101，FPN主要应用在Faster R-CNN中的RPN（用于bouding box proposal generation）和Fast R-CNN（用于object detection）两个模块中。</p>
<p>其中 RPN 和 Fast RCNN 分别关注的是召回率（recall）和精确率（precision），在这里对比的指标分别为 Average Recall(AR) 和 Average Precision(AP)。</p>
<p>注：Bottom-up可以理解为自底向上，Top-down可以理解为自顶向下。这里的下是指low-level，上是指high-level，分别对应于提取的低级（浅层）特征和高级语义（高层）特征。</p>
<p>Bottom-up pathway 是卷积网络的前向传播过程。在前向传播过程中，feature map的大小可以在某些层发生改变。一些尺度（scale）因子为2，所以后一层feature map的大小是前一层feature map大小的二分之一，根据此关系进而构成了feature pyramid（hierarchy）。</p>
<p>然而还有很多层输出的feature map是一样的大小（即不进行缩放的卷积），作者将这些层归为同一 stage。对于feature pyramid，作者为每个stage定义一个pyramid level。</p>
<p>作者将每个stage的最后一层的输出作为feature map，然后不同stage进行同一操作，便构成了feature pyramid。</p>
<p>具体来说，对于ResNets-101，作者使用了每个stage的最后一个残差结构的特征激活输出。将这些残差模块输出表示为{C2, C3, C4, C5}，对应于conv2，conv3，conv4和conv5的输出，并且注意它们相对于输入图像具有{4, 8, 16, 32}像素的步长。考虑到内存占用，没有将conv1包含在金字塔中。</p>
<p><img src="FPN-03.png" alt></p>
<p><strong>Top-down pathway and lateral connections</strong></p>
<p>Top-town pathway是上采样（upsampling）过程。而later connection（横向连接）是将上采样的结果和bottom-up pathway生成的相同大小的feature map进行融合（merge）。</p>
<p>注：上采样尺度因子为2，因为为了和之前下采样卷积的尺度因子=2一样。上采样是放大，下采样是缩小。</p>
<p>具体操作如下图所示，上采样（2x up）feature map与相同大小的bottom-up feature map进行逐像素相加融合（element-wise addition），其中bottom-up feature先要经过1x1卷积层，目的是为了减少通道维度（reduce channel dimensions）。</p>
<p>注：减少通道维度是为了将bottom-up feature map的通道数量与top-down feature map的通道数量保持一致，又因为两者feature map大小一致，所以可以进行对应位置像素的叠加（element-wise addition）。</p>
<p><img src="FPN-04.png" alt></p>
<h3 id="8-2-6-Mask-R-CNN">8.2.6 Mask R-CNN</h3>
<p><strong>Mask R-CNN有哪些创新点？</strong></p>
<ol>
<li>Backbone：ResNeXt-101+FPN</li>
<li>RoI Align替换RoI Pooling</li>
</ol>
<p>Mask R-CNN是一个实例分割（Instance segmentation）算法，主要是在目标检测的基础上再进行分割。Mask R-CNN算法主要是Faster R-CNN+FCN，更具体一点就是ResNeXt+RPN+RoI Align+Fast R-CNN+FCN。</p>
<p>![](ch8/Mask R-CNN-01.png)</p>
<p><strong>Mask R-CNN算法步骤</strong></p>
<ol>
<li>输入一幅你想处理的图片，然后进行对应的预处理操作，或者预处理后的图片；</li>
<li>将其输入到一个预训练好的神经网络中（ResNeXt等）获得对应的feature map；</li>
<li>对这个feature map中的每一点设定预定个的RoI，从而获得多个候选RoI；</li>
<li>将这些候选的RoI送入RPN网络进行二值分类（前景或背景）和BB回归，过滤掉一部分候选的RoI；</li>
<li>对这些剩下的RoI进行RoI Align操作（即先将原图和feature map的pixel对应起来，然后将feature map和固定的feature对应起来）；</li>
<li>对这些RoI进行分类（N类别分类）、BB回归和MASK生成（在每一个RoI里面进行FCN操作）。</li>
</ol>
<p><strong>RoI Pooling和RoI Align有哪些不同？</strong></p>
<p>ROI Align 是在Mask-RCNN中提出的一种区域特征聚集方式，很好地解决了RoI Pooling操作中两次量化造成的区域不匹配(mis-alignment)的问题。实验显示，在检测测任务中将 RoI Pooling 替换为 RoI Align 可以提升检测模型的准确性。</p>
<p>在常见的两级检测框架（比如Fast-RCNN，Faster-RCNN，RFCN）中，RoI Pooling 的作用是根据预选框的位置坐标在特征图中将相应区域池化为固定尺寸的特征图，以便进行后续的分类和包围框回归操作。由于预选框的位置通常是由模型回归得到的，一般来讲是浮点数，而池化后的特征图要求尺寸固定。故RoI Pooling这一操作存在两次量化的过程。</p>
<ul>
<li>将候选框边界量化为整数点坐标值。</li>
<li>将量化后的边界区域平均分割成  $k\times k$  个单元(bin),对每一个单元的边界进行量化。</li>
</ul>
<p>事实上，经过上述两次量化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度。在论文里，作者把它总结为“不匹配问题（misalignment）”。</p>
<p>下面我们用直观的例子具体分析一下上述区域不匹配问题。如下图所示，这是一个Faster-RCNN检测框架。输入一张 $800\times 800$ 的图片，图片上有一个 $665\times 665$ 的包围框（框着一只狗）。图片经过主干网络提取特征后，特征图缩放步长（stride）为32。因此，图像和包围框的边长都是输入时的1/32。800正好可以被32整除变为25。但665除以32以后得到20.78，带有小数，于是RoI Pooling 直接将它量化成20。接下来需要把框内的特征池化 $7\times 7$ 的大小，因此将上述包围框平均分割成 $7\times 7$ 个矩形区域。显然，每个矩形区域的边长为2.86，又含有小数。于是ROI Pooling 再次把它量化到2。经过这两次量化，候选区域已经出现了较明显的偏差（如图中绿色部分所示）。更重要的是，该层特征图上0.1个像素的偏差，缩放到原图就是3.2个像素。那么0.8的偏差，在原图上就是接近30个像素点的差别，这一差别不容小觑。</p>
<p>![](ch8/Mask R-CNN-02.png)</p>
<p>为了解决RoI Pooling的上述缺点，作者提出了RoI Align这一改进的方法(如图2)。</p>
<p>![](ch8/Mask R-CNN-03.png)</p>
<p>RoI Align的思路很简单：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值，从而将整个特征聚集过程转化为一个连续的操作。值得注意的是，在具体的算法操作上，RoI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，如下图所示：</p>
<ol>
<li>
<p>遍历每一个候选区域，保持浮点数边界不做量化。</p>
</li>
<li>
<p>将候选区域分割成 $k\times k$ 个单元，每个单元的边界也不做量化。</p>
</li>
<li>
<p>在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。</p>
</li>
</ol>
<p>这里对上述步骤的第三点作一些说明：这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。在相关实验中，作者发现将采样点设为4会获得最佳性能，甚至直接设为1在性能上也相差无几。事实上，RoI Align 在遍历取样点的数量上没有RoI Pooling那么多，但却可以获得更好的性能，这主要归功于解决了mis alignment的问题。值得一提的是，我在实验时发现，RoI Align在VOC 2007数据集上的提升效果并不如在COCO上明显。经过分析，造成这种区别的原因是COCO上小目标的数量更多，而小目标受mis alignment问题的影响更大（比如，同样是0.5个像素点的偏差，对于较大的目标而言显得微不足道，但是对于小目标，误差的影响就要高很多）。</p>
<p>![](ch8/Mask R-CNN-04.png)</p>
<h3 id="8-2-7-DetNet（贡献者：北京理工大学–明奇）">8.2.7  DetNet（贡献者：北京理工大学–明奇）</h3>
<p>DetNet是发表在ECCV2018的论文，比较新，出发点是现有的检测任务backbone都是从分类任务衍生而来的，因此作者想针对检测专用的backbone做一些讨论和研究而设计了DetNet，思路比较新奇。</p>
<ol>
<li><strong>Introduction</strong><br>
  很多backbone的提出都是用于挑战ImageNet分类任务后被应用到检测上来，而鲜有单独<u>针对检测任务设计的backbone</u>。</li>
</ol>
<p>  <strong>检测和分类有明显的区别</strong>：（1）不仅需要分类，还需要精确的定位 （2）最近的检测器都是基于类似FPN结构，在分类网络基础上加额外多尺度特征进行检测，应对不同尺度变化的目标。这两点又是相互补充，共同协助网络完成分类到检测任务的转变。例如分类任务是检测的一环所以必不可少，但是传统分类采用的最高级特征定位细节不够，因此很多最近网络设法用类似FPN的结构去处理尺度变化的问题，就将分类较好地过渡到检测任务上了。</p>
<ol start="2">
<li><strong>DetNet</strong></li>
</ol>
<p>2.1 <strong>Motivation</strong><br>
  主要着眼点是<strong>分辨率</strong>，从大目标和小目标分别阐述保持分辨率的重要性。所以DetNet也是从分辨率的保持着手，解决多尺度物体的识别问题。</p>
<ul>
<li>
<p>Weak visibility of large objects<br>
  网络在较深层如P6（FPN）P7（RetinaNet）大目标的边界不明确使精确定位困难。</p>
</li>
<li>
<p>Invisibility of small objects<br>
  小目标就很惨了，降采样容易丢。这个就不赘述了，所以只要避开降采样就能防止目标丢失，但是这种方法又会导致抽象能力不够</p>
</li>
</ul>
<p>​    2.2  <strong>DetNet Design</strong><br>
  保持分辨率有两个麻烦的问题：（1）内存消耗大，计算大 （2）降采样减少导致高层的抽象特征不足以很好地进行分类任务。下面设计时会同时考虑时间和高层抽象信息两点。<br>
  先放出DetNet的多尺度各stage的尺寸如下图， 可以看到，相比前两种方式，DetNet在P4之后就不再进一步降采样了，进行分辨率的保持。</p>
<p><img src="DetNet-1.png" alt></p>
<p>  实现细节如下图：</p>
<p><img src="DetNet-2.png" alt></p>
<ul>
<li>采用的backbone是ResNet-50，改进设计了DetNet-59。</li>
<li>对bottlenecks进行了改进，传统的其实不止C，也包含两种，即将AB的膨胀卷积换成普通卷积。AB是新的基础模块。</li>
<li>为了减少分辨率保持带来的时间和内存成本消耗，通道数固定为256（思考：降采样和膨胀卷积都会有信息丢失，这里可以想想）。</li>
<li>DetNet也可以加FPN结构，方法类似。</li>
</ul>
<ol start="3">
<li><strong>Experiments</strong><br>
  检测和训练的细节配置就不看了。</li>
</ol>
<p>3.1 <strong>Main Results</strong></p>
<p><img src="DetNet-3.png" alt></p>
<ul>
<li>在FPN基础上明显有大物体涨点，同时由于高分辨率，小物体也有不错的提升。</li>
<li>膨胀卷积提供的大感受野使得分类也不逊色<br>
<img src="DetNet-4.png" alt></li>
</ul>
<p>​	3.2  <strong>Results analysis</strong><br>
<img src="DetNet-5.png" alt></p>
<ul>
<li>
<p>从AP50看出，高好1.7；从AP80看出，高了3.7。由此可以看出确实提高了检测性能。（</p>
</li>
<li>
<p>从定位性能来看，大物体的提升比小物体更多。作者认为是高分辨率解决了大物体边界模糊的问题。其实有一种解释：小目标没有大目标明显，因为膨胀卷积核降采样都会丢失小目标，只是膨胀卷积可能离散采样不至于像降采样直接给到后面没了，但是没有根本性的解决，所以小目标不大。<br>
<img src="DetNet-6.png" alt></p>
</li>
<li>
<p>AR指标也有类似结论</p>
</li>
<li>
<p>AR50体现了小目标的查全率更好，这也印证上面分析的：相对降采样，膨胀卷积丢失会好点。此下大目标效果虽然提升不大但是也很高了，作者表示DetNet擅长找到更精确的定位目标，在AR85的高指标就能看出。</p>
</li>
<li>
<p>AR85看大目标丢失少，说明能够像 VGG一样对大目标效果优良。关于小目标的效果平平，作者认为没有必要太高，因为FPN结构对小目标已经利用地很充分了，这里即使不高也没事。</p>
</li>
</ul>
<p>3.3 <strong>Discussion</strong></p>
<ul>
<li>关于stage<br>
  为了研究backbone对检测的影响，首先研究stage的作用。前4个还好说，和ResNet一样，但是P5 P6就不同，没有尺度的变化，和传统意义的stage不一样了，需要重新定义。这里DetNet也是类似ResNet的方法，虽然没有尺度变化，但是AB模块的位置还是保持了，B开启一个stage（<s>听上去有点牵强</s>）。如下图，认为新加的仍属于P5。<br>
<img src="DetNet-7.png" alt></li>
</ul>
<p>  验证方法是做了实验，将P6开始的block换成上图所示的A模块对比效果如下图。 发现还是加了B效果更好。（但是这个stage和传统意义很不一样，所以很多性质不能相提并论，只是B模块的改变也不好判定什么）<br>
<img src="DetNet-8.png" alt></p>
<h3 id="8-2-8-CBNet">8.2.8  CBNet</h3>
<p>本部分介绍一篇在COCO数据集达到最高单模型性能——mAP 53.3的网络，论文于2019.9.3发布在ArXiv，全名是<em>CBNet: A Novel Composite Backbone Network Architecture for Object Detection</em></p>
<ol>
<li><strong>Introduction</strong></li>
</ol>
<p>  名义上是单模型，实际是多模型的特征融合，只是和真正的多模型策略略有不同。作者的起点是，设计新的模型往往需要在ImageNet上进行预训练，比较麻烦。因而提出的Composite Backbone Network (CBNet)，采用经典网络的多重组合的方式构建网络，一方面可以提取到更有效的特征，另一方面也能够直接用现成的预训练参数（如ResNet，ResNeXt等）比较简单高效。</p>
<ol start="2">
<li><strong>Proposed method</strong><br>
<img src="CBNet-1.png" alt><br>
2.1  <strong>Architecture of CBNet</strong><br>
<img src="CBNet-2.png" alt></li>
</ol>
<p>  如上图，模型中采用K个（K&gt;1）相同的结构进行紧密联结。其中两个相同backbone的叫Dual-Backbone (DB)，三个叫Triple- Backbone (TB)；L代表backbone的stage数目，这里统一设置为L=5。其中，和前任工作不同的地方在于，这里将不同的stage信息进行复用回传，以便获取更好的特征（为什么work不好说）。</p>
<p>2.2  <strong>Other possible composite styles</strong><br>
<img src="CBNet-3.png" alt></p>
<p>  相关工作的其他类似结构，大同小异。要么是前面backbone的stage往后传播，要么是往前一个传播，每个都有一篇论文，应该都会给出不同的解释；第四个结构不太一样，是类似densnet的结构，但是密集连接+多backbone assemble的内存消耗不出意外会非常大。但是脱离这些体系来看，多backbone的结构类似多模型的assemble，和单模型有点不公平。</p>
<ol start="3">
<li><strong>Experiment</strong></li>
</ol>
<ul>
<li><strong>result</strong><br>
<img src="CBNet-4.png" alt></li>
</ul>
<p>COCO数据集上的结果。看来提升还是有的。但是也能看出，大趋势上，三阶级联效果不如两阶的提升大，也是这部分的特征提升空间有限的缘故，到底哪部分在work不好说。下图的研究就更说明这一点了，斜率逐渐减小。</p>
<ul>
<li><strong>Comparisons of different composite styles</strong><br>
<img src="CBNet-5.png" alt></li>
</ul>
<p>他的级联网络相比，作者的阐述点只落脚于特征的利用情况，但是这个东西本身就很玄乎，不好说到底怎么算利用得好。硬要说这种做法的解释性，大概就是将backbone方向的后面高级语义特征传播回前面进行加强，相当于横向的FPN传播。</p>
<ul>
<li><strong>Number of backbones in CBNet</strong><br>
<img src="CBNet-6.png" alt></li>
</ul>
<p>速度慢是必然的，FPN+ResNeXt为8fps，加上两个backboen后为5.5FPS；如果减去backbone的前两个stage，可以节省部分参数达到6.9FPS，而精度下降不大（整体速度太低，这个实验意义不大）</p>
<ul>
<li><strong>Sharing weights for CBNet</strong><br>
<img src="CBNet-7.png" alt></li>
<li></li>
</ul>
<p>从中可以看出其实权重是否share区别不大， 不到一个点的降幅，参数量减少。</p>
<ul>
<li><strong>Effectiveness of basic feature enhancement by CBNet</strong><br>
<img src="CBNet-8.png" alt></li>
</ul>
<p>从中可以看出激活响应效果更好，确实是能够提取到更为有效的特征，对物体的响应更加敏感。</p>
<h2 id="8-3-One-Stage目标检测算法">8.3 One Stage目标检测算法</h2>
<p>我们将对单次目标检测器（包括SSD系列和YOLO系列等算法）进行综述。我们将分析FPN以理解多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。然后我们将分析Focal loss和RetinaNet，看看它们是如何解决训练过程中的类别不平衡问题的。</p>
<h3 id="8-3-1-SSD">8.3.1 SSD</h3>
<p><strong>SSD有哪些创新点？</strong></p>
<ol>
<li>基于Faster R-CNN中的Anchor，提出了相似的先验框（Prior box）</li>
<li>从不同比例的特征图（多尺度特征）中产生不同比例的预测，并明确地按长宽比分离预测。</li>
</ol>
<p>不同于前面的R-CNN系列，SSD属于one-stage方法。SSD使用 VGG16 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样），将后面的全连接层替换成卷积层，并在之后添加自定义卷积层，并在最后直接采用卷积进行检测。在多个特征图上设置不同缩放比例和不同宽高比的先验框以融合多尺度特征图进行检测，靠前的大尺度特征图可以捕捉到小物体的信息，而靠后的小尺度特征图能捕捉到大物体的信息，从而提高检测的准确性和定位的准确性。如下图是SSD的网络结构图。</p>
<p><img src="SSD-01.png" alt></p>
<p><strong>1. 怎样设置default boxes？</strong><br>
SSD中default box的概念有点类似于Faster R-CNN中的anchor。不同于Faster R-CNN只在最后一个特征层取anchor, SSD在多个特征层上取default box，可以得到不同尺度的default box。在特征图的每个单元上取不同宽高比的default box,一般宽高比在{1,2,3,1/2,1/3}中选取，有时还会额外增加一个宽高比为1但具有特殊尺度的box。如下图所示，在8x8的feature map和4x4的feature map上的每个单元取4个不同的default box。原文对于300x300的输入，分别在conv4_3, conv7,conv8_2,conv9_2,conv10_2,conv11_2的特征图上的每个单元取4,6,6,6,4,4个default box. 由于以上特征图的大小分别是38x38,19x19,10x10,5x5,3x3,1x1，所以一共得到38x38x4+19x19x6+10x10x6+5x5x6+<br>
3x3x4+1x1x4=8732个default box.对一张300x300的图片输入网络将会针对这8732个default box预测8732个边界框。</p>
<p><img src="SSD-02.png" alt></p>
<p><strong>2. 怎样对先验框进行匹配？</strong><br>
SSD在训练的时候只需要输入图像和图像中每个目标对应的ground truth. 先验框与ground truth 的匹配遵循两个原则：</p>
<p>（1）对图片中的每个ground truth, 在先验框中找到与其IOU最大的先验框，则该先验框对应的预测边界框与ground truth 匹配。</p>
<p>（2）对于（1）中每个剩下的没有与任何ground truth匹配到的先验框，找到与其IOU最大的ground truth，若其与该ground truth的IOU值大于某个阈值（一般设为0.5），则该先验框对应的预测边界框与该ground truth匹配。</p>
<p>按照这两个原则进行匹配，匹配到ground truth的先验框对应的预测边界框作为正样本，没有匹配到ground truth的先验框对应的预测边界框作为负样本。尽管一个ground truth可以与多个先验框匹配，但是ground truth的数量相对先验框还是很少，按照上面的原则进行匹配还是会造成负样本远多于正样本的情况。为了使正负样本尽量均衡（一般保证正负样本比例约为1：3），SSD采用hard negative mining, 即对负样本按照其预测背景类的置信度进行降序排列，选取置信度较小的top-k作为训练的负样本。</p>
<p><strong>3. 怎样得到预测的检测结果？</strong></p>
<p>最后分别在所选的特征层上使用3x3卷积核预测不同default boxes所属的类别分数及其预测的边界框location。由于对于每个box需要预测该box属于每个类别的置信度（假设有c类，包括背景，例如20class的数据集合，c=21）和该box对应的预测边界框的location(包含4个值，即该box的中心坐标和宽高)，则每个box需要预测c+4个值。所以对于某个所选的特征层，该层的卷积核个数为（c+4）x 该层的default box个数.最后将每个层得到的卷积结果进行拼接。对于得到的每个预测框，取其类别置信度的最大值，若该最大值大于置信度阈值，则最大值所对应的类别即为该预测框的类别，否则过滤掉此框。对于保留的预测框根据它对应的先验框进行解码得到其真实的位置参数（这里还需注意要防止预测框位置超出图片），然后根据所属类别置信度进行降序排列，取top-k个预测框，最后进行NMS，过滤掉重叠度较大的预测框，最后得到检测结果。</p>
<p>SSD优势是速度比较快，整个过程只需要一步，首先在图片不同位置按照不同尺度和宽高比进行密集抽样，然后利用CNN提取特征后直接进行分类与回归，所以速度比较快，但均匀密集采样会造成正负样本不均衡的情况使得训练比较困难，导致模型准确度有所降低。另外，SSD对小目标的检测没有大目标好，因为随着网络的加深，在高层特征图中小目标的信息丢失掉了，适当增大输入图片的尺寸可以提升小目标的检测效果。</p>
<h3 id="8-3-2-DSSD">8.3.2 DSSD</h3>
<p><strong>DSSD有哪些创新点？</strong></p>
<ol>
<li>Backbone：将ResNet替换SSD中的VGG网络，增强了特征提取能力</li>
<li>添加了Deconvolution层，增加了大量上下文信息</li>
</ol>
<p>为了解决SSD算法检测小目标困难的问题，DSSD算法将SSD算法基础网络从VGG-16更改为ResNet-101，增强网络特征提取能力，其次参考FPN算法思路利用去Deconvolution结构将图像深层特征从高维空间传递出来，与浅层信息融合，联系不同层级之间的图像语义关系，设计预测模块结构，通过不同层级特征之间融合特征输出预测物体类别信息。</p>
<p>DSSD算法中有两个特殊的结构：Prediction模块；Deconvolution模块。前者利用提升每个子任务的表现来提高准确性，并且防止梯度直接流入ResNet主网络。后者则增加了三个Batch Normalization层和三个3×3卷积层，其中卷积层起到了缓冲的作用，防止梯度对主网络影响太剧烈，保证网络的稳定性。</p>
<p>SSD和DSSD的网络模型如下图所示：</p>
<p><img src="DSSD-01.png" alt></p>
<p><strong>Prediction Module</strong></p>
<p>SSD直接从多个卷积层中单独引出预测函数，预测量多达7000多，梯度计算量也很大。MS-CNN方法指出，改进每个任务的子网可以提高准确性。根据这一思想，DSSD在每一个预测层后增加残差模块，并且对于多种方案进行了对比，如下图所示。结果表明，增加残差预测模块后，高分辨率图片的检测精度比原始SSD提升明显。</p>
<p><img src="DSSD-02.png" alt></p>
<p><strong>Deconvolution模块</strong></p>
<p>为了整合浅层特征图和deconvolution层的信息，作者引入deconvolution模块，如下图所示。作者受到论文Learning to Refine Object Segments的启发，认为用于精细网络的deconvolution模块的分解结构达到的精度可以和复杂网络一样，并且更有效率。作者对其进行了一定的修改：其一，在每个卷积层后添加批归一化（batch normalization）层；其二，使用基于学习的deconvolution层而不是简单地双线性上采样；其三，作者测试了不同的结合方式，元素求和（element-wise sum）与元素点积（element-wise product）方式，实验证明元素点积计算能得到更好的精度。</p>
<p><img src="DSSD-03.png" alt></p>
<h3 id="8-3-3-YOLOv1">8.3.3 YOLOv1</h3>
<p><strong>YOLOv1有哪些创新点？</strong></p>
<ol>
<li>将整张图作为网络的输入，直接在输出层回归bounding box的位置和所属的类别</li>
<li>速度快，one stage detection的开山之作</li>
</ol>
<p><strong>YOLOv1介绍</strong></p>
<p>YOLO（You Only Look Once: Unified, Real-Time Object Detection）是one-stage detection的开山之作。之前的物体检测方法首先需要产生大量可能包含待检测物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。这种基于先产生候选区再检测的方法虽然有相对较高的检测准确率，但运行速度较慢。</p>
<p>YOLO创造性的将物体检测任务直接当作回归问题（regression problem）来处理，将候选区和检测两个阶段合二为一。只需一眼就能知道每张图像中有哪些物体以及物体的位置。下图展示了各物体检测系统的流程图。</p>
<p><img src="YOLOv1-01.png" alt></p>
<p>事实上，YOLO也并没有真正的去掉候选区，而是直接将输入图片划分成7x7=49个网格，每个网格预测两个边界框，一共预测49x2=98个边界框。可以近似理解为在输入图片上粗略的选取98个候选区，这98个候选区覆盖了图片的整个区域，进而用回归预测这98个候选框对应的边界框。</p>
<p><strong>1. 网络结构是怎样的？</strong></p>
<p>YOLO网络借鉴了GoogLeNet分类网络结构，不同的是YOLO使用1x1卷积层和3x3卷积层替代inception module。如下图所示，整个检测网络包括24个卷积层和2个全连接层。其中，卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。</p>
<p><img src="YOLOv1-02.png" alt></p>
<p><strong>2. YOLO的输入、输出、损失函数分别是什么？</strong></p>
<p>前面说到YOLO将输入图像分成7x7的网格，最后输出是7x7xk的张量。YOLO网络最后接了两个全连接层，全连接层要求输入是固定大小的，所以YOLO要求输入图像有固定大小，论文中作者设计的输入尺寸是448x448。</p>
<p>YOLO将输入图像分成7x7的网格，每个网格预测2个边界框。若某物体的ground truth的中心落在该网格，则该网格中与这个ground truth IOU最大的边界框负责预测该物体。对每个边界框会预测5个值，分别是边界框的中心x,y（相对于所属网格的边界），边界框的宽高w,h（相对于原始输入图像的宽高的比例），以及这些边界框的confidencescores（边界框与ground truth box的IOU值）。同时每个网格还需要预测c个类条件概率 （是一个c维向量，表示某个物体object在这个网格中，且该object分别属于各个类别的概率，这里的c类物体不包含背景）。论文中的c=20，则每个网格需要预测2x5+20=30个值，这些值被映射到一个30维的向量。<br>
为了让边界框坐标损失、分类损失达到很好的平衡，损失函数设计如下图所示。</p>
<p><img src="YOLOv1-03.png" alt></p>
<p>如上图所示，损失函数分为坐标预测（蓝色框）、含有物体的边界框的confidence预测（红色框）、不含有物体的边界框的confidence预测（黄色框）、分类预测（紫色框）四个部分。</p>
<p>由于不同大小的边界框对预测偏差的敏感度不同，小的边界框对预测偏差的敏感度更大。为了均衡不同尺寸边界框对预测偏差的敏感度的差异。作者巧妙的对边界框的w,h取均值再求L2 loss. YOLO中更重视坐标预测，赋予坐标损失更大的权重，记为 coord，在pascal voc训练中coodd=5 ，classification error部分的权重取1。</p>
<p>某边界框的置信度定义为：某边界框的confidence = 该边界框存在某类对象的概率pr(object)*该边界框与该对象的ground truth的IOU值 ，若该边界框存在某个对象pr(object)=1 ，否则pr(object)=0 。由于一幅图中大部分网格中是没有物体的，这些网格中的边界框的confidence置为0，相比于有物体的网格，这些不包含物体的网格更多，对梯度更新的贡献更大，会导致网络不稳定。为了平衡上述问题，YOLO损失函数中对没有物体的边界框的confidence error赋予较小的权重，记为 noobj，对有物体的边界框的confidence error赋予较大的权重。在pascal VOC训练中noobj=0.5 ，有物体的边界框的confidence error的权重设为1.</p>
<p><strong>3. YOLO怎样预测？</strong></p>
<p>YOLO最后采用非极大值抑制（NMS）算法从输出结果中提取最有可能的对象和其对应的边界框。</p>
<p>输入一张图片到YOLO网络将输出一个7<em>7</em>30的张量表示图片中每个网格对应的可能的两个边界框以及每个边界框的置信度和包含的对象属于各个类别的概率。由此可以计算某对象i属于类别 同时在第j个边界框中的得分：</p>
<p><img src="YOLOv1-04.png" alt></p>
<p>每个网格有20个类条件概率，2个边界框置信度，相当于每个网格有40个得分，7x7个网格有1960个得分，每类对象有1960/20=98个得分，即98个候选框。</p>
<p><strong>NMS步骤如下：</strong></p>
<p>1.设置一个Score的阈值，一个IOU的阈值；</p>
<p>2.对于每类对象，遍历属于该类的所有候选框，</p>
<p>①过滤掉Score低于Score阈值的候选框；</p>
<p>②找到剩下的候选框中最大Score对应的候选框，添加到输出列表；</p>
<p>③进一步计算剩下的候选框与②中输出列表中每个候选框的IOU，若该IOU大于设置的IOU阈值，将该候选框过滤掉，否则加入输出列表中；</p>
<p>④最后输出列表中的候选框即为图片中该类对象预测的所有边界框</p>
<p>3.返回步骤2继续处理下一类对象。</p>
<p>YOLO将识别与定位合二为一，结构简便，检测速度快，更快的Fast YOLO可以达到155FPS。相对于R-CNN系列, YOLO的整个流程中都能看到整张图像的信息，因此它在检测物体时能很好的利用上下文信息，从而不容易在背景上预测出错误的物体信息。同时YOLO可以学习到高度泛化的特征，能将一个域上学到的特征迁移到不同但相关的域上，如在自然图像上做训练的YOLO，在艺术图片上可以得到较好的测试结果。</p>
<p>由于YOLO网格设置比较稀疏，且每个网格只预测2个边界框，其总体预测精度不高，略低于Fast RCNN。其对小物体的检测效果较差，尤其是对密集的小物体表现比较差。</p>
<h3 id="8-3-4-YOLOv2">8.3.4 YOLOv2</h3>
<p><strong>YOLOv2 有哪些创新点？</strong></p>
<p>YOLOv1虽然检测速度快，但在定位方面不够准确，并且召回率较低。为了提升定位准确度，改善召回率，YOLOv2在YOLOv1的基础上提出了几种改进策略，如下图所示，可以看到，一些改进方法能有效提高模型的mAP。</p>
<ol>
<li>大尺度预训练分类</li>
<li>New Network：Darknet-19</li>
<li>加入anchor</li>
</ol>
<p><img src="YOLOv2-01.png" alt></p>
<p><strong>YOLOv2 介绍</strong></p>
<p><strong>（1）Batch Normalization</strong></p>
<p>YOLOv2中在每个卷积层后加Batch Normalization(BN)层，去掉dropout. BN层可以起到一定的正则化效果，能提升模型收敛速度，防止模型过拟合。YOLOv2通过使用BN层使得mAP提高了2%。<br>
<strong>（2）High Resolution Classifier</strong></p>
<p>目前的大部分检测模型都会使用主流分类网络（如vgg、resnet）在ImageNet上的预训练模型作为特征提取器,<br>
而这些分类网络大部分都是以小于256x256的图片作为输入进行训练的，低分辨率会影响模型检测能力。YOLOv2将输入图片的分辨率提升至448x448，为了使网络适应新的分辨率，YOLOv2先在ImageNet上以448x448的分辨率对网络进行10个epoch的微调，让网络适应高分辨率的输入。通过使用高分辨率的输入，YOLOv2的mAP提升了约4%。</p>
<p><strong>（3）Convolutional With Anchor Boxes</strong></p>
<p>YOLOv1利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。YOLOv2去掉了YOLOv1中的全连接层，使用Anchor Boxes预测边界框，同时为了得到更高分辨率的特征图，YOLOv2还去掉了一个池化层。由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLOv2通过缩减网络，使用416x416的输入，模型下采样的总步长为32，最后得到13x13的特征图，然后对13x13的特征图的每个cell预测5个anchor boxes，对每个anchor box预测边界框的位置信息、置信度和一套分类概率值。使用anchor<br>
boxes之后，YOLOv2可以预测13x13x5=845个边界框，模型的召回率由原来的81%提升到88%，mAP由原来的69.5%降低到69.2%.召回率提升了7%，准确率下降了0.3%。</p>
<p><strong>（4）Dimension Clusters</strong></p>
<p>在Faster R-CNN和SSD中，先验框都是手动设定的，带有一定的主观性。YOLOv2采用k-means聚类算法对训练集中的边界框做了聚类分析，选用boxes之间的IOU值作为聚类指标。综合考虑模型复杂度和召回率，最终选择5个聚类中心，得到5个先验框，发现其中中扁长的框较少，而瘦高的框更多，更符合行人特征。通过对比实验，发现用聚类分析得到的先验框比手动选择的先验框有更高的平均IOU值，这使得模型更容易训练学习。</p>
<p><strong>（5）New Network：Darknet-19</strong></p>
<p>YOLOv2采用Darknet-19，其网络结构如下图所示，包括19个卷积层和5个max pooling层，主要采用3x3卷积和1x1卷积，这里1x1卷积可以压缩特征图通道数以降低模型计算量和参数，每个卷积层后使用BN层以加快模型收敛同时防止过拟合。最终采用global avg pool 做预测。采用YOLOv2，模型的mAP值没有显著提升，但计算量减少了。</p>
<p><img src="YOLOv2-02.png" alt></p>
<p><strong>（6）Direct location prediction</strong></p>
<p>Faster R-CNN使用anchor boxes预测边界框相对先验框的偏移量，由于没有对偏移量进行约束，每个位置预测的边界框可以落在图片任何位置，会导致模型不稳定，加长训练时间。YOLOv2沿用YOLOv1的方法，根据所在网格单元的位置来预测坐标,则Ground Truth的值介于0到1之间。网络中将得到的网络预测结果再输入sigmoid函数中，让输出结果介于0到1之间。设一个网格相对于图片左上角的偏移量是cx，cy。先验框的宽度和高度分别是pw和ph，则预测的边界框相对于特征图的中心坐标(bx，by)和宽高bw、bh的计算公式如下图所示。</p>
<p><img src="YOLOv2-03.png" alt></p>
<p>YOLOv2结合Dimention Clusters, 通过对边界框的位置预测进行约束，使模型更容易稳定训练，这种方式使得模型的mAP值提升了约5%。</p>
<p><strong>（7）Fine-Grained Features</strong></p>
<p>YOLOv2借鉴SSD使用多尺度的特征图做检测，提出pass through层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度检测。YOLOv2提取Darknet-19最后一个max pool层的输入，得到26x26x512的特征图。经过1x1x64的卷积以降低特征图的维度，得到26x26x64的特征图，然后经过pass through层的处理变成13x13x256的特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13x13x1024大小的特征图连接，变成13x13x1280的特征图，最后在这些特征图上做预测。使用Fine-Grained Features，YOLOv2的性能提升了1%.</p>
<p><strong>（8）Multi-Scale Training</strong></p>
<p>YOLOv2中使用的Darknet-19网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。YOLOv2采用多尺度输入的方式训练，在训练过程中每隔10个batches,重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32的倍数{320,352,…,608}。采用Multi-Scale Training, 可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。</p>
<p>YOLOv2借鉴了很多其它目标检测方法的一些技巧，如Faster R-CNN的anchor boxes, SSD中的多尺度检测。除此之外，YOLOv2在网络设计上做了很多tricks,使它能在保证速度的同时提高检测准确率，Multi-Scale Training更使得同一个模型适应不同大小的输入，从而可以在速度和精度上进行自由权衡。</p>
<p><strong>YOLOv2的训练</strong></p>
<p>YOLOv2的训练主要包括三个阶段。<br>
第一阶段：先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为 $224\times 224$ ,共训练160个epochs。<br>
第二阶段：将网络的输入调整为 $448\times 448$ ,继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。<br>
第三个阶段：修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。<br>
网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个 $3\times 3 \times 2014$ 卷积层，同时增加了一个passthrough层，最后使用 $1\times 1$ 卷积层输出预测结果。</p>
<h3 id="8-3-5-YOLO9000">8.3.5 YOLO9000</h3>
<p>github：<a target="_blank" rel="noopener" href="http://pjreddie.com/yolo9000/">http://pjreddie.com/yolo9000/</a></p>
<p>YOLO9000是在YOLOv2的基础上提出的一种联合训练方法，可以检测超过9000个类别的模型。YOLOv2混合目标检测数据集和分类数据集，用目标检测数据集及其类别标记信息和位置标注信息训练模型学习预测目标定位和分类，用分类数据集及其类别标记信息进一步扩充模型所能识别的物体类别同时能增强模型鲁棒性。</p>
<p><strong>1. YOLO9000是怎么组织数据的？</strong></p>
<p>YOLO9000根据各个类别之间的从属关系建立一种树结WordTree, 将COCO数据集和ImageNet数据集组织起来。</p>
<p>WordTree的生成方式如下：</p>
<p>①首先遍历ImageNet中的类别名词。</p>
<p>②对每个名词，在WordNet(一种结构化概念及概念之间关系的语言数据库)上找到从它所在位置到根节点（设根节点为实体对象physical object）的最短路径，由于在WordNet中大多数同义词只有一个路径，所以先把将该路径上的词全都加到树中。</p>
<p>③迭代地检查剩下的名词，取它到根节点的最短路径，将该最短路径上的还没出现在层次树中的词加入到树中。<br>
混合后的数据集形成一个有9418类的WordTree.生成的WordTree模型如下图所示。另外考虑到COCO数据集相对于ImageNet数据集数据量太少了，为了平衡两个数据集，作者进一步对COCO数据集过采样，使COCO数据集与ImageNet数据集的数据量比例接近1：4。</p>
<p><img src="YOLOv2-04.png" alt></p>
<p>对于物体的标签，采用one-hot编码的形式，数据集中的每个物体的类别标签被组织成1个长度为9418的向量，向量中除在WordTree中从该物体对应的名词到根节点的路径上出现的词对应的类别标号处为1，其余位置为0。</p>
<p><strong>2. YOLO9000是怎么进行联合训练的？</strong></p>
<p>YOLO9000采用YOLOv2的结构，anchorbox由原来的5调整到3，对每个anchorbox预测其对应的边界框的位置信息x,y,w,h和置信度以及所包含的物体分别属于9418类的概率，所以每个anchorbox需要预测4+1+9418=9423个值。每个网格需要预测3x9423=28269个值。在训练的过程中，当网络遇到来自检测数据集的图片时，用完整的YOLOv2loss进行反向传播计算，当网络遇到来自分类数据集的图片时，只用分类部分的loss进行反向传播。</p>
<p><strong>3. YOLO9000是怎么预测的？</strong></p>
<p>WordTree中每个节点的子节点都属于同一个子类，分层次的对每个子类中的节点进行一次softmax处理，以得到同义词集合中的每个词的下义词的概率。当需要预测属于某个类别的概率时，需要预测该类别节点的条件概率。即在WordTree上找到该类别名词到根节点的路径，计算路径上每个节点的概率之积。预测时，YOLOv2得到置信度，同时会给出边界框位置以及一个树状概率图，沿着根节点向下，沿着置信度最高的分支向下，直到达到某个阈值，最后到达的节点类别即为预测物体的类别。</p>
<p>YOLO9000使用WordTree混合目标检测数据集和分类数据集，并在其上进行联合训练，使之能实时检测出超过9000个类别的物体，其强大令人赞叹不已。YOLO9000尤其对动物的识别效果很好，但是对衣服或者设备等类别的识别效果不是很好，可能的原因是与目标检测数据集中的数据偏向有关。</p>
<h3 id="8-3-6-YOLOv3">8.3.6 YOLOv3</h3>
<p>YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。</p>
<p><strong>YOLOv3有哪些创新点？</strong></p>
<ol>
<li>新网络结构：DarkNet-53</li>
<li>融合FPN</li>
<li>用逻辑回归替代softmax作为分类器</li>
</ol>
<p><strong>1. YOLOv3对网络结构做了哪些改进？</strong></p>
<p>YOLOv3在之前Darknet-19的基础上引入了残差块，并进一步加深了网络，改进后的网络有53个卷积层，取名为Darknet-53，网络结构如下图所示（以256*256的输入为例）。</p>
<p><img src="YOLOv3-01.png" alt></p>
<p>为了比较Darknet-53与其它网络结构的性能，作者在TitanX上，采用相同的实验设置，将256x256的图片分别输入以Darknet-19，ResNet-101，ResNet-152和Darknet-53为基础网络的分类模型中，实验得到的结果如下图所示。可以看到Darknet-53比ResNet-101的性能更好，而且速度是其1.5倍，Darknet-53与ResNet-152性能相似但速度几乎是其2倍。注意到，Darknet-53相比于其它网络结构实现了每秒最高的浮点计算量，说明其网络结构能更好的利用GPU。</p>
<p><img src="YOLOv3-02.png" alt></p>
<p><strong>2.YOLOv3中怎样实现多尺度检测？</strong></p>
<p>YOLOv3借鉴了FPN的思想，从不同尺度提取特征。相比YOLOv2，YOLOv3提取最后3层特征图，不仅在每个特征图上分别独立做预测，同时通过将小特征图上采样到与大的特征图相同大小，然后与大的特征图拼接做进一步预测。用维度聚类的思想聚类出9种尺度的anchor box，将9种尺度的anchor box均匀的分配给3种尺度的特征图.如下图是在网络结构图的基础上加上多尺度特征提取部分的示意图（以在COCO数据集(80类)上256x256的输入为例）：</p>
<p><img src="YOLOv3-03.png" alt></p>
<p>从YOLOv1到YOLOv2再到YOLO9000、YOLOv3, YOLO经历三代变革，在保持速度优势的同时，不断改进网络结构，同时汲取其它优秀的目标检测算法的各种trick，先后引入anchor box机制、引入FPN实现多尺度检测等。</p>
<h3 id="8-3-7-RetinaNet">8.3.7 RetinaNet</h3>
<p><strong>研究背景</strong></p>
<ul>
<li>Two-Stage检测器（如Faster R-CNN、FPN）效果好，但速度相对慢</li>
<li>One-Stage检测器（如YOLO、SSD）速度快，但效果一般</li>
</ul>
<p><img src="RetinaNet-01.png" alt></p>
<p>作者对one-stage检测器准确率不高的问题进行探究，发现主要问题在于正负类别不均衡（简单-难分类别不均衡）。</p>
<blockquote>
<p>We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause.</p>
</blockquote>
<p>作者建议通过重新设计标准的交叉熵损失（cross entropy loss）来解决这种类别不平衡（class inbalance）问题，即提出Focal Loss。</p>
<blockquote>
<p>We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training.</p>
</blockquote>
<p>结合Focal Loss的one-stage检测器称为RetinaNet，该检测器在COCO上mAP可以和特征金字塔网络（feature pyramid network，FPN）或者Mask R-CNN接近，</p>
<p><strong>问：什么是类别不均衡（class imbalance）？</strong></p>
<p>答：负样本的数量极大于正样本的数量，比如包含物体的区域（正样本）很少，而不包含物体的区域（负样本）很多。比如检测算法在早期会生成一大波的bbox。而一幅常规的图片中，顶多就那么几个object。这意味着，绝大多数的bbox属于background。</p>
<p><strong>问：样本的类别不均衡会带来什么问题？</strong></p>
<p>答：由于大多数都是简单易分的负样本（属于背景的样本），使得训练过程不能充分学习到属于那些有类别样本的信息；其次简单易分的负样本太多，可能掩盖了其他有类别样本的作用（这些简单易分的负样本仍产生一定幅度的loss，见下图蓝色曲线，数量多会对loss起主要贡献作用，因此就主导了梯度的更新方向，掩盖了重要的信息）</p>
<blockquote>
<p>This imbalance causes two problems: (1) training is inefficient as most locations are easy negatives that contribute no useful learning signal; (2) en masse, the easy negatives can overwhelm training and lead to degenerate models.</p>
</blockquote>
<p>简单来说，因为bbox数量爆炸。 正是因为bbox中属于background的bbox太多了，所以如果分类器无脑地把所有bbox统一归类为background，accuracy也可以刷得很高。于是乎，分类器的训练就失败了。分类器训练失败，检测精度自然就低了。</p>
<p><strong>问：为什么在two-stage检测器中，没有出现类别不均衡（class imbalamce）问题呢？</strong></p>
<p>答：因为通过RPN阶段可以减少候选目标区域，而在分类阶段，可以固定前景与背景比值（foreground-to-background ratio）为1:3，或者使用OHEM（online hard example mining）使得前景和背景的数量达到均衡。</p>
<p><strong>RetinaNet有哪些创新点？</strong></p>
<p><strong>概述：</strong></p>
<ul>
<li>New loss：提出Focal Loss函数解决class imbalance</li>
</ul>
 $$
FL(p_t) = -(1-p_t)^\gamma \log(p_t)FL(pt)=−(1−pt)γlog(pt)
$$ 
<ul>
<li>New detector：RetinaNet = ResNet + FPN + Two sub-networks + Focal Loss</li>
</ul>
<p>Focal Loss更加聚焦在困难样本（hard examples）上的训练。</p>
<p><img src="RetinaNet-02.png" alt></p>
<p>将Focal Loss与ResNet-101-FPN backbone结合提出RetinaNet（one-stage检测器），RetinaNet在COCO test-dev上达到39.1mAP，速度为5FPS。</p>
<p>RetinaNet检测器与当时最佳的其它检测器进行比较，无论是速度上还是准确率上都是最佳：</p>
<p><img src="RetinaNet-03.png" alt></p>
<p><strong>详解：</strong></p>
<p>作者提出一种新的损失函数，思路是希望那些hard examples对损失的贡献变大，使网络更倾向于从这些样本上学习。</p>
<p>作者以二分类为例进行说明：</p>
<p><strong>交叉熵函数CE</strong></p>
<p>首先是我们常使用的交叉熵损失函数：</p>
<p><img src="RetinaNet-04.png" alt></p>
<p>上式中，y=+1或者y=-1。p∈[0,1]是y=+1的估计概率。作者定义pt为：</p>
<p><img src="RetinaNet-05.png" alt></p>
<p><img src="RetinaNet-06.png" alt></p>
<p>注：对交叉熵函数不了解的，可以参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/chaipp0607/article/details/73392175">理解交叉熵作为损失函数在神经网络中的作用</a></p>
<p><strong>均衡交叉熵函数</strong></p>
<p>要对类别不均衡问题对loss的贡献进行一个控制，即加上一个控制权重即可，最初作者的想法即如下这样，对于属于少数类别的样本，增大α即可</p>
<p><img src="RetinaNet-07.png" alt></p>
<p>但这样有一个问题，它仅仅解决了正负样本之间的平衡问题，并没有区分易分/难分样本，按作者的话说：</p>
<blockquote>
<p>While α balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. Instead, we propose to reshape the loss function to down-weight easy examples and thus focus training on hard negatives.</p>
</blockquote>
<p>问：为什么公式(3)只解决正负样本不均衡问题？</p>
<p>答：增加了一个系数αt，跟pt的定义类似，当label=1的时候，αt=a；当label=-1的时候，αt=1-a，a的范围也是0到1。因此可以通过设定a的值（一般而言假如1这个类的样本数比-1这个类的样本数多很多，那么a会取0到0.5来增加-1这个类的样本的权重）来控制正负样本对总的loss的共享权重。</p>
<p><strong>Focal Loss</strong></p>
<p>作者一开始给交叉熵损失函数添加modulating factor：</p>
 $$
(1-pt)^γ(1−pt)γ
$$ 
<p><img src="RetinaNet-08.png" alt></p>
<p>显然，样本越易分，pt就越大（pt—&gt;1），modulating factor趋近于0，则贡献的loss就越小，同样地，样本越难分，其pt就越小，modulating factor接近于1，则贡献的loss不受影响。</p>
<p>问：为什么pt越大，FL值越小？</p>
<p>答：根据公式（4）可知，FL与log(pt)中的pt成反比，与1-pt成正比，因此FL与pt的关系成反比。这是交叉熵函数的基本性质。当pt很大时（接近于1），FL值很小；而当pt很小时（接近于0），FL值会很大。</p>
<p>注：这里有个超参数—focusing parameter γ。</p>
<p>γ 放大了modulating factor的作用。</p>
<p>举原文中的一个例子，当pt=0.9时，带有modulating factor的focal loss是CE loss的100分之一，即进一步减小了正确分类的损失。</p>
<blockquote>
<p>For instance, with γ = 2, an example classified with pt = 0.9 would have 100× lower loss compared with CE and with pt ≈ 0.968 it would have 1000× lower loss. This in turn increases the importance of correcting misclassified examples (whose loss is scaled down by at most 4× for pt ≤ .5 and γ = 2).</p>
</blockquote>
<p>在实际中，作者采用如下公式，即综合了公式(3)和公式(4)的形式，这样机能调整正负样本的权重，又能控制难易分类样本的权重：</p>
<p><img src="RetinaNet-09.png" alt></p>
<p>这里的两个参数 α和γ 来控制，在实验中a的选择范围也很广，一般而言当γ增加的时候，a需要减小一点，本文作者采用α=0.25，γ=2效果最好。</p>
<p><strong>RetinaNet Detector</strong></p>
<p>RetinaNet是由backbone网络和两个特殊任务的子网络（subnet）组成（属于one-stage检测器）。Backbone用来计算feature map；第一个子网络用来object classification，第二个子网络用来bounding box regression。</p>
<p><strong>Feature Pyramid Network Backbone</strong></p>
<p><img src="RetinaNet-10.png" alt></p>
<p><strong>Anchor</strong></p>
<p><strong>Classification Subnet</strong></p>
<p><strong>Box Regression Subnet</strong></p>
<p><img src="RetinaNet-11.png" alt></p>
<p><img src="RetinaNet-12.png" alt></p>
<p>RetinaNet结构注意内容：</p>
<ol>
<li>训练时FPN每一级的所有example都被用于计算Focal Loss，loss值加到一起用来训练；</li>
<li>测试时FPN每一级只选取score最大的1000个example来做nms；</li>
<li>整个结构不同层的head部分(上图中的c和d部分)共享参数，但分类和回归分支间的参数不共享；</li>
<li>分类分支的最后一级卷积的bias初始化成前面提到的-log((1-π)/π);</li>
</ol>
<p>作者：张磊_0503 链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/204d9ad9507f">https://www.jianshu.com/p/204d9ad9507f</a> 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</p>
<p><strong>实验结果</strong></p>
<p>Table1是关于RetinaNet和Focal Loss的一些实验结果。（a）是在交叉熵的基础上加上参数a，a=0.5就表示传统的交叉熵，可以看出当a=0.75的时候效果最好，AP值提升了0.9。（b）是对比不同的参数γ和a的实验结果，可以看出随着γ的增加，AP提升比较明显。（d）通过和OHEM的对比可以看出最好的Focal Loss比最好的OHEM提高了3.2AP。这里OHEM1:3表示在通过OHEM得到的minibatch上强制positive和negative样本的比例为1:3，通过对比可以看出这种强制的操作并没有提升AP。（e）加入了运算时间的对比，可以和前面的Figure2结合起来看，速度方面也有优势！注意这里RetinaNet-101-800的AP是37.8，当把训练时间扩大1.5倍同时采用scale jitter，AP可以提高到39.1，这就是全文和table2中的最高的39.1AP的由来。</p>
<p><img src="RetinaNet-13.png" alt></p>
<p><img src="RetinaNet-14.png" alt></p>
<h3 id="8-3-8-RFBNet">8.3.8 RFBNet</h3>
<p><strong>RFBNet有哪些创新点？</strong></p>
<ol>
<li>提出RF block（RFB）模块</li>
</ol>
<p>RFBNet主要想利用一些技巧使得轻量级模型在速度和精度上达到很好的trade-off的检测器。灵感来自人类视觉的感受野结构Receptive Fields (RFs) ，提出了新奇的RF block（RFB）模块，来验证感受野尺寸和方向性的对提高有鉴别鲁棒特征的关系。RFBNet是以主干网络（backbone）为VGG16的SSD来构建的，主要是在Inception的基础上加入了dilated卷积层（dilated convolution），从而有效增大了感受野（receptive field）。整体上因为是基于SSD网络进行改进，所以检测速度还是比较快，同时精度也有一定的保证。</p>
<p><strong>RFB介绍</strong></p>
<p>RFB是一个类似Inception模块的多分支卷积模块，它的内部结构可分为两个组件：多分支卷积层和dilated卷积层。如下图：</p>
<p><img src="RFBNet-01.png" alt></p>
<p><strong>1.多分支卷积层</strong><br>
​      根据RF的定义，用多种尺寸的卷积核来实现比固定尺寸更好。具体设计：1.瓶颈结构，1x1-s2卷积减少通道特征，然后加上一个nxn卷积。2.替换5x5卷积为两个3x3卷积去减少参数，然后是更深的非线性层。有些例子，使用1xn和nx1代替nxn卷积；shortcut直连设计来自于ResNet和Inception ResNet V2。3.为了输出，卷积经常有stride=2或者是减少通道，所以直连层用一个不带非线性激活的1x1卷积层。</p>
<p><strong>2.Dilated 卷积层</strong></p>
<p>设计灵感来自Deeplab，在保持参数量和同样感受野的情况下，用来获取更高分辨率的特征。下图展示两种RFB结构：RFB和RFB-s。每个分支都是一个正常卷积后面加一个dilated卷积，主要是尺寸和dilated因子不同。（a）RFB。整体结构上借鉴了Inception的思想，主要不同点在于引入3个dilated卷积层（比如3x3conv，rate=1），这也是RFBNet增大感受野的主要方式之一；（b）RFB-s。RFB-s和RFB相比主要有两个改进，一方面用3x3卷积层代替5x5卷积层，另一方面用1x3和3x1卷积层代替3x3卷积层，主要目的应该是为了减少计算量，类似Inception后期版本对Inception结构的改进。</p>
<p><img src="RFBNet-02.png" alt></p>
<p>RFBNet300的整体结构如下图所示，基本上和SSD类似。RFBNet和SSD不同的是：1、主干网上用两个RFB结构替换原来新增的两层。2、conv4_3和conv7_fc在接预测层之前分别接RFB-s和RFB结构。</p>
<p><img src="RFBNet-03.png" alt></p>
<h3 id="8-3-9-M2Det">8.3.9 M2Det</h3>
<p><strong>M2Det有哪些创新点？</strong></p>
<ol>
<li>提出了多层次特征金字塔网络（MLFPN）来构建更有效的特征金字塔，用于检测不同尺度的对象。</li>
</ol>
<p>M2Det的整体架构如下所示。M2Det使用backbone和多级特征金字塔网络（MLFPN）从输入图像中提取特征，然后类似于SSD，根据学习的特征生成密集的边界框和类别分数，最后是非最大抑制（NMS）操作以产生最终结果。 MLFPN由三个模块组成：特征融合模块（FFM），简化的U形模块（TUM）和按基于尺度的特征聚合模块（SFAM）。 FFMv1通过融合骨干网络的特征图，将语义信息丰富为基本特征。每个TUM生成一组多尺度特征，然后交替连接的TUM和FFMv2提取多级多尺度特征。此外，SFAM通过按比例缩放的特征连接操作和自适应注意机制将特征聚合到多级特征金字塔中。下面介绍有关M2Det中三个核心模块和网络配置的更多详细信息。</p>
<p><img src="M2Det-01.png" alt></p>
<p><strong>FFMs</strong></p>
<p>FFM融合了M2Det中不同层次的特征，这对于构建最终的多级特征金字塔至关重要。它们使用1x1卷积层来压缩输入特征的通道，并使用连接操作来聚合这些特征图。特别是，由于FFMv1以backbone中不同比例的两个特征图作为输入，因此它采用一个上采样操作，在连接操作之前将深度特征重新缩放到相同的尺度。同时，FFMv2采用基本特征和前一个TUM的最大输出特征图 - 这两个具有相同的比例 - 作为输入，并产生下一个TUM的融合特征。 FFMv1和FFMv2的结构细节分别如下图（a）和（b）所示。</p>
<p><img src="M2Det-02.png" alt></p>
<p><strong>TUMs</strong></p>
<p>TUM不同于FPN和RetinaNet，TUM采用简化的U形结构，如上图（c）所示。编码器是一系列3x3，步长为2的卷积层.并且解码器将这些层的输出作为其参考特征集，而原始FPN选择ResNet主干网络中每个阶段的最后一层的输出。此外，在解码器分支的上采样层后添加1x1卷积层和按元素求和的操作，以增强学习能力并保持特征的平滑性。每个TUM的解码器中的所有输出形成当前级别的多尺度特征。整体而言，堆叠TUM的输出形成多层次多尺度特征，而前TUM主要提供浅层特征，中间TUM提供中等特征，后TUM提供深层特征。</p>
<p><strong>SFAM</strong></p>
<p>SFAM旨在将由TUM生成的多级多尺度特征聚合成多级特征金字塔，如下图所示。SFAM的第一阶段是沿着信道维度将等效尺度的特征连接在一起。聚合特征金字塔可以表示为 $X = [X_1,X_2,...,X_i,...,X_L]$ ，其中</p>
 $$X_i = Concat(X_{1i}, X_{2i}, ...., X_{Li}) \in R^{W_i \times H_i \times C}$$ 
<p>指的是尺度第i个最大的特征。这里，聚合金字塔中的每个比例都包含来自多级深度的特征。但是，简单的连接操作不太适合。在第二阶段，引入了通道注意模块，以促使特征集中在最有益的通道。在SE区块之后，使用全局平均池化来在挤压步骤中生成通道统计z∈RC。</p>
<p><img src="M2Det-03.png" alt></p>
<h2 id="8-4-人脸检测">8.4 人脸检测</h2>
<p>在目标检测领域可以划分为了人脸检测与通用目标检测，往往人脸这方面会有专门的算法（包括人脸检测、人脸识别、人脸其他属性的识别等等），并且和通用目标检测（识别）会有一定的差别，着主要来源于人脸的特殊性（有时候目标比较小、人脸之间特征不明显、遮挡问题等），下面将从人脸检测和通用目标检测两个方面来讲解目标检测。</p>
<h3 id="8-4-1-目前主要有人脸检测方法分类？">8.4.1 目前主要有人脸检测方法分类？</h3>
<p>目前人脸检测方法主要包含两个区域：传统人脸检测算法和基于深度学习的人脸检测算法。传统人脸检测算法主要可以分为4类：</p>
<p>（1）基于知识的人脸检测方法；</p>
<p>（2）基于模型的人脸检测方法；</p>
<p>（3）基于特征的人脸检测方法；</p>
<p>（4）基于外观的人脸检测方法。</p>
<p>由于本书着重关注深度学习，下面会着重介绍基于深度学习的人脸检测方法。</p>
<p>2006年Hinton首次提出深度学习（Deep Learning）的概念，它是通过组合低层的特征形成更高层的抽象特征。随后研究者将深度学习应用在人脸检测领域，主要集中在基于卷积神经网络（CNN）的人脸检测研究，如基于级联卷积神经网络的人脸检测（cascade cnn）、 基于多任务卷积神经网络的人脸检测（MTCNN）、Facebox等，很大程度上提高了人脸检测的鲁棒性。当然通用目标检测算法像Faster-rcnn、yolo、ssd等也有用在人脸检测领域，也可以实现比较不错的结果，但是和专门人脸检测算法比还是有差别。下面部分主要介绍基于深度学习的的人脸检测算法，基于深度学习的通用目标检测算法将在第二大节介绍。</p>
<h3 id="8-4-2-如何检测图片中不同大小的人脸？">8.4.2 如何检测图片中不同大小的人脸？</h3>
<p>传统人脸检测算法中针对不同大小人脸主要有两个策略：</p>
<p>（1）缩放图片的大小（图像金字塔如图8.4.1所示）；</p>
<p>（2）缩放滑动窗的大小（如图8.4.2所示）。</p>
<p><img src="8.4.1.png" alt></p>
<p>图 8.1 图像金字塔</p>
<p>​      <img src="8.4.2.png" alt></p>
<p>图 8.2 缩放滑动窗口</p>
<p>​	基于深度学习的人脸检测算法中针对不同大小人脸主要也有两个策略，但和传统人脸检测算法有点区别，主要包括:</p>
<p>（1）缩放图片大小。（不过也可以通过缩放滑动窗的方式，基于深度学习的滑动窗人脸检测方式效率会很慢存在多次重复卷积，所以要采用全卷积神经网络（FCN），用FCN将不能用滑动窗的方法。）</p>
<p>（2）通过anchor box的方法（如图8.3所示，不要和图8.2混淆，这里是通过特征图预测原图的anchor box区域，具体在facebox中有描述）。</p>
<p><img src="8.4.3.png" alt></p>
<p>图 8.3 anchor box</p>
<h3 id="8-4-3-如何设定算法检测最小人脸尺寸">8.4.3 如何设定算法检测最小人脸尺寸?</h3>
<p>主要是看滑动窗的最小窗口和anchorbox的最小窗口。</p>
<p>（1）滑动窗的方法</p>
<p>假设通过12×12的滑动窗，不对原图做缩放的话，就可以检测原图中12×12的最小人脸。但是往往通常给定最小人脸a=40、或者a=80，以这么大的输入训练CNN进行人脸检测不太现实，速度会很慢，并且下一次需求最小人脸a=30*30又要去重新训练，通常还会是12×12的输入，为满足最小人脸框a，只需要在检测的时候对原图进行缩放即可：w=w×12/a。</p>
<p>（2）anchorbox的方法</p>
<p>原理类似，这里主要看anchorbox的最小box，通过可以通过缩放输入图片实现最小人脸的设定。</p>
<h3 id="8-4-4-如何定位人脸的位置？">8.4.4 如何定位人脸的位置？</h3>
<p>（1）滑动窗的方式：</p>
<p>滑动窗的方式是基于分类器识别为人脸的框的位置确定最终的人脸，</p>
<p><img src="8.4.4.png" alt></p>
<p>图 8.4 滑动窗</p>
<p>（2）FCN的方式：</p>
<p>​    FCN的方式通过特征图映射到原图的方式确定最终识别为人脸的位置，特征图映射到原图人脸框是要看特征图相比较于原图有多少次缩放（缩放主要查看卷积的步长和池化层），假设特征图上(2,3)的点，可粗略计算缩放比例为8倍，原图中的点应该是(16,24)；如果训练的FCN为12*12的输入，对于原图框位置应该是(16,24,12,12),当然这只是估计位置，具体的再构建网络时要加入回归框的预测，主要是相对于原图框的一个平移与缩放。</p>
<p>（3）通过anchor box的方式：</p>
<p>​    通过特征图映射到图的窗口，通过特征图映射到原图到多个框的方式确定最终识别为人脸的位置。</p>
<h3 id="8-4-5-如何通过一个人脸的多个框确定最终人脸框位置？">8.4.5 如何通过一个人脸的多个框确定最终人脸框位置？</h3>
<p><img src="8.4.5.png" alt></p>
<p>图 8.5 通过NMS得到最终的人脸位置</p>
<p>NMS改进版本有很多，最原始的NMS就是判断两个框的交集，如果交集大于设定的阈值，将删除其中一个框，那么两个框应该怎么选择删除哪一个呢？ 因为模型输出有概率值，一般会优选选择概率小的框删除。</p>
<h3 id="8-4-6-基于级联卷积神经网络的人脸检测（Cascade-CNN）">8.4.6 基于级联卷积神经网络的人脸检测（Cascade CNN）</h3>
<ol>
<li>
<p>cascade cnn的框架结构是什么？</p>
<p><img src="8.4.6.png" alt></p>
</li>
</ol>
<p>级联结构中有6个CNN，3个CNN用于人脸非人脸二分类，另外3个CNN用于人脸区域的边框校正。给定一幅图像，12-net密集扫描整幅图片，拒绝90%以上的窗口。剩余的窗口输入到12-calibration-net中调整大小和位置，以接近真实目标。接着输入到NMS中，消除高度重叠窗口。下面网络与上面类似。</p>
<ol start="2">
<li>cascade cnn人脸校验模块原理是什么？</li>
</ol>
<p>该网络用于窗口校正，使用三个偏移变量：Xn:水平平移量，Yn:垂直平移量，Sn:宽高比缩放。候选框口(x,y,w,h)中，(x,y)表示左上点坐标，(w,h)表示宽和高。</p>
<p>我们要将窗口的控制坐标调整为：</p>
 $$
（x-{x_nw}/{s_n},y-{y_nh}/{s_n},{w}/{s_n},{h}/{s_n}）
$$ 
<p>这项工作中，我们有 $N=5×3×3=45$ 种模式。偏移向量三个参数包括以下值：</p>
 $$
Sn：(0.83,0.91,1.0,1.10,1.21)    
$$ 
 $$
Xn：(-0.17,0,0.17)
$$ 
 $$
Yn：(-0.17,0,0.17)
$$ 
<p>同时对偏移向量三个参数进行校正。</p>
<p><img src="8.4.8.png" alt></p>
<p>3、训练样本应该如何准备？</p>
<p>人脸样本：</p>
<p>非人脸样本：</p>
<ol start="4">
<li>级联的好处</li>
</ol>
<p>级联的工作原理和好处：</p>
<ul>
<li>最初阶段的网络可以比较简单，判别阈值可以设得宽松一点，这样就可以在保持较高召回率的同时排除掉大量的非人脸窗口；</li>
<li>最后阶段网络为了保证足够的性能，因此一般设计的比较复杂，但由于只需要处理前面剩下的窗口，因此可以保证足够的效率；</li>
<li>级联的思想可以帮助我们去组合利用性能较差的分类器，同时又可以获得一定的效率保证。</li>
</ul>
<h3 id="8-4-7-基于多任务卷积神经网络的人脸检测（MTCNN）">8.4.7 基于多任务卷积神经网络的人脸检测（MTCNN）</h3>
<p><img src="8.4.9.png" alt></p>
<p><img src="8.4.10.png" alt></p>
<p><img src="8.4.11.png" alt></p>
<p><img src="8.4.12.png" alt></p>
<p>1.MTCNN模型有三个子网络。分别是P-Net,R-Net,O-Net.我想问一下，1.模型中的三个input size是指的是同一张图resize到不同尺度下喂给不同模型，还是同一张图，依次经过三个模型，然后是不同的输入尺寸？（这部分能给我讲一下吗）2.每个模型它都有对应三个结果（face classification;bounding box;facial landmark）这三个在网络上是如何对应的呢？</p>
<p>为了检测不同大小的人脸，开始需要构建图像金字塔，先经过pNet模型，输出人脸类别和边界框（边界框的预测为了对特征图映射到原图的框平移和缩放得到更准确的框），将识别为人脸的框映射到原图框位置可以获取patch，之后每一个patch通过resize的方式输入到rNet，识别为人脸的框并且预测更准确的人脸框，最后rNet识别为人脸的的每一个patch通过resize的方式输入到oNet，跟rNet类似，关键点是为了在训练集有限情况下使模型更鲁棒。</p>
<p>还要注意一点构建图像金字塔的的缩放比例要保留，为了将边界框映射到最开始原图上的</p>
<p>还要注意一点：如何从featureMap映射回原图</p>
<h3 id="8-4-8-Facebox">8.4.8 Facebox</h3>
<p><img src="8.4.13.png" alt></p>
<p><strong>（1）Rapidly Digested Convolutional Layers(RDCL)</strong></p>
<p>在网络前期，使用RDCL快速的缩小feature map的大小。 主要设计原则如下：</p>
<ul>
<li>Conv1, Pool1, Conv2 和 Pool2 的stride分别是4, 2, 2 和 2。这样整个RDCL的stride就是32，可以很快把feature map的尺寸变小。</li>
<li>卷积(或pooling)核太大速度就慢，太小覆盖信息又不足。文章权衡之后，将Conv1, Pool1, Conv2 和 Pool2 的核大小分别设为7x7,3x3,5x5,3x3</li>
<li>使用CReLU来保证输出维度不变的情况下，减少卷积核数量。</li>
</ul>
<p><strong>（2）Multiple Scale Convolutional Layers(MSCL)</strong></p>
<p>在网络后期，使用MSCL更好地检测不同尺度的人脸。 主要设计原则有：</p>
<ul>
<li>类似于SSD，在网络的不同层进行检测；</li>
<li>采用Inception模块。由于Inception包含多个不同的卷积分支，因此可以进一步使得感受野多样化。</li>
</ul>
<p><strong>（3）Anchor densification strategy</strong></p>
<p>为了anchor密度均衡，可以对密度不足的anchor以中心进行偏移加倍，如下图所示：</p>
<p><img src="8.4.14.png" alt></p>
<h2 id="8-5-目标检测的技巧汇总">8.5 目标检测的技巧汇总</h2>
<h3 id="8-5-1-Data-Augmentation（贡献者：北京理工大学–明奇）">8.5.1 Data Augmentation（贡献者：北京理工大学–明奇）</h3>
<p>介绍一篇发表在Big Data上的数据增强相关的文献综述。</p>
<ol>
<li><strong>Introduction</strong></li>
</ol>
<ul>
<li>数据增强与过拟合<br>
验证是否过拟合的方法：画出loss曲线，如果训练集loss持续减小但是验证集loss增大，就说明是过拟合了。</li>
</ul>
<p><img src="8.5.1-1.png" alt></p>
<ul>
<li>
<p>数据增强目的<br>
通过数据增强实现数据更复杂的表征，从而减小验证集和训练集以及最终测试集的差距，让网络更好地学习迁移数据集上的数据分布。这也说明网络不是真正地理解数据，而是记忆数据分布。</p>
</li>
<li>
<p>数据增强的方法<br>
（1）数据变换增强<br>
包括几何变换、色彩空间变换，随机擦除，对抗训练，神经风格迁移等<br>
（2）重采样增强<br>
主要侧重于新的实例合成。如图像混合（mixup），特征空间的增强，GAN生成图片。一张图看明白：</p>
</li>
</ul>
<p><img src="8.5.1-2.png" alt></p>
<ol start="2">
<li><strong>Image Data Augmentation techniques</strong></li>
</ol>
<p>2.1 <strong>Data Augmentations based on basic image manipulations</strong></p>
<ul>
<li>
<p>Geometric transformations<br>
  如果数据集潜在的表征能够被观察和分离，那么简单的几何变换就能取得很好的效果。对于复杂的数据集如医学影像，数据小而且训练集和测试集的偏差大，几何变换等增强的合理运用就很关键。</p>
<ul>
<li>
<p>Flipping<br>
作者提到了要衡量普遍性的观点。但是这种变换对于数字数据集不具有安全性。</p>
</li>
<li>
<p>Color space<br>
主要提及的识别RGB通道上的变换，将三通道图进行分离，以及直方图变换增强等。（颜色空间更多增强方式可以参考A Preliminary Study on Data Augmentation of Deep Learning for Image Classification）</p>
</li>
<li>
<p>Cropping<br>
通常在输入图片的尺寸不一时会进行按中心的裁剪操作。裁剪某种程度上和平移操作有相似性。根据裁剪幅度变化，该操作具有一定的不安全性。</p>
</li>
<li>
<p>Rotation<br>
大幅度的旋转对数字集会有不安全性的考虑。</p>
</li>
<li>
<p>Translation<br>
平移也需要合理设计。如车站人脸检测，只需要中心检测时，就可以加合适的平移增强。平移后空出部分填0或者255，或用高斯分布噪声。</p>
</li>
<li>
<p>Noise injection<br>
在像素上叠加高斯分布的随机噪声。</p>
</li>
</ul>
</li>
<li>
<p>Color space transformations<br>
  由于实际图像中一定存在光线偏差，所以光线的增强十分有必要（但是IJCV的光流文章指出，3D建模的灯光增强实在是很难学习到，所以对于光线增强的效果不如几何也可能因为<strong>光线的复杂度更高，数据样本远远不够</strong>）。色彩变换十分多样，如像素限制、像素矩阵变换、像素值颠倒等；灰度图和彩图相比，计算时间成本大大较少，但是据实验效果会下降一些，很明显因为特征的维度被降维了；还有尝试将RGB映射到其他的色彩空间进行学习，YUV,CMY.HSV等。<br>
  除了计算大内存消耗和时间长等缺点，色彩变换也面临不安全性，比如识别人脸的关键信息是黄白黑，但是大量增强出红绿蓝，会丢信息。颜色变换的增强方法是从色彩空间角度拟合偏置，效果有限的可能性是多样的：1. 真实几何多样性比颜色更简单  2. 色彩的变化多样性更多，导致增强不够反而学不好，颜色空间的欠拟合 3. <strong>变换不安全</strong></p>
</li>
<li>
<p>Experiment<br>
<img src="8.5.1-3.png" alt></p>
</li>
</ul>
<p><strong>随机裁剪</strong>效果最好。</p>
<p>2.2  <strong>Geometric versus photometric transformations</strong></p>
<ul>
<li>
<p>Kernel filter<br>
滤波器核在图像处理用的比较广，这里提到用这种方法来增强。还提到了一种正则化增强方法PatchShuffle，在一个patch内随机交换像素值，使得对噪声的抵抗更强以及避免过拟合。<br>
文章指出关于应用滤波器增强的工作尚且不多，因为这种方法其实和CNN的机制是一样的，这么做也许还不如直接在原始CNN上加层加深网络。</p>
</li>
<li>
<p>Mixing images<br>
<s>就是那篇被ICLR拒稿的采样方法</s>直接均值相加混合。</p>
</li>
</ul>
<p><img src="8.5.1-4.png" alt></p>
<p>  还有非线性的mixup裁剪如下：</p>
<p><img src="8.5.1-5.png" alt></p>
<p>  以及随机裁剪的图像混合：</p>
<p><img src="8.5.1-6.png" alt></p>
<p>  这些混合方式是十分反人类直觉的，因此可解释性不强。只能说是可能增强了对底层低级特征如线条边缘等的鲁棒性。其实有点没有抓住关键点。</p>
<ul>
<li>Random erasing<br>
随机擦除就是类似cutout的思想，通过mask的遮挡使得网络能够提高遮挡情况的鲁棒性。需要手工设计的部分包括mask的大小以及生成方式。是一种比较有效的方法。这种方式也需要考量增强的安全性，比如MNIST数据集8cutout后可能出问题。</li>
</ul>
<p><img src="8.5.1-7.png" alt></p>
<ul>
<li>A note on combining augmentations<br>
组合的增强方式往往是连续变化的，导致数据集的容量会迅速扩大，这对于小数据集领域来说容易发生过拟合 ，所以需要设计合理的搜索算法设计恰当的训练数据集。</li>
</ul>
<p>2.3  <strong>Data Augmentations based on Deep Learning</strong></p>
<ul>
<li>
<p>Feature space augmentation<br>
之前刚看的基于SMOTE类别不平衡的过采样法来进行特征空间的插值操作进行数据增强，就实验效果而言不算特别出众。</p>
</li>
<li>
<p>Adversarial training<br>
对抗样本训练可以提高鲁棒性，但是实际应用中其实提高不一定明显，因为自然对抗样本的数目没有那么多。而NIPS的对抗攻击大赛很多从神经网络的学习策略下手，进行梯度攻击，更加偏向于人为的攻击了，对于普适的检测性能提高意义反而不大，更强调安全需求高的场合。</p>
</li>
<li>
<p>GAN‑based Data Augmentation</p>
</li>
<li>
<p>Neural Style Transfer</p>
</li>
</ul>
<p>不觉得这个效果会普遍很好，应该来说是针对特定域会有效（如白天黑夜），实际效果应该有限。</p>
<ul>
<li>Meta learning Data Augmentations
<ul>
<li>Neural augmentation</li>
<li>Smart Augmentation<br>
两个东西差不多，就是上次看到SmartAugment方法。随机采样类内图片进行通道叠加然后输出融合图像，学通过梯度下降使得输出图像的类内差距减小（没考虑类间关系，可能也不便处理）。</li>
</ul>
</li>
</ul>
<p><img src="8.5.1-8.png" alt></p>
<ul>
<li>AutoAugment<br>
谷歌最早做的自学习增强方法，走的NAS的思路RL+RNN搜索增强空间，还有后来最近发的检测增强也是大同小异，基本就是换汤不换药，问题在于<strong>搜索空间太大</strong>，复现搜索过于依赖硬件条件（<s>普通实验室玩不起</s>）</li>
</ul>
<ol start="3">
<li><strong>Design considerations for image Data Augmentation</strong></li>
</ol>
<p>3.1  <strong>Test-time augmentation</strong><br>
  许多都论文指出在检测阶段进行同等的数据增强能够获得较好的效果。归结可以认为是训练检测阶段的一致性。当然，这种手段时间成本太高，只在如医学影像等追求精度的关键领域可以使用。</p>
<p>3.2  <strong>Curriculum learning</strong><br>
  Bengio团队早年在ICML提出的观点，确实合理，一开始就进行大量的增强容易导致网络不收敛。<br>
从一个数据集学习到的数据增强也可以迁移到其他数据集。</p>
<p>3.3  <strong>Resolution impact</strong><br>
高清（1920×1080×3）或4K（3840×2160×3）等高分辨率图像需要更多的处理和内存来训练深度CNN。然而下一代模型更倾向于使用这样更高分辨率的图像。因为模型中常用的下采样会造成图像中信息的丢失，使图像识别更困难。<br>
研究人员发现，高分辨率图像和低分辨率图像一起训练的模型集合，比单独的任何一个模型都要好。<br>
某个实验（这里就不注明引用了）在256×256图像和512×512图像上训练的模型分别获得7.96%和7.42%的top-5 error。汇总后，他们的top-5 error变低，为6.97%。<br>
随着超分辨率网络的发展，将图像放大到更高的分辨率后训练模型，能够得到更好更健壮的图像分类器。</p>
<p>3.4  <strong>Final dataset size</strong><br>
  数据增强的形式可以分为在线和离线增强。前者是在加载数据时增强，可能造成额外的内存消耗（现在都是数据容量不变的随机增强）。<br>
  此外作者提到了一个比较有意思的点：当前数据集尤其是进行增广后是十分庞大的，明显能够在一定程度上缩小数据集但是保持性能下降不多的子集效率会高得多。</p>
<p>3.5 <strong>Alleviating class imbalance with Data Augmentation</strong><br>
  这也是值得借鉴的一点。通过增强在一定程度上解决类别不平衡问题。但增强需要仔细设计，否则会面对已经学习较好的类别或者场景造成过拟合等问题。</p>
<h3 id="8-5-2-OHEM">8.5.2  OHEM</h3>
<h3 id="8-5-3-NMS：Soft-NMS-Polygon-NMS-Inclined-NMS-ConvNMS-Yes-Net-NMS-Softer-NMS">8.5.3  NMS：Soft NMS/ Polygon NMS/ Inclined NMS/ ConvNMS/ Yes-Net NMS/ Softer NMS</h3>
<h3 id="8-5-4-Multi-Scale-Training-Testing">8.5.4  Multi Scale Training/Testing</h3>
<h3 id="8-5-5-建立小物体与context的关系">8.5.5  建立小物体与context的关系</h3>
<h3 id="8-5-6-参考relation-network">8.5.6  参考relation network</h3>
<h3 id="8-5-7-结合GAN">8.5.7  结合GAN</h3>
<h3 id="8-5-8-结合attention">8.5.8  结合attention</h3>
<h3 id="8-5-9-训练tricks（贡献者：北京理工大学–明奇）">8.5.9  训练tricks（贡献者：北京理工大学–明奇）</h3>
<p>介绍一篇2019.2.4亚马逊挂在ArXiv的目标检测训练tricks的文章（之前亚马逊发了篇分类的tricks在CVPR上）</p>
<ol>
<li><strong>Introduction</strong></li>
</ol>
<p>  上次亚马逊发了个分类的训练trick在CVPR上，这次是检测的，还没发表。就没什么多说的了，下面直接介绍。先看效果如下，其实摘要声称的5%是单阶段的yolov3的提升，说明：单阶段没有RoIPooling阶段很多性质确实不如两阶段，因此采用trick很有必要；相反，两阶段本身结构优于单阶段所以外加的trick提供的如不变性等网络自身能够学习和适应就不起作用了。</p>
<p><img src="8.5.9-1.png" alt></p>
<ol start="2">
<li><strong>Bag of Freebies</strong></li>
</ol>
<p>  提出了一种基于mixup的视觉联系图像混合方法，以及一些数据处理和训练策略。</p>
<p>2.1  <strong>Visually Coherent Image Mixup for Object Detection</strong><br>
  先介绍图像分类中的mixup方法，作用是提供了训练的正则化，应用到图像上如下图，将图像作简单的像素值输入mixup的凸函数中得到合成图；然后将one-hot编码类似处理得到新的label。</p>
<p><img src="8.5.9-2.png" alt></p>
<p>  技术细节：</p>
<ul>
<li>相比于分类的resize，为了保证检测图像不畸变影响效果，作者选择直接叠加，取最大的宽高，空白进行灰度填充，不进行缩放。</li>
<li>选择ab较大（如1.5,1.5）的Beta分布作为系数来混合图像，作者说是相干性视觉图像的更强；loss是两张图像物体的loss之和，loss计算权重分别是beta分布的系数</li>
</ul>
<p><img src="8.5.9-3.png" alt></p>
<p>2.2  <strong>Classification Head Label Smoothing</strong><br>
  标签平滑在检测的分类任务常有用到，最早是Inceptionv2中提出。<br>
  如果标签中有的是错的，或者不准，会导致网络过分信任标签而一起错下去。为了提高网络泛化能力，避免这种错误，在one-hot的label进行计算loss时，真实类别位置乘以一个系数（1-e），e很小如0.05，以0.95的概率送进去；非标注的类别原来为0，现在改为e=0.05送进去计算loss。网络的优化方向不变，但是相比0-1label会更加平滑。<br>
（标签平滑这个讲的不错：<a target="_blank" rel="noopener" href="https://juejin.im/post/5a29fd4051882534af25dc92%EF%BC%89">https://juejin.im/post/5a29fd4051882534af25dc92）</a></p>
<p><img src="8.5.9-4.png" alt></p>
<p>  这里进一步改进了一下label smooth的公式而已，在原来基础上除了个类别数。</p>
<p>2.3  <strong>Data Preprocessing</strong><br>
  就是数据增强，没什么其他的。至于分类也是几何变换和色彩变换。这么分区别其实是是否变换label。但是将真实世界就这么简单地分解过于粗糙了。好不容易谷歌的增强考虑到了如何学习一下检测任务的增强，但是也只是加了bbox_only的增强，就效果而言，一般；而且就实际来说，合理性和有效性有待商榷。<br>
  作者认为，两阶段网络的RPN生成就是对输入的任意裁剪，所以这个增强就够了；这老哥膨胀了，two-stage就不用裁剪的增强，虽然两阶段能提供一些不变性，但是用了一般来说都是更好的。</p>
<p>2.4  <strong>Training Schedule Revamping</strong><br>
训练策略上：余弦学习率调整+warmup</p>
<p>2.5  <strong>Synchronized Batch Normalization</strong><br>
跨多卡同步正则化，土豪专区，穷人退避</p>
<p>2.6  <strong>Random shapes training for single-stage object detection networks</strong><br>
多尺度训练，每经过一定的iteration更换一种尺度。举例是yolov3的尺度范围。</p>
<h2 id="8-6-目标检测的常用数据集">8.6 目标检测的常用数据集</h2>
<h3 id="8-6-1-PASCAL-VOC">8.6.1 PASCAL VOC</h3>
<p>​	VOC数据集是目标检测经常用的一个数据集，自2005年起每年举办一次比赛，最开始只有4类，到2007年扩充为20个类，共有两个常用的版本：2007和2012。学术界常用5k的train/val 2007和16k的train/val 2012作为训练集，test 2007作为测试集，用10k的train/val 2007+test 2007和16k的train/val 2012作为训练集，test2012作为测试集，分别汇报结果。</p>
<h3 id="8-6-2-MS-COCO">8.6.2 MS COCO</h3>
<p>​	COCO数据集是微软团队发布的一个可以用来图像recognition+segmentation+captioning 数据集，该数据集收集了大量包含常见物体的日常场景图片，并提供像素级的实例标注以更精确地评估检测和分割算法的效果，致力于推动场景理解的研究进展。依托这一数据集，每年举办一次比赛，现已涵盖检测、分割、关键点识别、注释等机器视觉的中心任务，是继ImageNet Chanllenge以来最有影响力的学术竞赛之一。</p>
<p>相比ImageNet，COCO更加偏好目标与其场景共同出现的图片，即non-iconic images。这样的图片能够反映视觉上的语义，更符合图像理解的任务要求。而相对的iconic images则更适合浅语义的图像分类等任务。</p>
<p>​	COCO的检测任务共含有80个类，在2014年发布的数据规模分train/val/test分别为80k/40k/40k，学术界较为通用的划分是使用train和35k的val子集作为训练集（trainval35k），使用剩余的val作为测试集（minival），同时向官方的evaluation server提交结果（test-dev）。除此之外，COCO官方也保留一部分test数据作为比赛的评测集。</p>
<h3 id="8-6-3-Google-Open-Image">8.6.3 Google Open Image</h3>
<p>​	Open Image是谷歌团队发布的数据集。最新发布的Open Images V4包含190万图像、600个种类，1540万个bounding-box标注，是当前最大的带物体位置标注信息的数据集。这些边界框大部分都是由专业注释人员手动绘制的，确保了它们的准确性和一致性。另外，这些图像是非常多样化的，并且通常包含有多个对象的复杂场景（平均每个图像 8 个）。</p>
<h3 id="8-6-4-ImageNet">8.6.4 ImageNet</h3>
<p>​	ImageNet是一个计算机视觉系统识别项目， 是目前世界上图像识别最大的数据库。ImageNet是美国斯坦福的计算机科学家，模拟人类的识别系统建立的。能够从图片识别物体。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。Imagenet数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。</p>
<h3 id="8-6-5-DOTA">8.6.5 DOTA</h3>
<p>​	DOTA是遥感航空图像检测的常用数据集，包含2806张航空图像，尺寸大约为4kx4k，包含15个类别共计188282个实例，其中14个主类，small vehicle 和 large vehicle都是vehicle的子类。其标注方式为四点确定的任意形状和方向的四边形。航空图像区别于传统数据集，有其自己的特点，如：尺度变化性更大；密集的小物体检测；检测目标的不确定性。数据划分为1/6验证集，1/3测试集，1/2训练集。目前发布了训练集和验证集，图像尺寸从800x800到4000x4000不等。</p>
<h2 id="8-7-目标检测常用标注工具">8.7 目标检测常用标注工具</h2>
<h3 id="8-7-1-LabelImg">8.7.1 LabelImg</h3>
<p>​	LabelImg 是一款开源的图像标注工具，标签可用于分类和目标检测，它是用 Python 编写的，并使用Qt作为其图形界面，简单好用。注释以 PASCAL VOC 格式保存为 XML 文件，这是 ImageNet 使用的格式。 此外，它还支持 COCO 数据集格式。</p>
<h3 id="8-7-2-labelme">8.7.2 labelme</h3>
<p>​	labelme 是一款开源的图像/视频标注工具，标签可用于目标检测、分割和分类。灵感是来自于 MIT 开源的一款标注工具 LabelMe。labelme 具有的特点是：</p>
<ul>
<li>支持图像的标注的组件有：矩形框，多边形，圆，线，点（rectangle, polygons, circle, lines, points）</li>
<li>支持视频标注</li>
<li>GUI 自定义</li>
<li>支持导出 VOC 格式用于 semantic/instance segmentation</li>
<li>支出导出 COCO 格式用于 instance segmentation</li>
</ul>
<h3 id="8-7-3-Labelbox">8.7.3 Labelbox</h3>
<p>​	Labelbox 是一家为机器学习应用程序创建、管理和维护数据集的服务提供商，其中包含一款部分免费的数据标签工具，包含图像分类和分割，文本，音频和视频注释的接口，其中图像视频标注具有的功能如下：</p>
<ul>
<li>可用于标注的组件有：矩形框，多边形，线，点，画笔，超像素等（bounding box, polygons, lines, points，brush, subpixels）</li>
<li>标签可用于分类，分割，目标检测等</li>
<li>以 JSON / CSV / WKT / COCO / Pascal VOC 等格式导出数据</li>
<li>支持 Tiled Imagery (Maps)</li>
<li>支持视频标注 （快要更新）</li>
</ul>
<h3 id="8-7-4-RectLabel">8.7.4 RectLabel</h3>
<p>​	RectLabel 是一款在线免费图像标注工具，标签可用于目标检测、分割和分类。具有的功能或特点：</p>
<ul>
<li>可用的组件：矩形框，多边形，三次贝塞尔曲线，直线和点，画笔，超像素</li>
<li>可只标记整张图像而不绘制</li>
<li>可使用画笔和超像素</li>
<li>导出为YOLO，KITTI，COCO JSON和CSV格式</li>
<li>以PASCAL VOC XML格式读写</li>
<li>使用Core ML模型自动标记图像</li>
<li>将视频转换为图像帧</li>
</ul>
<h3 id="8-7-5-CVAT">8.7.5 CVAT</h3>
<p>​	CVAT 是一款开源的基于网络的交互式视频/图像标注工具，是对加州视频标注工具（Video Annotation Tool） 项目的重新设计和实现。OpenCV团队正在使用该工具来标注不同属性的数百万个对象，许多 UI 和 UX 的决策都基于专业数据标注团队的反馈。具有的功能</p>
<ul>
<li>关键帧之间的边界框插值</li>
<li>自动标注（使用TensorFlow OD API 和 Intel OpenVINO IR格式的深度学习模型）</li>
</ul>
<h3 id="8-7-6-VIA">8.7.6 VIA</h3>
<p>​	VGG Image Annotator（VIA）是一款简单独立的手动注释软件，适用于图像，音频和视频。 VIA 在 Web 浏览器中运行，不需要任何安装或设置。 页面可在大多数现代Web浏览器中作为离线应用程序运行。</p>
<ul>
<li>支持标注的区域组件有：矩形，圆形，椭圆形，多边形，点和折线</li>
</ul>
<h3 id="8-7-6-其他标注工具">8.7.6 其他标注工具</h3>
<p>​	liblabel，一个用 MATLAB 写的轻量级 语义/示例(semantic/instance) 标注工具。<br>
ImageTagger：一个开源的图像标注平台。<br>
Anno-Mage：一个利用深度学习模型半自动图像标注工具，预训练模型是基于MS COCO数据集，用 RetinaNet 训练的。</p>
<br>
​	当然还有一些数据标注公司，可能包含更多标注功能，例如对三维目标检测的标注（3D Bounding box Labelling），激光雷达点云的标注（LIDAR 3D Point Cloud Labeling）等。
<h2 id="8-8-目标检测工具和框架（贡献者：北京理工大学–明奇）">8.8 目标检测工具和框架（贡献者：北京理工大学–明奇）</h2>
<p>各种不同的算法虽然部分官方会有公布代码，或者github上有人复现，但是囿于安装环境不一，实现的框架（pytorch、C++、Caffee、tensorflow、MXNet等）不同，每次更换算法都需要重新安装环境，并且代码之间的迁移性差，十分不方便。所以为了方便将不同的算法统一在一个代码库中，不同的大厂都提出了自己的解决方案。如facebook的Detectron、商汤科技的mmdetection、SimpleDet等。其中Detectron最早，所以用户量最大，其次是国内近段时间崛起的mmdetection，下面介绍该目标检测工具箱。</p>
<ol>
<li><strong>Introduction</strong><br>
MMdetection的特点：</li>
</ol>
<ul>
<li>模块化设计：将不同网络的部分进行切割，模块之间具有很高的复用性和独立性（十分便利，可以任意组合）</li>
<li>高效的内存使用</li>
<li>SOTA</li>
</ul>
<ol start="2">
<li><strong>Support Frameworks</strong></li>
</ol>
<ul>
<li>
<p>单阶段检测器<br>
SSD、RetinaNet、FCOS、FSAF</p>
</li>
<li>
<p>两阶段检测器<br>
Faster R-CNN、R-FCN、Mask R-CNN、Mask Scoring R-CNN、Grid R-CNN</p>
</li>
<li>
<p>多阶段检测器<br>
Cascade R-CNN、Hybrid Task Cascade</p>
</li>
<li>
<p>通用模块和方法<br>
soft-NMS、DCN、OHEN、Train from Scratch 、M2Det 、GN 、HRNet 、Libra R-CNN</p>
</li>
</ul>
<ol start="3">
<li><strong>Architecture</strong></li>
</ol>
<p>模型表征：划分为以下几个模块：<br>
Backbone（ResNet等）、Neck（FPN）、DenseHead（AnchorHead）、RoIExtractor、RoIHead（BBoxHead/MaskHead）<br>
结构图如下：<br>
<img src="mmdetection.png" alt></p>
<ol start="4">
<li><strong>Notice</strong></li>
</ol>
<ul>
<li>1x代表12epoch的COCO训练，2x类似推导</li>
<li>由于batch-size一般比较小（1/2这样的量级），所以大多数地方默认冻结BN层。可以使用GN代替。</li>
</ul>
<ol start="5">
<li><strong>参考链接</strong><br>
mmdetection代码高度模块化，十分好用和便利，更详细的文档直接参见官方文档：<br>
<a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetection</a></li>
</ol>
<p>注释版的mmdetection代码（更新至v1.0.0）：<a target="_blank" rel="noopener" href="https://github.com/ming71/mmdetection-annotated">https://github.com/ming71/mmdetection-annotated</a></p>
<p>使用方法简介：<br>
安装记录（可能过时，以官方文档为准）：<a target="_blank" rel="noopener" href="https://ming71.github.io/mmdetection-memo.html">https://ming71.github.io/mmdetection-memo.html</a><br>
使用方法（截止更新日期，如果过时以官方为准）：<a target="_blank" rel="noopener" href="https://ming71.github.io/mmdetection-instruction.html">https://ming71.github.io/mmdetection-instruction.html</a></p>
<h2 id="TODO">TODO</h2>
<ul>
<li>[ ] 目标检测基础知识：mAP、IoU和NMS等</li>
<li>[ ] 目标检测评测指标</li>
<li>[ ] 目标检测常见标注工具</li>
<li>[ ] 完善目标检测的技巧汇总</li>
<li>[ ] 目标检测的现在难点和未来发展</li>
</ul>
<h2 id="参考文献">参考文献</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/amusi/awesome-object-detection">https://github.com/amusi/awesome-object-detection</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/hoya012/deep_learning_object_detection">https://github.com/hoya012/deep_learning_object_detection</a></p>
<p><a target="_blank" rel="noopener" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html">https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/272322209/answer/482922713">https://www.zhihu.com/question/272322209/answer/482922713</a></p>
<p><a target="_blank" rel="noopener" href="http://blog.leanote.com/post/afanti.deng@gmail.com/b5f4f526490b">http://blog.leanote.com/post/afanti.deng@gmail.com/b5f4f526490b</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/hw5226349/article/details/78987385">https://blog.csdn.net/hw5226349/article/details/78987385</a></p>
<p>[1] Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 580-587.</p>
<p>[2] Girshick R. Fast r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1440-1448.</p>
<p>[3] He K, Zhang X, Ren S, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE transactions on pattern analysis and machine intelligence, 2015, 37(9): 1904-1916.</p>
<p>[4] Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99.</p>
<p>[5] Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2117-2125.</p>
<p>[6] He K, Gkioxari G, Dollár P, et al. Mask r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2961-2969.</p>
<p>[7] Liu W, Anguelov D, Erhan D, et al. Ssd: Single shot multibox detector[C]//European conference on computer vision. Springer, Cham, 2016: 21-37.</p>
<p>[8] Fu C Y, Liu W, Ranga A, et al. Dssd: Deconvolutional single shot detector[J]. arXiv preprint arXiv:1701.06659, 2017.</p>
<p>[9] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 779-788.</p>
<p>[10] Redmon J, Farhadi A. YOLO9000: better, faster, stronger[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7263-7271.</p>
<p>[11] Redmon J, Farhadi A. Yolov3: An incremental improvement[J]. arXiv preprint arXiv:1804.02767, 2018.</p>
<p>[12] Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2980-2988.</p>
<p>[13] Liu S, Huang D. Receptive field block net for accurate and fast object detection[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 385-400.</p>
<p>[14] Zhao Q, Sheng T, Wang Y, et al. M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network[J]. arXiv preprint arXiv:1811.04533, 2018</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/leetcode/%E5%B0%8F%E6%8A%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/leetcode/%E5%B0%8F%E6%8A%84/" class="post-title-link" itemprop="url">小抄</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-20 22:47:23" itemprop="dateModified" datetime="2024-03-20T22:47:23+08:00">2024-03-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LeetCode/" itemprop="url" rel="index"><span itemprop="name">LeetCode</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LeetCode/%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/" itemprop="url" rel="index"><span itemprop="name">题目汇总</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>快速初始化方法</h1>
<h2 id="zip用于旋转">zip用于旋转</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>(<span class="built_in">zip</span>(*a))[::-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>二维数组这么旋转最快</p>
<h2 id="高级乘法">高级乘法</h2>
<p>a&gt;&gt;1 表示a//2<br>
a&lt;&lt;1 表示a*2</p>
<h2 id="判断是否为数字等">判断是否为数字等</h2>
<p>i.isalnnum()是否是数字或字符串<br>
i.isalpha()判断是否字母<br>
isdigit函数判断是否数字<br>
isdecimal() 方法检查字符串是否只包含十进制字符这种方法只存在于unicode对象。</p>
<p><a target="_blank" rel="noopener" href="https://www.runoob.com/python/att-string-isdecimal.html">https://www.runoob.com/python/att-string-isdecimal.html</a></p>
<h2 id="负数求补码">负数求补码</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x &amp; <span class="number">0xffffffff</span></span><br></pre></td></tr></table></figure>
<p>补码如何变成一个数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~(a ^ x)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/solution/pythonjie-fa-xiang-xi-jie-du-wei-yun-sua-jrk8/">https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/solution/pythonjie-fa-xiang-xi-jie-du-wei-yun-sua-jrk8/</a></p>
<h2 id="不能返回数字用于后续对的判断">不能返回数字用于后续对的判断</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> contiions1:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>不能用该函数的结果给后面的代码做判断，万一输出的是0，但是这个0是有意义的，那你这么判断是有问题的。</p>
<h2 id="获取数字的二进制">获取数字的二进制</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">bin</span>(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h2 id="获取ascill码的转换">获取ascill码的转换</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">chr</span>(<span class="number">104</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h2 id="list和str赋值">list和str赋值</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="string">&quot;123&quot;</span>,<span class="string">&quot;456&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a)):</span><br><span class="line">    a[i] = a[i] + <span class="string">&#x27;_&#x27;</span></span><br><span class="line">可以得到[<span class="string">&quot;123_&quot;</span>,<span class="string">&quot;456_&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">    i = i + <span class="string">&quot;_&quot;</span></span><br><span class="line">得到[<span class="string">&quot;123&quot;</span>,<span class="string">&quot;456&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>只能对list中的值修改，不能对str进行修改</p>
<h2 id="快速间隔取数">快速间隔取数</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">43</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line">nums[<span class="number">1</span>::<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<h2 id="快速求余">快速求余</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def remainder(x, a, p):</span><br><span class="line">    rem = 1</span><br><span class="line">    for _ in range(a):</span><br><span class="line">        rem = (rem * x) % p</span><br><span class="line">    return rem</span><br><span class="line"></span><br><span class="line"># 求 (x^a) % p —— 快速幂求余</span><br><span class="line">def remainder(x, a, p):</span><br><span class="line">    rem = 1</span><br><span class="line">    while a &gt; 0:</span><br><span class="line">        if a % 2: rem = (rem * x) % p</span><br><span class="line">        x = x ** 2 % p</span><br><span class="line">        a //= 2</span><br><span class="line">    return rem</span><br></pre></td></tr></table></figure>
<p>参考 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/jian-sheng-zi-ii-lcof/solution/mian-shi-ti-14-ii-jian-sheng-zi-iitan-xin-er-fen-f/">https://leetcode-cn.com/problems/jian-sheng-zi-ii-lcof/solution/mian-shi-ti-14-ii-jian-sheng-zi-iitan-xin-er-fen-f/</a></p>
<h2 id="快速求进位数">快速求进位数</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(x&gt;&gt;10)&amp;1</span><br><span class="line">看x的第10位是啥</span><br></pre></td></tr></table></figure>
<h2 id="快速轮询">快速轮询</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">i%4表示以4作为循环</span><br><span class="line">for i in range(100):</span><br><span class="line">    res[i%4] += i</span><br><span class="line"></span><br><span class="line"># 循环访问数组</span><br><span class="line">res = 0</span><br><span class="line">week = [1,2,3,4,5,6,7]</span><br><span class="line">for i in range(n):</span><br><span class="line">    res = res + week[i%7] + i//7</span><br><span class="line">return res</span><br></pre></td></tr></table></figure>
<h2 id="快速判断奇偶">快速判断奇偶</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x&amp;1</span><br><span class="line">为1表示奇数</span><br></pre></td></tr></table></figure>
<h2 id="快速获取间隔的数组">快速获取间隔的数组</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]</span><br><span class="line">a[<span class="number">1</span>:<span class="number">10</span>:<span class="number">3</span>] 每间隔<span class="number">3</span></span><br></pre></td></tr></table></figure>
<h2 id="快速计算斐波那契数列">快速计算斐波那契数列</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">math.factorial(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="快速求中位数">快速求中位数</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 奇数</span></span><br><span class="line">n = <span class="built_in">len</span>(nums1)</span><br><span class="line"><span class="keyword">if</span> n &amp; <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">return</span> nums1[(n-<span class="number">1</span>)//<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 偶数</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> (nums1[(n-<span class="number">1</span>)//<span class="number">2</span>] + nums1[(n-<span class="number">1</span>)//<span class="number">2</span> + <span class="number">1</span>])/<span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="快速循环一个序列">快速循环一个序列</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    <span class="built_in">print</span>(nums[i])</span><br><span class="line">    index = (i + <span class="number">1</span>) % <span class="built_in">len</span>(nums)</span><br><span class="line">    <span class="keyword">while</span> index != i:</span><br><span class="line">        <span class="built_in">print</span>(nums[index])</span><br><span class="line">        index = (index + <span class="number">1</span>) % <span class="built_in">len</span>(nums)</span><br></pre></td></tr></table></figure>
<p>加油站[134]那道题</p>
<h2 id="快速判断是否为素数">快速判断是否为素数</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">is_prime</span>(<span class="params">n</span>):</span><br><span class="line">		<span class="keyword">return</span> n &gt;= <span class="number">2</span> <span class="keyword">and</span> <span class="built_in">all</span>(n%i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">int</span>(n**<span class="number">0.5</span>) + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h2 id="for也可以这么搞">for也可以这么搞</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        xxx</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        xxx</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    xxxx</span><br></pre></td></tr></table></figure>
<h2 id="排序和字典排序">排序和字典排序</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums.sort()</span><br><span class="line">t = list(zip(numn1,nu8m2))</span><br><span class="line">t.sort(key=lambda x:x[0])</span><br></pre></td></tr></table></figure>
<p>字典排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">cnt = <span class="built_in">list</span>(Counter(nums).items())</span><br><span class="line">cnt.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>或者使用defaultdict来排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">d = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">    d[i] = d[i] + <span class="number">1</span></span><br><span class="line">d_list = <span class="built_in">list</span>(d.items())</span><br><span class="line">d_list.sort(key=<span class="keyword">lambda</span> x: -x[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(d_list)</span><br></pre></td></tr></table></figure>
<h2 id="快速计算除数和余数">快速计算除数和余数</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a,b=<span class="built_in">divmod</span>(<span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">a=<span class="number">3</span></span><br><span class="line">b=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="快速复制">快速复制</h2>
<p>a = [1,2,3]<br>
b=a 这样的话a变了的话，b也会变<br>
b=a[:] 这样就不会了</p>
<p>b=a=0<br>
这样不会有问题的</p>
<h2 id="连续求和快速初始化">连续求和快速初始化</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">continueSum</span>(<span class="params">nums</span>):</span><br><span class="line">    target = <span class="number">9</span></span><br><span class="line">    c = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, target))</span><br><span class="line">    sums = [c[<span class="number">0</span>]]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(c)):</span><br><span class="line">        sums.append(c[i] + sums[-<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span>(sums)</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sums)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(sums)):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                val = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                val = sums[i - <span class="number">1</span>]</span><br><span class="line">            <span class="built_in">print</span>(i, j, sums[j] - val)</span><br><span class="line">continueSum([<span class="number">9</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>
<p>如果要求i-j的和，则必须sums[j] - sum[i-1]</p>
<h2 id="数组双循环">数组双循环</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">        <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure>
<h2 id="快速前缀和初始化">快速前缀和初始化</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">13</span>]</span><br><span class="line">prefix = [<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">    prefix.append(prefix[-<span class="number">1</span>] + i)</span><br><span class="line"><span class="comment"># 求解 i-j的值</span></span><br><span class="line"><span class="built_in">print</span>(prefix[j+<span class="number">1</span>]-prefix[i])</span><br></pre></td></tr></table></figure>
<p>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i, <span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">if</span> (prefix[j + <span class="number">1</span>] - prefix[i]) &gt;= lower <span class="keyword">and</span> (prefix[j + <span class="number">1</span>] - prefix[i]) &lt;= upper:</span><br><span class="line">            <span class="built_in">print</span>(i, j)</span><br><span class="line">            cnt += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>计算两个索引之间的差值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">arr = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">predix = [<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">    predix.append(predix[-<span class="number">1</span>] + i)</span><br><span class="line">res = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i, <span class="built_in">len</span>(arr)):</span><br><span class="line">        <span class="built_in">print</span>(i, j)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/count-of-range-sum/submissions/">https://leetcode-cn.com/problems/count-of-range-sum/submissions/</a></p>
<h2 id="快速累积和">快速累积和</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> accumulate</span><br><span class="line"></span><br><span class="line">nums = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>]</span><br><span class="line">res = <span class="built_in">list</span>(accumulate(nums))</span><br><span class="line"><span class="built_in">print</span>(res)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="动态规划初始化">动态规划初始化</h2>
<h3 id="初始化动态规划数组">初始化动态规划数组</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp = [[<span class="number">0</span>]* <span class="number">5</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)]</span><br></pre></td></tr></table></figure>
<p>注意，有的时候，行数和列数是不一样的，需要这样初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">2</span></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">dp = [[<span class="number">0</span>] * m <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line"><span class="built_in">print</span>(dp)</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<p>其中m为列数，n为行的数目</p>
<p>初始化3维的如下，这点在股票交易中常用到的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成一个3个2 x 2的矩阵</span></span><br><span class="line">dp = [[[<span class="number">0</span>] * <span class="number">2</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="多种遍历方法">多种遍历方法</h3>
<p>右上矩阵遍历</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - l):</span><br><span class="line">        j = l + i</span><br><span class="line">        <span class="built_in">print</span>(i, j)</span><br></pre></td></tr></table></figure>
<p>其它的遍历方方法的话，可以看连接<br>
<a target="_blank" rel="noopener" href="https://blog.csdn.net/miyagiSimple/article/details/110561865">https://blog.csdn.net/miyagiSimple/article/details/110561865</a><br>
大部分的情况下，使用横向的遍历就可以了</p>
<h2 id="二分查找Bisect">二分查找Bisect</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用bisect模块，使用如下</span></span><br><span class="line"><span class="keyword">import</span> bisect</span><br><span class="line">arr = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">12</span>,<span class="number">15</span>]</span><br><span class="line">value = <span class="number">3</span></span><br><span class="line">idx_left=bisect.bisect_left(arr,value) <span class="comment"># 结果1</span></span><br><span class="line">idx_right=bisect.bisect_right(arr,value) <span class="comment"># 结果3</span></span><br><span class="line">bisect.insort(arr,<span class="number">13</span>)</span><br></pre></td></tr></table></figure>
<p>注意哈，可能会有搜索到的值的索引在最后一个，这就需要判断index是不是&gt;=len(Nums)如果是大于的话，那就要报错了</p>
<p>二分查找的边界条件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(bisect.bisect_left(nums,<span class="number">12</span>))</span><br><span class="line"><span class="comment"># 得到的值为5，但是没有5的索引的，因此会报错</span></span><br><span class="line">index = bisect.bisect_left(array2, i + diff // <span class="number">2</span>)</span><br><span class="line">index = <span class="built_in">min</span>(<span class="built_in">len</span>(array2)-<span class="number">1</span>, index)</span><br><span class="line"><span class="keyword">if</span> array2[index] == i + diff // <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span> [i, i + diff // <span class="number">2</span>]</span><br><span class="line"><span class="comment"># 通过将值插入后，判断位置，如果是=len()的话，那直接为len()-1。</span></span><br></pre></td></tr></table></figure>
<p>二分查找中边界的设置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">res = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">    index = bisect.bisect_left(b, i)</span><br><span class="line">    <span class="keyword">if</span> index==<span class="built_in">len</span>(b):</span><br><span class="line">        diff = <span class="built_in">abs</span>(i-b[index-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">elif</span> index == <span class="number">0</span>:</span><br><span class="line">        diff = <span class="built_in">abs</span>(i - b[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">elif</span> index&gt;<span class="number">0</span>:</span><br><span class="line">        diff = <span class="built_in">min</span>(<span class="built_in">abs</span>(i-b[index-<span class="number">1</span>]), <span class="built_in">abs</span>(i-b[index]))</span><br><span class="line">    res = <span class="built_in">min</span>(diff, res)</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="堆模块heapq">堆模块heapq</h2>
<p>堆的队列，称为优先队列，通常是一个小顶锥，每次维护的头部是最小值。因此可以用来解决topK最大值问题。注意想一下，为啥是一个小顶锥，可以用来解决最大值问题。</p>
<p>注意：heapq.pop是弹出第一个元素，也就是最小的那个</p>
<h3 id="最大topk">最大topk</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最大topk问题</span></span><br><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bigestK</span>(<span class="params">arr, k</span>):</span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        l = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(l) &lt; k:</span><br><span class="line">                heapq.heappush(l, i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> i &gt; l[<span class="number">0</span>]:<span class="comment"># 注意点</span></span><br><span class="line">                    heapq.heappop(l)</span><br><span class="line">                    heapq.heappush(l, i)</span><br><span class="line">    <span class="keyword">return</span> l</span><br><span class="line">a = bigestK([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>], <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<p>可以看到，heapq就是对一个list做操作而已，因此。最后动的还是列表。<br>
可以简化写成如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for i in nums:</span><br><span class="line">    if len(res) &gt;= k:</span><br><span class="line">        if i &gt; res[0]:</span><br><span class="line">            heapq.heappop(res)</span><br><span class="line">    else:</span><br><span class="line">        heapq.heappush(res, i)</span><br></pre></td></tr></table></figure>
<h3 id="最小topk">最小topk</h3>
<p>题目见<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/smallest-k-lcci/">https://leetcode-cn.com/problems/smallest-k-lcci/</a><br>
对于最小的topk问题解法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最小tooK问题</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">smallestK</span>(<span class="params">arr, k</span>):</span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        l = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(l) &lt; k:</span><br><span class="line">                heapq.heappush(l, -i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> -i &gt; l[<span class="number">0</span>]: <span class="comment"># 注意点</span></span><br><span class="line">                    heapq.heappop(l)</span><br><span class="line">                    heapq.heappush(l, -i)</span><br><span class="line">    <span class="keyword">return</span> [-i <span class="keyword">for</span> i <span class="keyword">in</span> l]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = smallestK([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>], <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<h3 id="滑动窗口最大值">滑动窗口最大值</h3>
<p>解决滑动窗口最大值，题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/sliding-window-maximum/">https://leetcode-cn.com/problems/sliding-window-maximum/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxSlidingWindow2</span>(<span class="params">nums, k</span>):</span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ans = []</span><br><span class="line">        l = []</span><br><span class="line">        <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            <span class="keyword">if</span> i &lt; k: <span class="comment"># 别写错</span></span><br><span class="line">                heapq.heappush(l, (-j, i))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">while</span> l <span class="keyword">and</span> l[<span class="number">0</span>][<span class="number">1</span>] &lt;= i - k:<span class="comment"># 别写错</span></span><br><span class="line">                    heapq.heappop(l)</span><br><span class="line">                heapq.heappush(l, (-j, i))</span><br><span class="line">            <span class="keyword">if</span> i &gt;= k - <span class="number">1</span>:<span class="comment"># 别写错</span></span><br><span class="line">                ans.append(-l[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<p>这里注意为啥要取-的呢，因为我们只取一个数值，不会取前K个。<br>
简单写法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxSlidingWindow3</span>(<span class="params">nums, k</span>):</span><br><span class="line">    hp, ret = [], []</span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">        <span class="keyword">while</span> hp <span class="keyword">and</span> hp[<span class="number">0</span>][<span class="number">1</span>] &lt;= i - k:</span><br><span class="line">            heapq.heappop(hp)</span><br><span class="line">        heapq.heappush(hp, [-j, i])</span><br><span class="line">        <span class="keyword">if</span> i &gt;= k - <span class="number">1</span>:</span><br><span class="line">            ret.append(-hp[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>
<h2 id="单调栈">单调栈</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">heights = [2, 1, 5, 6, 2, 3]</span><br><span class="line">stack = []</span><br><span class="line">n = len(heights)</span><br><span class="line">right_min = [n] * n</span><br><span class="line">for i in range(n):</span><br><span class="line">    while stack and stack[-1][0] &gt; heights[i]:</span><br><span class="line">        val = stack.pop()</span><br><span class="line">        right_min[val[1]] = i</span><br><span class="line">    stack.append((heights[i], i))</span><br><span class="line"># 求出右边最大的也是可以的</span><br></pre></td></tr></table></figure>
<h2 id="队列queue">队列queue</h2>
<p>解决滑动窗口最大值，题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/sliding-window-maximum/">https://leetcode-cn.com/problems/sliding-window-maximum/</a><br>
解法如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxSlidingWindow</span>(<span class="params">nums, k</span>):</span><br><span class="line">    q, ret = deque(), []</span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">        <span class="keyword">while</span> q <span class="keyword">and</span> nums[q[-<span class="number">1</span>]] &lt; j:</span><br><span class="line">            q.pop()</span><br><span class="line">        <span class="keyword">if</span> q <span class="keyword">and</span> q[<span class="number">0</span>] &lt;= i - k:</span><br><span class="line">            q.popleft()</span><br><span class="line">        q.append(i)</span><br><span class="line">        <span class="keyword">if</span> i &gt;= k - <span class="number">1</span>:</span><br><span class="line">            ret.append(nums[q[<span class="number">0</span>]])</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>
<p>队列在使用中需要注意，需要判断队列是否为空，如在56 合并区间这道题中，如果你需要用队列的话，需要变成这样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index, value <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> q <span class="keyword">or</span> value[<span class="number">0</span>]&gt;a[-<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">and</span> value[<span class="number">0</span>]&lt;q[-<span class="number">1</span>][<span class="number">1</span>]:</span><br><span class="line">        q.append(value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        results.append(<span class="built_in">list</span>(q)</span><br></pre></td></tr></table></figure>
<h2 id="collection常用函数">collection常用函数</h2>
<h3 id="defaultdict">defaultdict</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from collections import defaultdict</span><br><span class="line">d = defaultdict(int)</span><br><span class="line">for i in a:</span><br><span class="line">    d[i] = d[i] + 1</span><br></pre></td></tr></table></figure>
<h3 id="Counter">Counter</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"><span class="keyword">from</span>  collections <span class="keyword">import</span> Counter</span><br><span class="line">ct = Counter(a)</span><br><span class="line"><span class="built_in">print</span>(ct)</span><br></pre></td></tr></table></figure>
<p>如果一个数字不在其中，则输出结果为0，如ct[6]</p>
<h2 id="deque">deque</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">d = deque()</span><br><span class="line">d.append(<span class="number">1</span>)</span><br><span class="line">d.append(<span class="number">2</span>)</span><br><span class="line">d.popleft()</span><br><span class="line">d.pop()</span><br></pre></td></tr></table></figure>
<h2 id="回溯结构">回溯结构</h2>
<h3 id="分割字符串">分割字符串</h3>
<p>将&quot;abc&quot;分割为[a,b,c],[ab,c],[abc],[a,bc]…<br>
代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seg_str</span>(<span class="params">s</span>):</span><br><span class="line">    res = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">back</span>(<span class="params">state, s</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(s) == <span class="number">0</span>:</span><br><span class="line">            res.append(state[:])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">                state.append(s[:i + <span class="number">1</span>])</span><br><span class="line">                back(state, s[i + <span class="number">1</span>:])</span><br><span class="line">                state.pop()</span><br><span class="line"></span><br><span class="line">    back([], s)</span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<h3 id="随机组合元素">随机组合元素</h3>
<p>将[1,2,3,4]变为[1],[1,2,3],[1,2,4],[3,4],[3],[3,4,5],[3,5],…<br>
代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">com_seq</span>(<span class="params">nums</span>):</span><br><span class="line">    res = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">back</span>(<span class="params">state, s</span>):</span><br><span class="line">        res.append(state[:])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            state.append(s[i])</span><br><span class="line">            back(state, s[i + <span class="number">1</span>:])</span><br><span class="line">            state.pop()</span><br><span class="line"></span><br><span class="line">    back([], nums)</span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<p>这里注意在back中没使用到if条件，因为不需要使用到if条件，如果来了一句</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>这点在单词拆分 II中用到了，可以看下。</p>
<h2 id="快速访问二维的list">快速访问二维的list</h2>
<p>下面介绍下访问二维的list的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">matrix = [[<span class="string">&quot;1&quot;</span>, <span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;0&quot;</span>, <span class="string">&quot;0&quot;</span>], [<span class="string">&quot;1&quot;</span>, <span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;1&quot;</span>], [<span class="string">&quot;1&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;1&quot;</span>], [<span class="string">&quot;1&quot;</span>, <span class="string">&quot;0&quot;</span>, <span class="string">&quot;0&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;0&quot;</span>]]</span><br><span class="line">m = <span class="built_in">len</span>(matrix)</span><br><span class="line">n = <span class="built_in">len</span>(matrix[<span class="number">0</span>])</span><br><span class="line">max_k = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">max</span>(m - i, n - i)):</span><br><span class="line">            c = [i[j:j + k] <span class="keyword">for</span> i <span class="keyword">in</span> matrix[i:i + k]]</span><br><span class="line">            <span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>
<h2 id="初始化结果表">初始化结果表</h2>
<p>我们在很多情况下，都会要保持结果，如果我们定义了res=[]和res=[0]*n<br>
这两张方式，哪种会更好呢，答案是res=[0]*n。具体可以看特殊数据结构这里的每日温度这道题。如果我们用[]的话，每次都要往里面添加数据，可能有的时候回漏掉数据，但是第二种方式就不会。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">res1 = []</span><br><span class="line">res1 = [<span class="number">0</span>] * <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        res1.append(i)</span><br><span class="line">        res1[i] = <span class="number">4</span></span><br></pre></td></tr></table></figure>
<p>上述得到的res1和res2结果是不一样的。</p>
<h1>常见必备基础算法</h1>
<h2 id="快速幂">快速幂</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">myPow</span>(<span class="params">self, x: <span class="built_in">float</span>, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="keyword">if</span> x == <span class="number">0.0</span>: <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line">        res = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">0</span>: x, n = <span class="number">1</span> / x, -n</span><br><span class="line">        <span class="keyword">while</span> n:</span><br><span class="line">            <span class="keyword">if</span> n &amp; <span class="number">1</span>: res *= x</span><br><span class="line">            x *= x</span><br><span class="line">            n &gt;&gt;= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="字典序">字典序</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lexicalOrder</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        ans = []</span><br><span class="line">        num = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(ans) &lt; n:</span><br><span class="line">            <span class="keyword">while</span> num &lt;= n:  <span class="comment"># 不断进入下一层</span></span><br><span class="line">                ans.append(num)</span><br><span class="line">                num *= <span class="number">10</span></span><br><span class="line">            <span class="keyword">while</span> num % <span class="number">10</span> == <span class="number">9</span> <span class="keyword">or</span> num &gt; n:  <span class="comment"># 不断返回上一层</span></span><br><span class="line">                num //= <span class="number">10</span></span><br><span class="line">            num += <span class="number">1</span>  <span class="comment"># 遍历该层下一个数</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h2 id="bisect快速赋值">bisect快速赋值</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">a[<span class="number">0</span>:<span class="number">0</span>] = [<span class="number">0</span>] 得到结果[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">a[<span class="number">0</span>:<span class="number">1</span>] = [] 得到[<span class="number">2</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<p>这样的效率会高很多</p>
<h2 id="快速定位数据的位数">快速定位数据的位数</h2>
<p>我们自做1011121314这种题目是，问道你n个数字对应的数值是多少时，可以通过如下简单的方式进行访问。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">digit = <span class="number">2</span></span><br><span class="line">n = <span class="number">5</span></span><br><span class="line">nums = <span class="number">10</span> + (n-<span class="number">1</span>)//digit</span><br><span class="line">v = <span class="built_in">str</span>(nums)[(n-<span class="number">1</span>)%digit]</span><br></pre></td></tr></table></figure>
<p>这里你也可以通过如下的方式来访问，不过很慢的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nums = <span class="number">10</span> + n//digit - <span class="number">1</span></span><br><span class="line">index = last_nums % digit</span><br><span class="line"><span class="keyword">if</span> index==<span class="number">0</span>:</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">str</span>(nums)[-<span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">str</span>(nums+<span class="number">1</span>)[index-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>这样也可以不过很蛮烦的。</p>
<h2 id="获取最长递增子序列">获取最长递增子序列</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stk = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> posAr:</span><br><span class="line">    <span class="keyword">if</span> stk <span class="keyword">and</span> x &lt;= stk[-<span class="number">1</span>]:</span><br><span class="line">        idx = bisect_left(stk, x)</span><br><span class="line">        stk[idx] = x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        stk.append(x)</span><br></pre></td></tr></table></figure>
<p>更简答的方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">stk = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> posAr:</span><br><span class="line">    pos = bisect.bisect_left(stk, x)</span><br><span class="line">    stk[pos: pos + <span class="number">1</span>] = [x]</span><br></pre></td></tr></table></figure>
<p>注意这里不是stk[pos:pos]</p>
<h2 id="列表快速插入和替换">列表快速插入和替换</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">a[<span class="number">1</span>:<span class="number">1</span>] = [<span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a[<span class="number">3</span>:<span class="number">4</span>] = []</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a[<span class="number">1</span>:<span class="number">2</span>] = [<span class="number">8</span>]</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">[<span class="number">1</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure>
<h2 id="埃及筛">埃及筛</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sign = [<span class="number">1</span>] * <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>*i, <span class="number">101</span>, i):</span><br><span class="line">        sign[j] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/leetcode/%E4%BA%8C%E5%8F%89%E6%A0%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/leetcode/%E4%BA%8C%E5%8F%89%E6%A0%91/" class="post-title-link" itemprop="url">二叉树</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-20 22:47:23" itemprop="dateModified" datetime="2024-03-20T22:47:23+08:00">2024-03-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LeetCode/" itemprop="url" rel="index"><span itemprop="name">LeetCode</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LeetCode/%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/" itemprop="url" rel="index"><span itemprop="name">题目汇总</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>总结</h1>
<h2 id="大纲">大纲</h2>
<p><img src="https://note.youdao.com/yws/res/9196/WEBRESOURCE074bd40e4844194049d30d776bac9322" alt="微信截图_20231210121917.png"></p>
<h2 id="相关细节">相关细节</h2>
<ol>
<li>夹在state.append和dfs(root, state)中间的判断是否target_sum正确的时候，不需要加return，不然会导致运行结果的问题。</li>
</ol>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">binaryTreePaths</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root, state</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            state.append(root.val)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">                res.append(state[:]) <span class="comment"># 这里不要return</span></span><br><span class="line">            dfs(root.left, state)</span><br><span class="line">            dfs(root.right, state)</span><br><span class="line">            state.pop()</span><br><span class="line">        dfs(root, [])</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;-&gt;&quot;</span>.join([<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> s]) <span class="keyword">for</span> s <span class="keyword">in</span> res]</span><br></pre></td></tr></table></figure>
<h1>遍历操作</h1>
<h2 id="介绍">介绍</h2>
<p>树结构如下<br>
<img src="https://upload-images.jianshu.io/upload_images/2405011-5f5b0b136713f744.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" width="50%" height="auto"></p>
<p>先序：1 2 4 6 7 8 3 5<br>
中序：4 7 6 8 2 1 3 5<br>
后序：7 8 6 4 2 5 3 1</p>
<h2 id="递归">递归</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TreeNode</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val=<span class="number">0</span>, left=<span class="literal">None</span>, right=<span class="literal">None</span></span>):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root = TreeNode(<span class="number">1</span>, left=TreeNode(<span class="number">2</span>, left=TreeNode(<span class="number">3</span>), right=TreeNode(<span class="number">4</span>)),</span><br><span class="line">                right=TreeNode(<span class="number">5</span>, left=TreeNode(<span class="number">6</span>), right=TreeNode(<span class="number">7</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前序遍历</span></span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preOrder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    res.append(root.val)</span><br><span class="line">    preOrder(root.left)</span><br><span class="line">    preOrder(root.right)</span><br><span class="line">preOrder(root)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前序结果是：&quot;</span>, res)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中序遍历</span></span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">inOrder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    inOrder(root.left)</span><br><span class="line">    res.append(root.val)</span><br><span class="line">    inOrder(root.right)</span><br><span class="line">inOrder(root)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;中序结果是：&quot;</span>, res)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 后序遍历</span></span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">postOrder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    postOrder(root.left)</span><br><span class="line">    postOrder(root.right)</span><br><span class="line">    res.append(root.val)</span><br><span class="line">postOrder(root)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;后序结果是：&quot;</span>, res)</span><br></pre></td></tr></table></figure>
<h2 id="迭代">迭代</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TreeNode</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val=<span class="number">0</span>, left=<span class="literal">None</span>, right=<span class="literal">None</span></span>):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root = TreeNode(<span class="number">1</span>, left=TreeNode(<span class="number">2</span>, left=TreeNode(<span class="number">3</span>), right=TreeNode(<span class="number">4</span>)),</span><br><span class="line">                right=TreeNode(<span class="number">5</span>, left=TreeNode(<span class="number">6</span>), right=TreeNode(<span class="number">7</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前序</span></span><br><span class="line">stack = [root]</span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">while</span> stack:</span><br><span class="line">    node = stack.pop()</span><br><span class="line">    res.append(node.val)</span><br><span class="line">    <span class="keyword">if</span> node.right:</span><br><span class="line">        stack.append(node.right)</span><br><span class="line">    <span class="keyword">if</span> node.left:</span><br><span class="line">        stack.append(node.left)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前序结果是：&quot;</span>, res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后序</span></span><br><span class="line">stack = [root]</span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">while</span> stack:</span><br><span class="line">    node = stack.pop()</span><br><span class="line">    res.append(node.val)</span><br><span class="line">    <span class="keyword">if</span> node.left:</span><br><span class="line">        stack.append(node.left)</span><br><span class="line">    <span class="keyword">if</span> node.right:</span><br><span class="line">        stack.append(node.right)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;后序结果是：&quot;</span>, res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中序,注意这里不能直接将root加入进去</span></span><br><span class="line">stack = []</span><br><span class="line">cur = root</span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">while</span> stack <span class="keyword">or</span> cur: </span><br><span class="line">    <span class="keyword">if</span> cur:</span><br><span class="line">        stack.append(cur)</span><br><span class="line">        cur = cur.left</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        cur = stack.pop()</span><br><span class="line">        res.append(cur.val)</span><br><span class="line">        cur = cur.right</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;中序结果是：&quot;</span>, res)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 层序遍历</span></span><br><span class="line"><span class="keyword">from</span> _collections <span class="keyword">import</span> deque</span><br><span class="line">queue = deque([root])</span><br><span class="line"><span class="keyword">while</span> queue:</span><br><span class="line">    temp = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">        node = queue.popleft()</span><br><span class="line">        temp.append(node.val)</span><br><span class="line">        <span class="keyword">if</span> node.left:</span><br><span class="line">            queue.append(node.left)</span><br><span class="line">        <span class="keyword">if</span> node.right:</span><br><span class="line">            queue.append(node.right)</span><br><span class="line">    <span class="built_in">print</span>(temp)</span><br><span class="line"><span class="comment"># 层序遍历2</span></span><br><span class="line"><span class="keyword">from</span> _collections <span class="keyword">import</span> deque</span><br><span class="line">queue = deque([root])</span><br><span class="line"><span class="keyword">while</span> queue:</span><br><span class="line">    temp = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">        node = queue.popleft()</span><br><span class="line">        <span class="keyword">if</span> node:</span><br><span class="line">            temp.append(node.val)</span><br><span class="line">            queue.append(node.left)</span><br><span class="line">            queue.append(node.right)</span><br><span class="line">    <span class="built_in">print</span>(temp)</span><br></pre></td></tr></table></figure>
<p>需要注意层序遍历1和层序遍历2的区别，层序遍历1中的queue只添加一些非None的节点，而层序遍历2中的话，连一些为None的节点也会添加，这点在对称二叉树中会用到第二种方法。</p>
<h1>结构操作</h1>
<h2 id="翻转二叉树-226">翻转二叉树[226]</h2>
<p>迭代题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">invertTree</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">        queue = deque([root])</span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">                node = queue.popleft()</span><br><span class="line">                <span class="keyword">if</span> node.left:</span><br><span class="line">                    queue.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right:</span><br><span class="line">                    queue.append(node.right)</span><br><span class="line">                node.left, node.right = node.right, node.left</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<p>递归解法如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">invertTree</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        root.left, root.right = root.right, root.left</span><br><span class="line">        self.invertTree(root.left)</span><br><span class="line">        self.invertTree(root.right)</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="对称二叉树-101">对称二叉树[101]</h2>
<p>迭代解法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isSymmetric</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">from</span> _collections <span class="keyword">import</span> deque</span><br><span class="line">        queue = deque([root])</span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            res = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">                node = queue.popleft()</span><br><span class="line">                <span class="keyword">if</span> node:</span><br><span class="line">                    res.append(node.val)</span><br><span class="line">                    queue.append(node.left)</span><br><span class="line">                    queue.append(node.right)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res.append(<span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">if</span> res != res[::-<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>递归解法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isSymmetric</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">check</span>(<span class="params">p,q</span>):</span><br><span class="line">            <span class="keyword">if</span> p <span class="keyword">and</span> <span class="keyword">not</span> q:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> p <span class="keyword">and</span> q:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> p <span class="keyword">and</span> <span class="keyword">not</span> q:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> p.val != q.val:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> check(p.left, q.right) <span class="keyword">and</span> check(p.right, q.left)       </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> check(root, root)</span><br></pre></td></tr></table></figure>
<p>这题主要要单独开一个sub-function出来操作。</p>
<h2 id="平衡二叉树-110">平衡二叉树[110]</h2>
<p>递归解法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isBalanced</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">get_height</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span> + <span class="built_in">max</span>(get_height(root.left), get_height(root.right))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">abs</span>(get_height(root.left) - get_height(root.right)) &lt;=<span class="number">1</span> <span class="keyword">and</span> self.isBalanced(root.left) <span class="keyword">and</span> self.isBalanced(root.right)</span><br></pre></td></tr></table></figure>
<p>优化完后的递归的解法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isBalanced</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">recu</span>(<span class="params">root</span>):</span><br><span class="line">          <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">          left = recu(root.left)</span><br><span class="line">          <span class="keyword">if</span> left==-<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">          right = recu(root.right)</span><br><span class="line">          <span class="keyword">if</span> right==-<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">          <span class="keyword">return</span> <span class="built_in">max</span>(left, right) + <span class="number">1</span> <span class="keyword">if</span> <span class="built_in">abs</span>(left-right)&lt;=<span class="number">1</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> recu(root)!=-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h1>深度和高度</h1>
<h2 id="完全二叉树的节点个数-222">完全二叉树的节点个数[222]</h2>
<p>迭代解法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countNodes</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">      <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">      queue = deque([root])</span><br><span class="line">      cnt = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span> queue:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">            node = queue.popleft()</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> node.left:</span><br><span class="line">              queue.append(node.left)</span><br><span class="line">            <span class="keyword">if</span> node.right:</span><br><span class="line">              queue.append(node.right)</span><br><span class="line">      <span class="keyword">return</span> cnt        </span><br></pre></td></tr></table></figure>
<p>递归解法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countNodes</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        self.cnt += <span class="number">1</span></span><br><span class="line">        self.countNodes(root.left)</span><br><span class="line">        self.countNodes(root.right)</span><br><span class="line">        <span class="keyword">return</span> self.cnt</span><br></pre></td></tr></table></figure>
<h2 id="二叉树最大高度-104">二叉树最大高度[104]</h2>
<p>迭代解法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxDepth</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">        queue = deque([root])</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            temp = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">                node = queue.popleft()</span><br><span class="line">                temp.append(node.val)</span><br><span class="line">                <span class="keyword">if</span> node.left:</span><br><span class="line">                    queue.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right:</span><br><span class="line">                    queue.append(node.right)</span><br><span class="line">            res.append(temp)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(res)</span><br></pre></td></tr></table></figure>
<p>递归法如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxDepth</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + <span class="built_in">max</span>(self.maxDepth(root.left), self.maxDepth(root.right))</span><br></pre></td></tr></table></figure>
<h2 id="二叉树最小深度-111">二叉树最小深度[111]</h2>
<p>迭代法如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minDepth</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">        queue = deque([root])</span><br><span class="line">        min_depth = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            min_depth += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">                node = queue.popleft()</span><br><span class="line">                <span class="keyword">if</span> node.left==<span class="literal">None</span> <span class="keyword">and</span> node.right==<span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">return</span> min_depth</span><br><span class="line">                <span class="keyword">if</span> node.left:</span><br><span class="line">                    queue.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right:</span><br><span class="line">                    queue.append(node.right)</span><br><span class="line">        <span class="keyword">return</span> minDepth</span><br></pre></td></tr></table></figure>
<p>递归法如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minDepth</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> root.right:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span> + self.minDepth(root.right) <span class="comment"># 容易写成不加1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> root.left:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span> + self.minDepth(root.left)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> + <span class="built_in">min</span>(self.minDepth(root.left), self.minDepth(root.right))</span><br></pre></td></tr></table></figure>
<h1>路径问题</h1>
<h2 id="二叉树的所有路径-257">二叉树的所有路径[257]</h2>
<p>迭代解法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">binaryTreePaths</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">          <span class="keyword">return</span> [<span class="built_in">str</span>(root.val)]</span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">        queue = deque([root])</span><br><span class="line">        queue2 = [<span class="built_in">str</span>(root.val)]</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">              node = queue.popleft()</span><br><span class="line">              node_value = queue2.pop(<span class="number">0</span>)</span><br><span class="line">              <span class="keyword">if</span> <span class="keyword">not</span> node.left <span class="keyword">and</span> <span class="keyword">not</span> node.right:</span><br><span class="line">                  res.append(node_value)</span><br><span class="line">              <span class="keyword">if</span> node.left:</span><br><span class="line">                  queue.append(node.left)</span><br><span class="line">                  queue2.append(<span class="built_in">str</span>(node_value) + <span class="string">&quot;-&gt;&quot;</span> + <span class="built_in">str</span>(node.left.val))</span><br><span class="line">              <span class="keyword">if</span> node.right:</span><br><span class="line">                  queue.append(node.right)</span><br><span class="line">                  queue2.append(<span class="built_in">str</span>(node_value) + <span class="string">&quot;-&gt;&quot;</span> + <span class="built_in">str</span>(node.right.val))</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>递归解法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用回溯1</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">binaryTreePaths</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root, state</span>):</span><br><span class="line">            <span class="keyword">if</span> root:</span><br><span class="line">                state.append(root.val)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">                    res.append(<span class="string">&quot;-&gt;&quot;</span>.join([<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> state[:]]))</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">                <span class="keyword">if</span> root.left:</span><br><span class="line">                    dfs(root.left, state)</span><br><span class="line">                    state.pop()</span><br><span class="line">                <span class="keyword">if</span> root.right:</span><br><span class="line">                    dfs(root.right, state)</span><br><span class="line">                    state.pop()</span><br><span class="line">        dfs(root, [])</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">        </span><br><span class="line"><span class="comment"># [推荐写法] 回溯2</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">binaryTreePaths</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root, state</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            state.append(root.val)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">                res.append(state[:])</span><br><span class="line">            dfs(root.left, state)</span><br><span class="line">            dfs(root.right, state)</span><br><span class="line">            state.pop()</span><br><span class="line">        dfs(root, [])</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;-&gt;&quot;</span>.join([<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> s]) <span class="keyword">for</span> s <span class="keyword">in</span> res]</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 还有一种回溯的写法，不需要显示的调用pop函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">binaryTreePaths</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: List[str]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">construct_paths</span>(<span class="params">root, path</span>):</span><br><span class="line">            <span class="keyword">if</span> root:</span><br><span class="line">                path += <span class="built_in">str</span>(root.val)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:  <span class="comment"># 当前节点是叶子节点</span></span><br><span class="line">                    paths.append(path)  <span class="comment"># 把路径加入到答案中</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    path += <span class="string">&#x27;-&gt;&#x27;</span>  <span class="comment"># 当前节点不是叶子节点，继续递归遍历</span></span><br><span class="line">                    construct_paths(root.left, path)</span><br><span class="line">                    construct_paths(root.right, path)</span><br><span class="line"></span><br><span class="line">        paths = []</span><br><span class="line">        construct_paths(root, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> paths</span><br></pre></td></tr></table></figure>
<h2 id="路径总和-112">路径总和[112]</h2>
<p>迭代解法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasPathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], targetSum: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">      <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">      queue = deque([root]) <span class="comment"># 使用了两个deque</span></span><br><span class="line">      queue2 = deque([root.val])</span><br><span class="line">      <span class="keyword">while</span> queue:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">          node = queue.popleft()</span><br><span class="line">          node_value = queue2.popleft()</span><br><span class="line">          <span class="keyword">if</span> node.left==<span class="literal">None</span> <span class="keyword">and</span> node.right==<span class="literal">None</span> <span class="keyword">and</span> node_value==targetSum:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">          <span class="keyword">if</span> node.left:</span><br><span class="line">            queue.append(node.left)</span><br><span class="line">            queue2.append(node_value+node.left.val)</span><br><span class="line">          <span class="keyword">if</span> node.right:</span><br><span class="line">            queue.append(node.right)</span><br><span class="line">            queue2.append(node_value+node.right.val)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>递归解法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正确解答1</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasPathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], targetSum: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">            <span class="keyword">return</span> targetSum == root.val</span><br><span class="line">        <span class="keyword">return</span> self.hasPathSum(root.left, targetSum - root.val) <span class="keyword">or</span> self.hasPathSum(root.right, targetSum - root.val)</span><br><span class="line"><span class="comment"># [推荐写法]正确解答2【为了和后面的112保持一致，使用该解法】</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasPathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], targetSum: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        targetSum = targetSum - root.val</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> targetSum==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> self.hasPathSum(root.left, targetSum) <span class="keyword">or</span> self.hasPathSum(root.right, targetSum)</span><br><span class="line"><span class="comment"># 错写</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasPathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], targetSum: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> targetSum==<span class="number">0</span>: <span class="comment"># 注意</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span> <span class="comment"># 注意</span></span><br><span class="line">        <span class="keyword">return</span> self.hasPathSum(root.left, targetSum - root.val) <span class="keyword">or</span> self.hasPathSum(root.right, targetSum - root.val)</span><br></pre></td></tr></table></figure>
<h2 id="路径总和-II-113">路径总和 II[113]</h2>
<p>递归解法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回溯解法1，按照标准的回溯来写的</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], targetSum: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root, state</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            state.append(root.val)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">sum</span>(state[:])==targetSum:</span><br><span class="line">                    res.append(state[:])</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">if</span> root.left:</span><br><span class="line">                dfs(root.left, state)</span><br><span class="line">                state.pop()</span><br><span class="line">            <span class="keyword">if</span> root.right:</span><br><span class="line">                dfs(root.right, state)</span><br><span class="line">                state.pop()</span><br><span class="line">        dfs(root, [])</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">        </span><br><span class="line"><span class="comment"># [推荐写法]回溯解法2， 按照标准回溯来</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], targetSum: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        ret = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root, state</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            state.append(root.val)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> <span class="built_in">sum</span>(state[:]) == targetSum:</span><br><span class="line">                ret.append(state[:])</span><br><span class="line">            dfs(root.left, state)</span><br><span class="line">            dfs(root.right, state)</span><br><span class="line">            state.pop()</span><br><span class="line">        dfs(root, [])</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 回溯解法2，使用非标准的回溯来写的</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pathSum</span>(<span class="params">self, root: TreeNode, targetSum: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        ret = <span class="built_in">list</span>()</span><br><span class="line">        path = <span class="built_in">list</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root: TreeNode, targetSum: <span class="built_in">int</span></span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            path.append(root.val)</span><br><span class="line">            targetSum -= root.val</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> targetSum == <span class="number">0</span>:</span><br><span class="line">                ret.append(path[:])</span><br><span class="line">            dfs(root.left, targetSum)</span><br><span class="line">            dfs(root.right, targetSum)</span><br><span class="line">            path.pop()</span><br><span class="line">        </span><br><span class="line">        dfs(root, targetSum)</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 回溯解法3，使用非标准回溯来</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pathSum</span>(<span class="params">self, root, targetSum</span>):</span><br><span class="line">        ret = <span class="built_in">list</span>()</span><br><span class="line">        path = <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root: TreeNode</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            path.append(root.val)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> <span class="built_in">sum</span>(path[:]) == targetSum:</span><br><span class="line">                ret.append(path[:])</span><br><span class="line">            dfs(root.left)</span><br><span class="line">            dfs(root.right)</span><br><span class="line">            path.pop()</span><br><span class="line"></span><br><span class="line">        dfs(root)</span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>
<p>非标准的意思是，这里的path是一个不在dfs中传进去的值</p>
<h2 id="路径总和III-437">路径总和III[437]</h2>
<p>我的解法如下，超时了,妈的不打算改了就这样吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], targetSum: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">root, state</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            state.append(root.val)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">sum</span>(state[:])==targetSum:</span><br><span class="line">                res.append(state[:])</span><br><span class="line">            dfs(root.left, state)</span><br><span class="line">            dfs(root.right, state)</span><br><span class="line">            state.pop()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs2</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            dfs(root,[])</span><br><span class="line">            dfs2(root.left)</span><br><span class="line">            dfs2(root.right)</span><br><span class="line"></span><br><span class="line">        dfs2(root)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(res)</span><br></pre></td></tr></table></figure>
<h2 id="二叉树中的最大路径和-124">二叉树中的最大路径和[124]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/binary-tree-maximum-path-sum/description/">https://leetcode.cn/problems/binary-tree-maximum-path-sum/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.max_sum = -<span class="number">88</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxPathSum</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">maxGain</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            leftGain = <span class="built_in">max</span>(maxGain(root.left), <span class="number">0</span>)</span><br><span class="line">            rightGrain = <span class="built_in">max</span>(maxGain(root.right), <span class="number">0</span>)</span><br><span class="line">            cur_root_gain = root.val + leftGain + rightGrain</span><br><span class="line">            <span class="keyword">if</span> cur_root_gain &gt; self.max_sum:</span><br><span class="line">                self.max_sum = cur_root_gain</span><br><span class="line">            <span class="keyword">return</span> root.val + <span class="built_in">max</span>(rightGrain,leftGain) <span class="comment">#写错为root.val + rightGrain + leftGain</span></span><br><span class="line">        maxGain(root)</span><br><span class="line">        <span class="keyword">return</span> self.max_sum</span><br></pre></td></tr></table></figure>
<p>注意这里的maxGain是需要定义好的，返回的值是以当前节点为开始或者结束的收益值，因此需要写成root.val+max(rightGain, leftGain)</p>
<h2 id="二叉树的最近公共祖先-236">二叉树的最近公共祖先[236]</h2>
<p>递归法如下，最主要的点在代码中已经说了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误解法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lowestCommonAncestor</span>(<span class="params">self, root, p, q</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :type p: TreeNode</span></span><br><span class="line"><span class="string">        :type q: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: TreeNode</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> root.right:</span><br><span class="line">            <span class="keyword">return</span> self.lowestCommonAncestor(root.right, p, q)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> root.left:</span><br><span class="line">            <span class="keyword">return</span> self.lowestCommonAncestor(root.left, p, q)</span><br><span class="line">        left = self.lowestCommonAncestor(root.left, p, q)</span><br><span class="line">        right = self.lowestCommonAncestor(root.right, p, q)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> left:</span><br><span class="line">            <span class="keyword">return</span> right</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> right:</span><br><span class="line">            <span class="keyword">return</span> left</span><br><span class="line">        <span class="keyword">if</span> left <span class="keyword">and</span> right:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line"><span class="comment"># 正确解法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lowestCommonAncestor</span>(<span class="params">self, root: <span class="string">&#x27;TreeNode&#x27;</span>, p: <span class="string">&#x27;TreeNode&#x27;</span>, q: <span class="string">&#x27;TreeNode&#x27;</span></span>) -&gt; <span class="string">&#x27;TreeNode&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">if</span> root==p <span class="keyword">or</span> root==q: <span class="comment"># 最重要的点</span></span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        left = self.lowestCommonAncestor(root.left, p, q)</span><br><span class="line">        right = self.lowestCommonAncestor(root.right, p, q)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> left:</span><br><span class="line">            <span class="keyword">return</span> right</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> right:</span><br><span class="line">            <span class="keyword">return</span> left</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h1>构造二叉树</h1>
<h2 id="最大二叉树-654">最大二叉树[654]</h2>
<p>迭代法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">constructMaximumBinaryTree</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        max_index = nums.index(<span class="built_in">max</span>(nums))</span><br><span class="line">        root = TreeNode(val=nums[max_index])</span><br><span class="line">        root.left = self.constructMaximumBinaryTree(nums[:max_index])</span><br><span class="line">        root.right = self.constructMaximumBinaryTree(nums[max_index+<span class="number">1</span>:])</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="合并二叉树-617">合并二叉树[617]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mergeTrees</span>(<span class="params">self, root1: <span class="type">Optional</span>[TreeNode], root2: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root1 <span class="keyword">and</span> <span class="keyword">not</span> root2:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root1 <span class="keyword">and</span> root2:</span><br><span class="line">            <span class="keyword">return</span> root2</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root2 <span class="keyword">and</span> root1:</span><br><span class="line">            <span class="keyword">return</span> root1</span><br><span class="line">        root = TreeNode(root1.val + root2.val)</span><br><span class="line">        root.left = self.mergeTrees(root1.left, root2.left)</span><br><span class="line">        root.right = self.mergeTrees(root1.right, root2.right)</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="从前序与中序遍历序列构造二叉树-105">从前序与中序遍历序列构造二叉树[105]</h2>
<p>递归方法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildTree</span>(<span class="params">self, preorder: <span class="type">List</span>[<span class="built_in">int</span>], inorder: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> preorder:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> inorder:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        root = TreeNode(preorder[<span class="number">0</span>])</span><br><span class="line">        inorder_index = inorder.index(preorder[<span class="number">0</span>])</span><br><span class="line">        root.left = self.buildTree(preorder[<span class="number">1</span>:inorder_index+<span class="number">1</span>], inorder[:inorder_index])</span><br><span class="line">        root.right = self.buildTree(preorder[inorder_index+<span class="number">1</span>:], inorder[inorder_index+<span class="number">1</span>:])</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="从中序与后序遍历序列构造二叉树-106">从中序与后序遍历序列构造二叉树[106]</h2>
<p>递归解法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildTree</span>(<span class="params">self, inorder: <span class="type">List</span>[<span class="built_in">int</span>], postorder: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> inorder:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> postorder:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        root = TreeNode(postorder[-<span class="number">1</span>])</span><br><span class="line">        inorder_index = inorder.index(postorder[-<span class="number">1</span>])</span><br><span class="line">        root.left = self.buildTree(inorder[:inorder_index], postorder[:inorder_index])</span><br><span class="line">        root.right = self.buildTree(inorder[inorder_index+<span class="number">1</span>:], postorder[inorder_index:-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h1>二叉搜索树</h1>
<h2 id="将有序数组转换为二叉搜索树-108">将有序数组转换为二叉搜索树[108]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortedArrayToBST</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums)==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        mid = <span class="built_in">int</span>(<span class="built_in">len</span>(nums)/<span class="number">2</span>)</span><br><span class="line">        root = TreeNode(nums[mid])</span><br><span class="line">        root.left = self.sortedArrayToBST(nums[:mid])</span><br><span class="line">        root.right = self.sortedArrayToBST(nums[mid+<span class="number">1</span>:])</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="二叉搜索树中的搜索-700">二叉搜索树中的搜索[700]</h2>
<p>迭代法，就是类似链表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">searchBST</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], val: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        cur = root</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            f = cur.val</span><br><span class="line">            <span class="keyword">if</span> f &lt; val:</span><br><span class="line">                cur = cur.right</span><br><span class="line">            <span class="keyword">elif</span> f &gt; val:</span><br><span class="line">                cur = cur.left</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> cur</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>递归法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">searchBST</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], val: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> root.val == val:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">elif</span> root.val &gt; val:</span><br><span class="line">            <span class="keyword">return</span> self.searchBST(root.left, val)</span><br><span class="line">        <span class="keyword">elif</span> root.val &lt; val:</span><br><span class="line">            <span class="keyword">return</span> self.searchBST(root.right, val)</span><br></pre></td></tr></table></figure>
<h2 id="验证二叉搜索树-98">验证二叉搜索树[98]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isValidBST</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">inOrder</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            inOrder(root.left)</span><br><span class="line">            res.append(root.val)</span><br><span class="line">            inOrder(root.right)</span><br><span class="line">        inOrder(root)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(res)-<span class="number">1</span>):</span><br><span class="line">          <span class="keyword">if</span> res[i+<span class="number">1</span>] &lt;= res[i]:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h2 id="二叉搜索树的最小绝对差-530">二叉搜索树的最小绝对差[530]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getMinimumDifference</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            inorder(root.left)</span><br><span class="line">            res.append(root.val)</span><br><span class="line">            inorder(root.right)</span><br><span class="line">        inorder(root)</span><br><span class="line">        mins = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(res)-<span class="number">1</span>):</span><br><span class="line">            mins = <span class="built_in">min</span>(mins, <span class="built_in">abs</span>(res[i+<span class="number">1</span>] - res[i]))</span><br><span class="line">        <span class="keyword">return</span> mins</span><br></pre></td></tr></table></figure>
<h2 id="二叉搜索树中的众数-501">二叉搜索树中的众数[501]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMode</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            inorder(root.left)</span><br><span class="line">            res.append(root.val)</span><br><span class="line">            inorder(root.right)</span><br><span class="line">        inorder(root)</span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        cnt = Counter(res)</span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        new_d = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> cnt.items():</span><br><span class="line">            new_d[v].append(k)</span><br><span class="line">        new_d = <span class="built_in">sorted</span>(new_d.items(),key=<span class="keyword">lambda</span> x:-x[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> new_d[<span class="number">0</span>][<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h2 id="二叉搜索树的最近公共祖先-235">二叉搜索树的最近公共祖先[235]</h2>
<p>递归解法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lowestCommonAncestor</span>(<span class="params">self, root: <span class="string">&#x27;TreeNode&#x27;</span>, p: <span class="string">&#x27;TreeNode&#x27;</span>, q: <span class="string">&#x27;TreeNode&#x27;</span></span>) -&gt; <span class="string">&#x27;TreeNode&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">if</span> root == p <span class="keyword">or</span> root == q:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">if</span> root.val &lt; <span class="built_in">min</span>(p.val, q.val):</span><br><span class="line">            <span class="keyword">return</span> self.lowestCommonAncestor(root.right, p, q)</span><br><span class="line">        <span class="keyword">if</span> root.val &gt; <span class="built_in">max</span>(p.val, q.val):</span><br><span class="line">            <span class="keyword">return</span> self.lowestCommonAncestor(root.left, p, q)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="二叉树的最近公共祖先-236-2">二叉树的最近公共祖先[236]</h2>
<p>递归法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正确解法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lowestCommonAncestor</span>(<span class="params">self, root: <span class="string">&#x27;TreeNode&#x27;</span>, p: <span class="string">&#x27;TreeNode&#x27;</span>, q: <span class="string">&#x27;TreeNode&#x27;</span></span>) -&gt; <span class="string">&#x27;TreeNode&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">if</span> root==p <span class="keyword">or</span> root==q:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        left = self.lowestCommonAncestor(root.left, p, q)</span><br><span class="line">        right = self.lowestCommonAncestor(root.right, p, q)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> left:</span><br><span class="line">            <span class="keyword">return</span> right</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> right:</span><br><span class="line">            <span class="keyword">return</span> left</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="二叉搜索树中的插入操作-701">二叉搜索树中的插入操作[701]</h2>
<p>迭代法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insertIntoBST</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], val: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        res_trees = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            inorder(root.left)</span><br><span class="line">            res_trees.append(root)</span><br><span class="line">            inorder(root.right)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> TreeNode(val)</span><br><span class="line">        inorder(root)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val &lt; res_trees[<span class="number">0</span>].val:</span><br><span class="line">            res_trees[<span class="number">0</span>].left = TreeNode(val)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> val &gt; res_trees[-<span class="number">1</span>].val:</span><br><span class="line">            res_trees[-<span class="number">1</span>].right = TreeNode(val)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(res_trees)-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> val&gt;res_trees[i].val <span class="keyword">and</span> val&lt;res_trees[i+<span class="number">1</span>].val:</span><br><span class="line">                left = res_trees[i]</span><br><span class="line">                right = res_trees[i+<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> left.right==<span class="literal">None</span>:</span><br><span class="line">                    left.right = TreeNode(val)</span><br><span class="line">                <span class="keyword">elif</span> right.left==<span class="literal">None</span>:</span><br><span class="line">                    right.left = TreeNode(val)</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<p>官方迭代法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insertIntoBST</span>(<span class="params">self, root: TreeNode, val: <span class="built_in">int</span></span>) -&gt; TreeNode:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> TreeNode(val)</span><br><span class="line">        </span><br><span class="line">        pos = root</span><br><span class="line">        <span class="keyword">while</span> pos:</span><br><span class="line">            <span class="keyword">if</span> val &lt; pos.val:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> pos.left:</span><br><span class="line">                    pos.left = TreeNode(val)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    pos = pos.left</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> pos.right:</span><br><span class="line">                    pos.right = TreeNode(val)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    pos = pos.right</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="删除二叉搜索树中的节点-450">删除二叉搜索树中的节点[450]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleteNode</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], key: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> root.val &gt; key:</span><br><span class="line">            root.left = self.deleteNode(root.left, key)</span><br><span class="line">        <span class="keyword">elif</span> root.val &lt; key:</span><br><span class="line">            root.right = self.deleteNode(root.right, key)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 判断root点的情况</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> root.right:</span><br><span class="line">                <span class="keyword">return</span> root.right</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.right <span class="keyword">and</span> root.left:</span><br><span class="line">                <span class="keyword">return</span> root.left           </span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span>    </span><br><span class="line">            <span class="keyword">if</span> root.left <span class="keyword">and</span> root.right:</span><br><span class="line">                <span class="comment"># 找到最左边的节点</span></span><br><span class="line">                t = node = root.right</span><br><span class="line">                <span class="keyword">while</span> node.left:</span><br><span class="line">                    node = node.left</span><br><span class="line">                node.left = root.left</span><br><span class="line">                <span class="keyword">return</span> t</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="修剪二叉搜索树-669">修剪二叉搜索树[669]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误解法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">trimBST</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], low: <span class="built_in">int</span>, high: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">and</span> root.val &lt; low:</span><br><span class="line">            root = root.right</span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">and</span> root.val &gt; high:</span><br><span class="line">            root = root.left</span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">and</span> root.left <span class="keyword">and</span> root.left.val &lt; low:</span><br><span class="line">            root.left = root.left.right</span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">and</span> root.right <span class="keyword">and</span> root.right.val &gt; high:</span><br><span class="line">            root.right = root.right.left</span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">and</span> root.left:</span><br><span class="line">            self.trimBST(root.left, low, high)</span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">and</span> root.right:</span><br><span class="line">            self.trimBST(root.right, low, high)</span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line"><span class="comment"># 【推荐解法】</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">trimBST</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], low: <span class="built_in">int</span>, high: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">if</span> root.val &lt; low:</span><br><span class="line">            <span class="keyword">return</span> self.trimBST(root.right, low, high)</span><br><span class="line">        <span class="keyword">elif</span> root.val &gt; high:</span><br><span class="line">            <span class="keyword">return</span> self.trimBST(root.left, low, high)</span><br><span class="line">        root.left = self.trimBST(root.left, low, high)</span><br><span class="line">        root.right = self.trimBST(root.right, low, high)</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="把二叉搜索树转换为累加树-538">把二叉搜索树转换为累加树[538]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">convertBST</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            inorder(root.left)</span><br><span class="line">            res.append(root)</span><br><span class="line">            inorder(root.right)</span><br><span class="line">        inorder(root)</span><br><span class="line">        res=res[::-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(res)):</span><br><span class="line">            res[i].val = res[i].val + res[i-<span class="number">1</span>].val</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
<h2 id="把二叉搜索树转换为累加树-538-2">把二叉搜索树转换为累加树[538]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">convertBST</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">root</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            inorder(root.left)</span><br><span class="line">            res.append(root)</span><br><span class="line">            inorder(root.right)</span><br><span class="line">        inorder(root)</span><br><span class="line">        res=res[::-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(res)):</span><br><span class="line">            res[i].val = res[i].val + res[i-<span class="number">1</span>].val</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/19/leetcode/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tom">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法工程师的日常">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/19/leetcode/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" class="post-title-link" itemprop="url">动态规划</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-19 23:29:20" itemprop="dateCreated datePublished" datetime="2024-03-19T23:29:20+08:00">2024-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-03-20 22:47:23" itemprop="dateModified" datetime="2024-03-20T22:47:23+08:00">2024-03-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LeetCode/" itemprop="url" rel="index"><span itemprop="name">LeetCode</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LeetCode/%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/" itemprop="url" rel="index"><span itemprop="name">题目汇总</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>总结</h1>
<h2 id="大纲">大纲</h2>
<h2 id="相关细节">相关细节</h2>
<ol>
<li>如果题目求最大值，则dp初始化为[-float(“inf”)]*n的数据</li>
<li>循环比较一般使用dp[i] = max(dp[i], dp[j] + nums[i])，而不是dp[i] = max(dp[i], dp[j]) + nums[i]</li>
</ol>
<h1>序列</h1>
<h2 id="单序列">单序列</h2>
<h3 id="模板">模板</h3>
<ul>
<li>一般初始化的时候，都会dp = [0] * len(str),而不是dp = [0] * (len(str)+1)</li>
<li>注意有的时候不是输出dp[n-1]而是max(dp)</li>
<li>其他点</li>
</ul>
<h3 id="最长递增子序列-300">最长递增子序列[300]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/longest-increasing-subsequence">https://leetcode.cn/problems/longest-increasing-subsequence</a><br>
dp表达式如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if (nums[i] &gt; nums[j]) dp[i] = max(dp[i], dp[j] + 1);</span><br></pre></td></tr></table></figure>
<p>代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLIS</span>(<span class="params">self, nums</span>):</span><br><span class="line">        dp = [<span class="number">1</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">                <span class="keyword">if</span> nums[i] &gt; nums[j]:</span><br><span class="line">                    dp[i] = <span class="built_in">max</span>(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></table></figure>
<h3 id="最长连续递增序列-674">最长连续递增序列[674]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/longest-continuous-increasing-subsequence">https://leetcode.cn/problems/longest-continuous-increasing-subsequence</a><br>
dp表达式如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if (nums[i] &gt; nums[i-1]) dp[i] = max(dp[i], dp[i-1] + 1);</span><br></pre></td></tr></table></figure>
<p>代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findLengthOfLCIS</span>(<span class="params">self, nums</span>):</span><br><span class="line">        dp = [<span class="number">1</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[i] &gt; nums[i-<span class="number">1</span>]:</span><br><span class="line">                dp[i] = <span class="built_in">max</span>(dp[i], dp[i-<span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></table></figure>
<h3 id="最大子数组和-53">最大子数组和[53]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/maximum-subarray">https://leetcode.cn/problems/maximum-subarray</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxSubArray</span>(<span class="params">self, nums</span>):</span><br><span class="line">        dp = [<span class="number">0</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">        dp[<span class="number">0</span>] = nums[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            dp[i] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>]+nums[i], nums[i])</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></table></figure>
<h3 id="回文子串-647">回文子串[647]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/palindromic-substrings/description/">https://leetcode.cn/problems/palindromic-substrings/description/</a><br>
动规表达式如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dp[i][i] = 1</span><br><span class="line">if s[i]==s[j] and j-i==1: dp[i][j]=1</span><br><span class="line">if j-i&gt;1 and dp[i+1][j-1] and s[i]==s[j]:dp[i][j]=1</span><br></pre></td></tr></table></figure>
<p>题解如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countSubstrings</span>(<span class="params">self, s</span>):</span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        dp = [[<span class="number">0</span>]*n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dp[i][i] = <span class="number">1</span></span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(j):</span><br><span class="line">                <span class="keyword">if</span> j-i==<span class="number">1</span> <span class="keyword">and</span> s[i]==s[j]:</span><br><span class="line">                    dp[i][j] = <span class="number">1</span></span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> j -i &gt; <span class="number">1</span> <span class="keyword">and</span> dp[i+<span class="number">1</span>][j-<span class="number">1</span>] <span class="keyword">and</span> s[i]==s[j]:</span><br><span class="line">                    dp[i][j] = <span class="number">1</span></span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> cnt</span><br></pre></td></tr></table></figure>
<p>遍历顺序是这样的<br>
<img src="https://pic.leetcode-cn.com/17dc7e2a1d0cb9916917e2121ec59d838bc453a4c83df1f60b2f28f10a1f986e-image.png" alt="image"></p>
<h3 id="最长回文子串-5">最长回文子串[5]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/longest-palindromic-substring">https://leetcode.cn/problems/longest-palindromic-substring</a><br>
子串是连续的，子序列不是，这里要注意，和上面解法一样，代码没变</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s</span>):</span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        dp = [[<span class="number">0</span>]*n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        m = <span class="number">0</span></span><br><span class="line">        res = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dp[i][i] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i-i+<span class="number">1</span>&gt;m:</span><br><span class="line">                m = i-i+<span class="number">1</span></span><br><span class="line">                res = [i,i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(j):</span><br><span class="line">                <span class="keyword">if</span> j-i==<span class="number">1</span> <span class="keyword">and</span> s[i]==s[j]:</span><br><span class="line">                    dp[i][j] = <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> j-i+<span class="number">1</span>&gt;m:</span><br><span class="line">                        m = j-i+<span class="number">1</span></span><br><span class="line">                        res = [i,j]</span><br><span class="line">                <span class="keyword">elif</span> j-i&gt;<span class="number">1</span> <span class="keyword">and</span> s[i]==s[j] <span class="keyword">and</span> dp[i+<span class="number">1</span>][j-<span class="number">1</span>]==<span class="number">1</span>:</span><br><span class="line">                    dp[i][j] = <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> j-i+<span class="number">1</span>&gt;m: <span class="comment"># 要在里面写不能算完dp后再比较</span></span><br><span class="line">                        m = j-i+<span class="number">1</span></span><br><span class="line">                        res = [i,j]</span><br><span class="line">        <span class="keyword">return</span> s[res[<span class="number">0</span>]:res[<span class="number">1</span>]+<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 推荐代码，如果按照上面写法做最长回文子序列会报错，因为循环的时候方向不对的，可以推导一下</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        dp = [[<span class="literal">False</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        max_flag = <span class="number">0</span></span><br><span class="line">        ind = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dp[i][i] = <span class="literal">True</span></span><br><span class="line">            ind = (i, i)  </span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - l):</span><br><span class="line">                j = i + l</span><br><span class="line">                <span class="keyword">if</span> j-i==<span class="number">1</span> <span class="keyword">and</span> s[i]==s[j]:</span><br><span class="line">                    dp[i][j] = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">if</span> j - i &gt; max_flag: </span><br><span class="line">                        max_flag = j - i</span><br><span class="line">                        ind = (i, j)</span><br><span class="line">                <span class="keyword">if</span> j-i&gt;<span class="number">1</span> <span class="keyword">and</span> s[i] == s[j]:</span><br><span class="line">                    dp[i][j] = dp[i + <span class="number">1</span>][j - <span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">if</span> dp[i][j] <span class="keyword">and</span> j - i &gt; max_flag:</span><br><span class="line">                        max_flag = j - i</span><br><span class="line">                        ind = (i, j)</span><br><span class="line">        <span class="keyword">return</span> (s[ind[<span class="number">0</span>]:ind[<span class="number">1</span>] + <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h3 id="环形子数组的最大和-918">环形子数组的最大和[918]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/maximum-sum-circular-subarray/description/">https://leetcode.cn/problems/maximum-sum-circular-subarray/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxSubarraySumCircular</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line"></span><br><span class="line">        max_ = -<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        dp_max = [<span class="number">0</span>] * n</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dp_max[i] = <span class="built_in">max</span>(<span class="number">0</span>, dp_max[i-<span class="number">1</span>]) + nums[i]</span><br><span class="line">            <span class="keyword">if</span> dp_max[i] &gt; max_:</span><br><span class="line">                max_ = dp_max[i]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">max</span>(dp_max) &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">max</span>(nums)</span><br><span class="line">        </span><br><span class="line">        min_ = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        dp_min = [<span class="number">0</span>] * n</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dp_min[i] = <span class="built_in">min</span>(<span class="number">0</span>, dp_min[i-<span class="number">1</span>]) + nums[i]</span><br><span class="line">            <span class="keyword">if</span> dp_min[i] &lt; min_:</span><br><span class="line">                min_ = dp_min[i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(max_, <span class="built_in">sum</span>(nums) - min_)        </span><br></pre></td></tr></table></figure>
<h3 id="最长回文子序列-516">最长回文子序列[516]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/longest-palindromic-subsequence">https://leetcode.cn/problems/longest-palindromic-subsequence</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindromeSubseq</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [[<span class="number">0</span>] * <span class="built_in">len</span>(s) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s))]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            dp[i][i] = <span class="number">1</span></span><br><span class="line">        n = <span class="built_in">len</span>(s)</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - l):</span><br><span class="line">                j = i + l</span><br><span class="line">                <span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">                    dp[i][j] = dp[i + <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">2</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i + <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][n-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="戳气球-312">戳气球[312]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/burst-balloons">https://leetcode.cn/problems/burst-balloons</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 递归</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxCoins</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        val = [<span class="number">1</span>] + nums + [<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line"><span class="meta">        @lru_cache(<span class="params"><span class="literal">None</span></span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">left: <span class="built_in">int</span>, right: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">            <span class="keyword">if</span> left &gt;= right - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            best = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(left + <span class="number">1</span>, right):</span><br><span class="line">                total = val[left] * val[i] * val[right]</span><br><span class="line">                total += solve(left, i) + solve(i, right)</span><br><span class="line">                best = <span class="built_in">max</span>(best, total)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">return</span> best</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> solve(<span class="number">0</span>, n + <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 动规</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxCoins</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        rec = [[<span class="number">0</span>] * (n + <span class="number">2</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">2</span>)]</span><br><span class="line">        val = [<span class="number">1</span>] + nums + [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">2</span>, n + <span class="number">2</span>):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, j):</span><br><span class="line">                    total = val[i] * val[k] * val[j]</span><br><span class="line">                    total += rec[i][k] + rec[k][j]</span><br><span class="line">                    rec[i][j] = <span class="built_in">max</span>(rec[i][j], total)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> rec[<span class="number">0</span>][n + <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://leetcode.cn/problems/burst-balloons/solutions/337630/zhe-ge-cai-pu-zi-ji-zai-jia-ye-neng-zuo-guan-jian-/">https://leetcode.cn/problems/burst-balloons/solutions/337630/zhe-ge-cai-pu-zi-ji-zai-jia-ye-neng-zuo-guan-jian-/</a></p>
</blockquote>
<h3 id="鸡蛋掉落-887">鸡蛋掉落[887]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/super-egg-drop">https://leetcode.cn/problems/super-egg-drop</a>  本题需要反过来想，如果我们可以做 t 次操作，而且有 k 个鸡蛋，那么我们能找到答案的最高的 n 是多少</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">superEggDrop</span>(<span class="params">self, k: <span class="built_in">int</span>, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># k鸡蛋，n是楼层</span></span><br><span class="line">        <span class="keyword">if</span> n==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        dp = [[<span class="number">0</span>]*(k+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n+<span class="number">1</span>)] <span class="comment"># t次操作，k个鸡蛋，最高的n</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,k+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">1</span>][i] = <span class="number">1</span></span><br><span class="line">        ans = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,n+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,k+<span class="number">1</span>):</span><br><span class="line">                dp[i][j] = <span class="number">1</span> + dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + dp[i-<span class="number">1</span>][j] <span class="comment"># 鸡蛋碎和没碎</span></span><br><span class="line">            <span class="keyword">if</span> dp[i][k] &gt;=n: <span class="comment"># 注意这里是k</span></span><br><span class="line">                ans = i</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<p>建议还是记下来吧，不然很容易忘</p>
<h2 id="双序列">双序列</h2>
<h3 id="模板-2">模板</h3>
<ul>
<li>初始化dp为dp=[[0]*(n+1) for _ in range(m+1)],要多一位</li>
<li>在判断的时候要少一位，比如num[i-1]就是对应到dp[i]这里来</li>
<li>初始化一般都是0</li>
<li>注意判断好条件</li>
</ul>
<p>简单代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n1 = <span class="built_in">len</span>(text1)</span><br><span class="line">n2 = <span class="built_in">len</span>(text2)</span><br><span class="line">dp = [[<span class="number">0</span>]*(n2+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n1+<span class="number">1</span>)] <span class="comment"># 注意点1，都为0，且行列为n1+1啊</span></span><br><span class="line">result = -<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n1+<span class="number">1</span>): <span class="comment"># 注意点2，是从1开始</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n2+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> text1[i-<span class="number">1</span>] == text2[j-<span class="number">1</span>]: <span class="comment"># 注意点3，这里判断i-1是否对的</span></span><br><span class="line">            dp[i][j] = func(dp[i][j], dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">1</span>) </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dp[i][j] = func(..)</span><br></pre></td></tr></table></figure>
<h3 id="最长重复子数组-718">最长重复子数组[718]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/maximum-length-of-repeated-subarray">https://leetcode.cn/problems/maximum-length-of-repeated-subarray</a><br>
注意题目中说的子数组，其实就是连续子序列。和最长连续递增序列[674]有点类似，下面的最长公共子序列和最长递增子序列[300]有点像。</p>
<p>dp表达式如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A[i - 1] 和B[j - 1]相等的时候，dp[i][j] = dp[i - 1][j - 1] + 1;;</span><br></pre></td></tr></table></figure>
<p>代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findLength</span>(<span class="params">self, nums1, nums2</span>):</span><br><span class="line">        n1 = <span class="built_in">len</span>(nums1)</span><br><span class="line">        n2 = <span class="built_in">len</span>(nums2)</span><br><span class="line">        dp = [[<span class="number">0</span>]*(n2+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n1+<span class="number">1</span>)] <span class="comment"># 注意1：这里多了一位</span></span><br><span class="line">        max_res = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n1+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n2+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> nums1[i-<span class="number">1</span>] == nums2[j-<span class="number">1</span>]: <span class="comment"># 这里判断i-1,就是       对应dp的i,主要是为了判断方便</span></span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i][j], dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line">                max_res = <span class="built_in">max</span>(max_res, dp[i][j])</span><br><span class="line">        <span class="keyword">return</span> max_res</span><br></pre></td></tr></table></figure>
<h3 id="最长公共子序列-1143">最长公共子序列[1143]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/longest-common-subsequence">https://leetcode.cn/problems/longest-common-subsequence</a><br>
和 最长重复子数组[718] 不一样，这道题不需要连续的数组。</p>
<p>p表达式如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A[i - 1] 和B[j - 1]相等的时候，dp[i][j] = dp[i - 1][j - 1] + 1;</span><br><span class="line">不等的时候dp[i][j] = max(dp[i-1][j], dp[i][j-1])</span><br></pre></td></tr></table></figure>
<p>代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestCommonSubsequence</span>(<span class="params">self, text1, text2</span>):</span><br><span class="line">        n1 = <span class="built_in">len</span>(text1)</span><br><span class="line">        n2 = <span class="built_in">len</span>(text2)</span><br><span class="line">        dp = [[<span class="number">0</span>]*(n2+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n1+<span class="number">1</span>)]</span><br><span class="line">        result = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n1+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n2+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> text1[i-<span class="number">1</span>] == text2[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i][j], dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">1</span>) </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])</span><br><span class="line">                result = <span class="built_in">max</span>(result, dp[i][j])</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h3 id="不相交的线-1035">不相交的线[1035]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/uncrossed-lines">https://leetcode.cn/problems/uncrossed-lines</a><br>
和最长公共子序列一样，代码也是一样的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxUncrossedLines</span>(<span class="params">self, nums1, nums2</span>):</span><br><span class="line">        n1 = <span class="built_in">len</span>(nums1)</span><br><span class="line">        n2 = <span class="built_in">len</span>(nums2)</span><br><span class="line">        dp = [[<span class="number">0</span>]*(n2+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n1+<span class="number">1</span>)]</span><br><span class="line">        result = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n1+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n2+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> nums1[i-<span class="number">1</span>] == nums2[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i][j], dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">1</span>) </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])</span><br><span class="line">                result = <span class="built_in">max</span>(result, dp[i][j])</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h3 id="判断子序列-392">判断子序列[392]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/is-subsequence">https://leetcode.cn/problems/is-subsequence</a><br>
dp[i][j] 表示以下标i-1为结尾的字符串s，和以下标j-1为结尾的字符串t，相同子序列的长度为dp[i][j]。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑距离的基础题衍生的</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isSubsequence</span>(<span class="params">self, s, t</span>):</span><br><span class="line">        n1 = <span class="built_in">len</span>(s)</span><br><span class="line">        n2 = <span class="built_in">len</span>(t)</span><br><span class="line">        dp = [[<span class="number">0</span>]*(n2+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n1+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n1+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n2+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> s[i-<span class="number">1</span>]==t[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i][j],dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> dp[-<span class="number">1</span>][-<span class="number">1</span>]==<span class="built_in">len</span>(s):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="编辑距离-72">编辑距离[72]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/edit-distance">https://leetcode.cn/problems/edit-distance</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minDistance</span>(<span class="params">self, word1: <span class="built_in">str</span>, word2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n1 = <span class="built_in">len</span>(word1)</span><br><span class="line">        n2 = <span class="built_in">len</span>(word2)</span><br><span class="line">        dp = [[<span class="number">0</span>]*(n2+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n1+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n1+<span class="number">1</span>):</span><br><span class="line">            dp[i][<span class="number">0</span>] = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n2+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][j] = j</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n1+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n2+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> word1[i-<span class="number">1</span>]==word2[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">min</span>(dp[i-<span class="number">1</span>][j-<span class="number">1</span>],dp[i-<span class="number">1</span>][j],dp[i][j-<span class="number">1</span>]) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> dp[n1][n2]            </span><br></pre></td></tr></table></figure>
<h3 id="不同的子序列-115">不同的子序列[115]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/distinct-subsequences">https://leetcode.cn/problems/distinct-subsequences</a><br>
困难题动态规划方程不易想到</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numDistinct</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n1 = <span class="built_in">len</span>(s)</span><br><span class="line">        n2 = <span class="built_in">len</span>(t)</span><br><span class="line">        dp = [[<span class="number">0</span>]*(n2+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n1+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n1+<span class="number">1</span>):</span><br><span class="line">            dp[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n2+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][j] = <span class="number">0</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n1+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n2+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> s[i-<span class="number">1</span>]==t[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + dp[i-<span class="number">1</span>][j]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j]</span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>][-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="两个字符串的删除操作-583">两个字符串的删除操作[583]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/delete-operation-for-two-strings">https://leetcode.cn/problems/delete-operation-for-two-strings</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minDistance</span>(<span class="params">self, word1: <span class="built_in">str</span>, word2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [[<span class="number">0</span>] * (<span class="built_in">len</span>(word2)+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word1)+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word1)+<span class="number">1</span>):</span><br><span class="line">            dp[i][<span class="number">0</span>] = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word2)+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">0</span>][j] = j</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(word1)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(word2)+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> word1[i-<span class="number">1</span>] == word2[j-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="built_in">min</span>(dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">2</span>, dp[i-<span class="number">1</span>][j] + <span class="number">1</span>, dp[i][j-<span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>][-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h1>爬楼梯类</h1>
<h2 id="斐波那契数-509">斐波那契数[509]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/fibonacci-number">https://leetcode.cn/problems/fibonacci-number</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> n==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> n==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        dp = [<span class="number">0</span>]*(n+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        dp[<span class="number">1</span>]= <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n+<span class="number">1</span>):</span><br><span class="line">            dp[i] = dp[i-<span class="number">1</span>] + dp[i-<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure>
<h2 id="爬楼梯-70">爬楼梯[70]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/climbing-stairs">https://leetcode.cn/problems/climbing-stairs</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">climbStairs</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> n==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n==<span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">        dp = [<span class="number">0</span>] * (n+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,n+<span class="number">1</span>):</span><br><span class="line">            dp[i] = dp[i-<span class="number">2</span>] + dp[i-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> dp[n-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h2 id="爬楼梯2-三步问题">爬楼梯2[三步问题]</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">waysToStep</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        run_maps = &#123;<span class="number">1</span>:<span class="number">1</span>,<span class="number">2</span>:<span class="number">2</span>,<span class="number">3</span>:<span class="number">4</span>&#125;</span><br><span class="line">        <span class="keyword">if</span> n <span class="keyword">in</span> run_maps:</span><br><span class="line">            <span class="keyword">return</span> run_maps[n]</span><br><span class="line">        dp = [<span class="number">0</span>] * (n+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        dp[<span class="number">2</span>] = <span class="number">2</span></span><br><span class="line">        dp[<span class="number">3</span>] = <span class="number">4</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>, n+<span class="number">1</span>):</span><br><span class="line">            dp[i] = (dp[i-<span class="number">1</span>] + dp[i-<span class="number">2</span>] + dp[i-<span class="number">3</span>])%<span class="number">1000000007</span></span><br><span class="line">        <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure>
<h2 id="爬楼梯3使用最小花费爬楼梯-LCR-088">爬楼梯3使用最小花费爬楼梯[LCR 088]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/min-cost-climbing-stairs">https://leetcode.cn/problems/min-cost-climbing-stairs</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minCostClimbingStairs</span>(<span class="params">self, cost: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="number">0</span>] * <span class="built_in">len</span>(cost)</span><br><span class="line">        dp[<span class="number">0</span>] = cost[<span class="number">0</span>]</span><br><span class="line">        dp[<span class="number">1</span>] = cost[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(cost)):</span><br><span class="line">            dp[i] = <span class="built_in">min</span>(dp[i-<span class="number">1</span>], dp[i-<span class="number">2</span>]) + cost[i]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">min</span>(dp[<span class="built_in">len</span>(cost)-<span class="number">1</span>],dp[<span class="built_in">len</span>(cost)-<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<h2 id="完全平方数-279">完全平方数[279]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/perfect-squares">https://leetcode.cn/problems/perfect-squares</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numSquares</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)] * (n+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span> <span class="comment"># 注意</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(i**<span class="number">0.5</span>)+<span class="number">1</span>):</span><br><span class="line">                dp[i] = <span class="built_in">min</span>(dp[i], dp[i-j*j]+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h2 id="跳跃游戏-55">跳跃游戏[55]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/jump-game">https://leetcode.cn/problems/jump-game</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canJump</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        dp = [<span class="number">0</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> dp[j] <span class="keyword">and</span> nums[j] + j &gt;= i:</span><br><span class="line">                    dp[i] = <span class="number">1</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span> <span class="keyword">if</span> dp[<span class="built_in">len</span>(nums)-<span class="number">1</span>] <span class="keyword">else</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="跳跃游戏II-45">跳跃游戏II[45]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/jump-game-ii">https://leetcode.cn/problems/jump-game-ii</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">jump</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)] * <span class="built_in">len</span>(nums)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">                <span class="keyword">if</span> nums[j] + j &gt;= i:</span><br><span class="line">                    dp[i] = <span class="built_in">min</span>(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h2 id="跳跃游戏-III-1306">跳跃游戏 III[1306]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/jump-game-iii">https://leetcode.cn/problems/jump-game-iii</a><br>
代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BFS</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canReach</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>], start: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">        used = &#123;&#125;</span><br><span class="line">        queue = deque([start])</span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            node = queue.popleft()</span><br><span class="line">            <span class="keyword">if</span> arr[node]==<span class="number">0</span>: <span class="comment"># 容易写到2出</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> node <span class="keyword">in</span> used:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            used[node] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> [node-arr[node], node+arr[node]]:</span><br><span class="line">                <span class="keyword">if</span> v &gt;= <span class="number">0</span> <span class="keyword">and</span> v&lt;<span class="built_in">len</span>(arr):</span><br><span class="line">                    <span class="keyword">if</span> v <span class="keyword">in</span> used:</span><br><span class="line">                        <span class="keyword">continue</span> <span class="comment"># 2</span></span><br><span class="line">                    queue.append(v)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="comment"># DFS</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canReach</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>], start: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        s = &#123;&#125;</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">arr, st</span>):</span><br><span class="line">            <span class="keyword">if</span> st <span class="keyword">not</span> <span class="keyword">in</span> s <span class="keyword">and</span> arr[st]==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            s[st] = <span class="number">1</span></span><br><span class="line">            current_step = arr[st]</span><br><span class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> [st+current_step, st-current_step]:</span><br><span class="line">                <span class="keyword">if</span> step&gt;=<span class="number">0</span> <span class="keyword">and</span> step&lt; <span class="built_in">len</span>(arr) <span class="keyword">and</span> step <span class="keyword">not</span> <span class="keyword">in</span> s <span class="keyword">and</span> dfs(arr, step):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> dfs(arr, start)</span><br></pre></td></tr></table></figure>
<h2 id="跳跃游戏-VI-1696">跳跃游戏 VI[1696]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/jump-game-vi">https://leetcode.cn/problems/jump-game-vi</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxResult</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [-<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)] * <span class="built_in">len</span>(nums) <span class="comment"># 容易写错</span></span><br><span class="line">        dp[<span class="number">0</span>] = nums[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(<span class="number">0</span>, i - k), i):</span><br><span class="line">                dp[i] = <span class="built_in">max</span>(dp[i], dp[j] + nums[i]) </span><br><span class="line">                <span class="comment"># 这里错写为max(dp[i], dp[j] + nums[i])</span></span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h2 id="青蛙过河-403">青蛙过河[403]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/frog-jump">https://leetcode.cn/problems/frog-jump</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 递归</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canCross</span>(<span class="params">self, stones: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(stones)):</span><br><span class="line">            <span class="keyword">if</span> stones[i] &gt; i + stones[i-<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        end = stones[-<span class="number">1</span>]</span><br><span class="line">        stones = <span class="built_in">set</span>(stones)</span><br><span class="line"></span><br><span class="line"><span class="meta">        @functools.lru_cache(<span class="params"><span class="literal">None</span></span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">location, step</span>):</span><br><span class="line">            <span class="keyword">if</span> location == end: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> [step-<span class="number">1</span>, step, step+<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">if</span> step &gt; <span class="number">0</span> <span class="keyword">and</span> location+step <span class="keyword">in</span> stones <span class="keyword">and</span> dfs(location+step, step):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> dfs(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># DP</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canCross</span>(<span class="params">self, stones: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(stones)</span><br><span class="line">        dp = [[<span class="literal">False</span>]* n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">                k = stones[i] - stones[j]</span><br><span class="line">                <span class="keyword">if</span> k&gt;j+<span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                dp[i][k] = dp[j][k] <span class="keyword">or</span> dp[j][k-<span class="number">1</span>] <span class="keyword">or</span> dp[j][k+<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> dp[n-<span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">if</span> i:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>不懂看这里 <a target="_blank" rel="noopener" href="https://www.ai2news.com/blog/2980406/">https://www.ai2news.com/blog/2980406/</a></p>
<h2 id="打家劫舍-198">打家劫舍[198]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/house-robber">https://leetcode.cn/problems/house-robber</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rob</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> nums[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums)==<span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">max</span>(nums)</span><br><span class="line">        dp = [<span class="number">0</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">        dp[<span class="number">0</span>] = nums[<span class="number">0</span>]</span><br><span class="line">        dp[<span class="number">1</span>] = nums[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i-<span class="number">1</span>):</span><br><span class="line">                dp[i] = <span class="built_in">max</span>(dp[i], dp[j]+nums[i])</span><br><span class="line">                <span class="comment"># 容易写错为dp[i] = max(dp[i], dp[j]+nums[j])</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></table></figure>
<h2 id="打家劫舍-II-213">打家劫舍 II[213]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/house-robber-ii">https://leetcode.cn/problems/house-robber-ii</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rob</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:        </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dp_f</span>(<span class="params">nums</span>):</span><br><span class="line">            dp = [<span class="number">0</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">            dp[<span class="number">0</span>] = nums[<span class="number">0</span>]</span><br><span class="line">            dp[<span class="number">1</span>] = nums[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="built_in">len</span>(nums)):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i-<span class="number">1</span>):</span><br><span class="line">                    dp[i] = <span class="built_in">max</span>(dp[i], dp[j]+nums[i])</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> nums[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums)==<span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">max</span>(nums)</span><br><span class="line">        max_dp1 = dp_f(nums[<span class="number">0</span>:<span class="built_in">len</span>(nums)-<span class="number">1</span>])</span><br><span class="line">        max_dp2 = dp_f(nums[<span class="number">1</span>:<span class="built_in">len</span>(nums)])</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(max_dp1, max_dp2)</span><br></pre></td></tr></table></figure>
<p>分开来做就好</p>
<h2 id="打家劫舍-III-337">打家劫舍 III[337]</h2>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/house-robber-iii">https://leetcode.cn/problems/house-robber-iii</a><br>
递归法，实现思路：用root点和不用root点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rob</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 用root点</span></span><br><span class="line">        money = root.val</span><br><span class="line">        <span class="keyword">if</span> root.left:</span><br><span class="line">            money = money + self.rob(root.left.left) + self.rob(root.left.right)</span><br><span class="line">        <span class="keyword">if</span> root.right:</span><br><span class="line">            money = money + self.rob(root.right.left) + self.rob(root.right.right)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(money, self.rob(root.left)+self.rob(root.right)) <span class="comment">#不用root点</span></span><br></pre></td></tr></table></figure>
<p>递归优化法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.d = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rob</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">in</span> self.d:</span><br><span class="line">            <span class="keyword">return</span> self.d[root]</span><br><span class="line">        money = root.val</span><br><span class="line">        <span class="keyword">if</span> root.left:</span><br><span class="line">            money = money + self.rob(root.left.left) + self.rob(root.left.right)</span><br><span class="line">        <span class="keyword">if</span> root.right:</span><br><span class="line">            money = money + self.rob(root.right.left) + self.rob(root.right.right)</span><br><span class="line">        result = <span class="built_in">max</span>(money, self.rob(root.left)+self.rob(root.right))</span><br><span class="line">        self.d[root] = result</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h1>背包零钱</h1>
<h2 id="背包问题介绍">背包问题介绍</h2>
<p>可以看如下几个连接，增加对某些点的理解</p>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://leetcode.cn/problems/coin-change-ii/solutions/1412584/by-flix-e1vv/">https://leetcode.cn/problems/coin-change-ii/solutions/1412584/by-flix-e1vv/</a><br>
这个说的挺好的，总结的题目都有，后面的题解也是围绕这个来的</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://labuladong.github.io/algo/di-er-zhan-a01c6/bei-bao-le-34bd4/jing-dian--28f3c">https://labuladong.github.io/algo/di-er-zhan-a01c6/bei-bao-le-34bd4/jing-dian--28f3c</a><br>
这个是labuladong的算法笔记，说的也挺好的</p>
</li>
</ul>
<p>整体上，背包问题的动态规划写成如下的形式：</p>
<p>0-1背包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j]=max(dp[i-1][j],dp[i-1][j-w_i]+v_i, 0&lt;=w_i&lt;=j)</span><br></pre></td></tr></table></figure>
<p>完全背包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j]=max(dp[i-1][j],dp[i][j-w_i]+v_i, 0&lt;=w_i&lt;=j)</span><br></pre></td></tr></table></figure>
<p>注意：在使用的时候，大部分组合问题，因此for两层循环的话，外层是N个物件或者N种币，内层是背包的容量W或者是要凑的零钱大小W。关于组合还有排序的问题可以看 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/coin-change-ii/solutions/143948/ling-qian-dui-huan-iihe-pa-lou-ti-wen-ti-dao-di-yo/">https://leetcode.cn/problems/coin-change-ii/solutions/143948/ling-qian-dui-huan-iihe-pa-lou-ti-wen-ti-dao-di-yo/</a></p>
<h2 id="逻辑分开">逻辑分开</h2>
<p>做题的逻辑哈<br>
<img src="https://note.youdao.com/yws/res/9439/WEBRESOURCE355ec888043d505907a9f4cfb7a1c279" alt="图片1.png"></p>
<h2 id="模板-3">模板</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二维</span></span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>] = x</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):<span class="comment">#注意加1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(amount+<span class="number">1</span>): <span class="comment">#注意加1</span></span><br><span class="line">        <span class="keyword">if</span> xx:</span><br><span class="line">            dp[i][j] = <span class="built_in">min</span>(dp[i-<span class="number">1</span>][j] , dp[i][j-coins[i-<span class="number">1</span>]]+<span class="number">1</span>) <span class="comment"># 注意i-1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ...</span><br><span class="line"><span class="comment"># 一维</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> coins: <span class="comment"># 1. 遍历硬币</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(amount,i-<span class="number">1</span>, -<span class="number">1</span>): <span class="comment"># 2.遍历金额，逆序，注意最小值</span></span><br><span class="line">        dp[j] = <span class="built_in">min</span>,<span class="built_in">max</span>,..., dp[j], dp[j-i] <span class="comment"># 3. 写动态函数</span></span><br></pre></td></tr></table></figure>
<h2 id="0-1背包">0-1背包</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二维</span></span><br><span class="line">N = <span class="number">3</span>  <span class="comment"># 物品数量</span></span><br><span class="line">W = <span class="number">4</span>  <span class="comment"># 背包容量</span></span><br><span class="line">Wt = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]  <span class="comment"># 所占用的容量</span></span><br><span class="line">val = [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>]  <span class="comment"># 价值</span></span><br><span class="line">dp = [[<span class="number">0</span>] * (W + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N + <span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, W + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> w - Wt[i - <span class="number">1</span>] &lt; <span class="number">0</span>:</span><br><span class="line">            dp[i][w] = dp[i - <span class="number">1</span>][w]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dp[i][w] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][w], dp[i - <span class="number">1</span>][w - Wt[i - <span class="number">1</span>]] + val[i - <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 【推荐写法】一维</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_1_wei_bag_problem</span>():</span><br><span class="line">    weight = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">    value = [<span class="number">15</span>, <span class="number">20</span>, <span class="number">30</span>]</span><br><span class="line">    bagWeight = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    dp = [<span class="number">0</span>] * (bagWeight + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weight)):  <span class="comment"># 遍历物品</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(bagWeight, weight[i] - <span class="number">1</span>, -<span class="number">1</span>):  <span class="comment"># 遍历背包容量</span></span><br><span class="line">            dp[j] = <span class="built_in">max</span>(dp[j], dp[j - weight[i]] + value[i])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(dp[bagWeight])</span><br></pre></td></tr></table></figure>
<h3 id="目标和-494">目标和[494]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/target-sum">https://leetcode.cn/problems/target-sum</a></p>
<p>分析：0-1背包，外循环硬币，内循环金额，内循环逆序，注意内循环最小值</p>
<p>dp累加dp[i] = dp[i] + dp[j-i]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二维</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findTargetSumWays</span>(<span class="params">self, nums, target</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        total = <span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(target) &gt; total:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> (total+target)%<span class="number">2</span>==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        pos = (total + target) // <span class="number">2</span></span><br><span class="line">        neg = (total - target) // <span class="number">2</span></span><br><span class="line">        C = <span class="built_in">min</span>(pos,  neg) <span class="comment"># 金额</span></span><br><span class="line">        n = <span class="built_in">len</span>(nums) <span class="comment"># 硬币数量</span></span><br><span class="line"></span><br><span class="line">        dp = [[<span class="number">0</span>]*(C+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n+<span class="number">1</span>)]</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(C+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> j &gt;= nums[i-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j] + dp[i-<span class="number">1</span>][j-nums[i-<span class="number">1</span>]] <span class="comment"># 0-1背包</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j]</span><br><span class="line">        <span class="keyword">return</span> dp[n][C]</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 【推荐写法】一维</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findTargetSumWays</span>(<span class="params">self, nums, target</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        total = <span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(target) &gt; total:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> (total+target)%<span class="number">2</span>==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        pos = (total + target) // <span class="number">2</span></span><br><span class="line">        neg = (total - target) // <span class="number">2</span></span><br><span class="line">        C = <span class="built_in">min</span>(pos,  neg) <span class="comment"># 金额</span></span><br><span class="line">        n = <span class="built_in">len</span>(nums) <span class="comment"># 硬币数量</span></span><br><span class="line"></span><br><span class="line">        dp = [<span class="number">0</span>] * (C+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums: <span class="comment"># 1. 外循环：遍历硬币 </span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(C, i-<span class="number">1</span>, -<span class="number">1</span>): <span class="comment"># 2. 内循环 逆序：遍历目标金额，最小为i-1一定</span></span><br><span class="line">                dp[j] = dp[j] + dp[j-i]</span><br><span class="line">        <span class="keyword">return</span> dp[C]</span><br></pre></td></tr></table></figure>
<h3 id="分割等和子集-416">分割等和子集[416]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/partition-equal-subset-sum">https://leetcode.cn/problems/partition-equal-subset-sum</a></p>
<p>分析：0-1背包，外循环硬币，内循环金额，内循环逆序，注意内循环最小值</p>
<p>dp累求|即可，为dp[i] = dp[i] | dp[j-i]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一维动态规划</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canPartition</span>(<span class="params">self, nums</span>):</span><br><span class="line">        <span class="comment"># 边界</span></span><br><span class="line">        total = <span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="keyword">if</span> total % <span class="number">2</span>==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        target = total // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">max</span>(nums) &gt; target:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="comment"># dp</span></span><br><span class="line">        dp = [<span class="literal">False</span>] * (target+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums: <span class="comment"># 1.遍历物品即硬币</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(target, num-<span class="number">1</span>, -<span class="number">1</span>): <span class="comment"># 2. 遍历目标（金额），逆序，最小为num-1</span></span><br><span class="line">                dp[j] |= dp[j-num] <span class="comment"># 3. 动规表达式</span></span><br><span class="line">        <span class="keyword">return</span> dp[target]</span><br></pre></td></tr></table></figure>
<h3 id="最后一块石头的重量-II-1049">最后一块石头的重量 II[1049]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/last-stone-weight-ii">https://leetcode.cn/problems/last-stone-weight-ii</a></p>
<p>分析：0-1背包，外循环硬币，内循环金额，内循环逆序，注意内循环最小值</p>
<p>dp为 dp[j] = max(dp[j], dp[j - stone] + stone)</p>
<p>分析：本题其实就是尽量让石头分成重量相同的两堆，相撞之后剩下的石头最小，这样就化解成01背包问题了。分析到物品的重量为stones[i]，物品的价值也为stones[i]。</p>
<p>dp数组含义，表示前i个stone得到不超过taget的最大重量，这里是最大，不是刚刚好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一维动态规划</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lastStoneWeightII</span>(<span class="params">self, stones: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="number">0</span>] * <span class="number">15001</span></span><br><span class="line">        total_sum = <span class="built_in">sum</span>(stones)</span><br><span class="line">        target = total_sum // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> stone <span class="keyword">in</span> stones:  <span class="comment"># 1. 遍历物品</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(target, stone - <span class="number">1</span>, -<span class="number">1</span>):  <span class="comment"># 2. 遍历背包，逆序，最小为stone-1</span></span><br><span class="line">                dp[j] = <span class="built_in">max</span>(dp[j], dp[j - stone] + stone)</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> total_sum - dp[target] - dp[target]</span><br></pre></td></tr></table></figure>
<h3 id="一和零-474">一和零[474]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/ones-and-zeroes">https://leetcode.cn/problems/ones-and-zeroes</a><br>
分析：0-1背包，外循环硬币，内循环金额，内循环逆序，注意内循环最小值</p>
<p>dp表达式为 dp[m][n] = max(dp[m][n], dp[m-neg][n-pos]+1)</p>
<p>分析：从一个list的字符串中选择一些数，使得1的数不超过m,0的数不超过n.<br>
dp[i][j]表示不超过i和j的最大集合的长度，和上面类似，这是这里二维。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMaxForm</span>(<span class="params">self, strs, m, n</span>):</span><br><span class="line">        dp = [[<span class="number">0</span>]*(n+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> strs: <span class="comment"># 1. 遍历硬币</span></span><br><span class="line">            pos = s.count(<span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">            neg = <span class="built_in">len</span>(s) - pos</span><br><span class="line">            <span class="keyword">for</span> m1 <span class="keyword">in</span> <span class="built_in">range</span>(m,neg-<span class="number">1</span>,-<span class="number">1</span>): <span class="comment"># 2. 遍历金额，逆序，最小值</span></span><br><span class="line">                <span class="keyword">for</span> n1 <span class="keyword">in</span> <span class="built_in">range</span>(n,pos-<span class="number">1</span>,-<span class="number">1</span>):<span class="comment"># 3. 遍历金额，逆序，最小值</span></span><br><span class="line">                    dp[m1][n1] = <span class="built_in">max</span>(dp[m1][n1], dp[m1-neg][n1-pos]+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></table></figure>
<h2 id="完全背包">完全背包</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">3</span>  <span class="comment"># 物品数量</span></span><br><span class="line">W = <span class="number">4</span>  <span class="comment"># 背包容量</span></span><br><span class="line">Wt = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]  <span class="comment"># 所占用的容量</span></span><br><span class="line">val = [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>]  <span class="comment"># 价值</span></span><br><span class="line">dp = [[<span class="number">0</span>] * (W + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N + <span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, W + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> w - Wt[i - <span class="number">1</span>] &lt; <span class="number">0</span>:</span><br><span class="line">            dp[i][w] = dp[i - <span class="number">1</span>][w]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dp[i][w] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][w], dp[i][w - Wt[i - <span class="number">1</span>]] + val[i - <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h3 id="零钱兑换-322">零钱兑换[322]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/coin-change">https://leetcode.cn/problems/coin-change</a></p>
<p>分析：完全背包，外循环硬币，内循环金额，内循环顺序</p>
<p>dp公式dp[j] = min(dp[j],dp[j-i]+1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二维解法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">coinChange</span>(<span class="params">self, coins, amount</span>):</span><br><span class="line">        <span class="comment"># 1. 初始化</span></span><br><span class="line">        N = <span class="built_in">len</span>(coins)</span><br><span class="line">        dp = [[<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)]*(amount+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N+<span class="number">1</span>)]</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 2. 循环</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,N+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(amount+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> j &gt;= coins[i-<span class="number">1</span>]:</span><br><span class="line">                    dp[i][j] = <span class="built_in">min</span>(dp[i-<span class="number">1</span>][j] , dp[i][j-coins[i-<span class="number">1</span>]]+<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j]</span><br><span class="line">        ans = dp[N][amount]</span><br><span class="line">        <span class="keyword">return</span> ans <span class="keyword">if</span> ans!=<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line"><span class="comment"># 一维解法I</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">coinChange</span>(<span class="params">self, coins, amount</span>):</span><br><span class="line">        <span class="comment"># 1. 初始化</span></span><br><span class="line">        N = <span class="built_in">len</span>(coins)</span><br><span class="line">        dp = [<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)]*(amount+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 2. 循环</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(amount+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> j &gt;= coins[i]:</span><br><span class="line">                    dp[j] = <span class="built_in">min</span>(dp[j], dp[j-coins[i]]+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> dp[amount] <span class="keyword">if</span> dp[amount]!=<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line"><span class="comment"># 一维解法II</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">coinChange</span>(<span class="params">self, coins, amount</span>):</span><br><span class="line">        <span class="keyword">if</span> amount==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        dp = [<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] * (amount+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> coins:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i, amount+<span class="number">1</span>):</span><br><span class="line">                dp[j] = <span class="built_in">min</span>(dp[j],dp[j-i]+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[amount] <span class="keyword">if</span> dp[amount]!=<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">else</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>注意一维和二维中，在二维初始化的时候长度是amount+1, 一维也是，但是在N这个地方，二维是N+1，一维是N。</p>
<p>解法 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/coin-change/solutions/1412324/by-flix-su7s/">https://leetcode.cn/problems/coin-change/solutions/1412324/by-flix-su7s/</a></p>
<h3 id="零钱兑换II-518">零钱兑换II[518]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/coin-change-ii">https://leetcode.cn/problems/coin-change-ii</a></p>
<p>分析：完全背包，外循环硬币，内循环金额，内循环顺序</p>
<p>dp公式dp[j] = dp[j] + dp[j-i]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二维</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">change</span>(<span class="params">self, amount, coins</span>):</span><br><span class="line">        <span class="comment"># 1. 初始化</span></span><br><span class="line">        N = <span class="built_in">len</span>(coins)</span><br><span class="line">        dp = [[<span class="number">0</span>]*(amount+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N+<span class="number">1</span>)] <span class="comment"># 方便后面取数</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># 2. 循环</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(amount+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> coins[i-<span class="number">1</span>] &gt; j:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j] + dp[i][j-coins[i-<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">return</span> dp[N][amount]</span><br></pre></td></tr></table></figure>
<p>当然，官方简单的方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 压缩为1维</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">change</span>(<span class="params">self, amount, coins</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> amount:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>      </span><br><span class="line">        <span class="comment"># 1. 初始化</span></span><br><span class="line">        dp = [<span class="number">0</span>] * (amount+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># 2. 循环</span></span><br><span class="line">        N = <span class="built_in">len</span>(coins)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N): </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(amount+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> j &gt;= coins[i]:                    </span><br><span class="line">                    dp[j] = dp[j] + dp[j-coins[i]]</span><br><span class="line">        <span class="keyword">return</span> dp[amount]</span><br><span class="line"><span class="comment"># 2.再简化</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">change</span>(<span class="params">self, amount: <span class="built_in">int</span>, coins: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> amount:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>      </span><br><span class="line">        dp = [<span class="number">0</span>] * (amount+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> coins: </span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i, amount+<span class="number">1</span>):</span><br><span class="line">                dp[j] = dp[j] + dp[j-i]</span><br><span class="line">        <span class="keyword">return</span> dp[amount]</span><br></pre></td></tr></table></figure>
<h3 id="硬币问题-面试题-08-11">硬币问题[面试题 08.11]</h3>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/coin-lcci">https://leetcode-cn.com/problems/coin-lcci</a> 题解如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">waysToChange</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        mod = <span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span></span><br><span class="line">        coins = [<span class="number">25</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        f = [<span class="number">1</span>] + [<span class="number">0</span>] * n</span><br><span class="line">        <span class="keyword">for</span> coin <span class="keyword">in</span> coins:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(coin, n + <span class="number">1</span>):</span><br><span class="line">                f[i] += f[i - coin]</span><br><span class="line">        <span class="keyword">return</span> f[n] % mod</span><br></pre></td></tr></table></figure>
<h3 id="分割数组的最大值-410">分割数组的最大值[410]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/split-array-largest-sum">https://leetcode.cn/problems/split-array-largest-sum</a><br>
分析：完全背包，外循环硬币，内循环金额，内循环顺序</p>
<p>dp公式f[i][j] = min(f[i][j], max(f[k][j - 1], sub[i] - sub[k]))</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">splitArray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], m: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        f = [[<span class="number">10</span>**<span class="number">18</span>] * (m + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">        sub = [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> elem <span class="keyword">in</span> nums:</span><br><span class="line">            sub.append(sub[-<span class="number">1</span>] + elem)</span><br><span class="line">        </span><br><span class="line">        f[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">min</span>(i, m) + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">                    f[i][j] = <span class="built_in">min</span>(f[i][j], <span class="built_in">max</span>(f[k][j - <span class="number">1</span>], sub[i] - sub[k]))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> f[n][m]</span><br></pre></td></tr></table></figure>
<h3 id="组数总和Ⅳ">组数总和Ⅳ</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/combination-sum-iv">https://leetcode.cn/problems/combination-sum-iv</a><br>
分析：完全背包，外循环金额【排列问题】，内循环硬币，内循环顺序</p>
<p>dp公式dp[i] = dp[i] + dp[i-j]</p>
<p>题目见 <a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/combination-sum-iv/">https://leetcode-cn.com/problems/combination-sum-iv/</a>  这道题和爬楼梯的问题有点类似，但是解法不太一样。动态规划的方程是<br>
dp[i] = sum(dp[i-j]) j&lt;=i.解法如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combinationSum4</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="number">0</span>] * (target+<span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,target+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> nums:</span><br><span class="line">                <span class="keyword">if</span> i &gt;= j:</span><br><span class="line">                    dp[i] = dp[i] + dp[i-j]</span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="爬楼梯-70-2">爬楼梯[70]</h3>
<p>位于 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/climbing-stairs">https://leetcode.cn/problems/climbing-stairs</a><br>
分析：完全背包，外循环金额【排列问题】，内循环硬币，内循环顺序</p>
<p>dp公式dp[i] = dp[i] + dp[i-j]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">target = <span class="number">10</span> <span class="comment"># 爬到阶梯数</span></span><br><span class="line">w = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>] <span class="comment"># 每次可以爬多少</span></span><br><span class="line">dp = [<span class="number">0</span>] * (target + <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(target + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w)):</span><br><span class="line">        <span class="keyword">if</span> i - j &gt;= <span class="number">0</span>:</span><br><span class="line">            dp[i] = dp[i] + dp[i - j]</span><br><span class="line"><span class="built_in">print</span>(dp[<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<h3 id="剪绳子">剪绳子</h3>
<p>题目见</p>
<h1>股票买卖问题</h1>
<h2 id="模板-4">模板</h2>
<p>这里主要说一下DP的思路，按照labuladong的做法，基本上上可以在一个模板上丝毫不动，就可以得到结果</p>
<p>统一的动态规划方程如下</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 初始化</span><br><span class="line">dp = [[<span class="number">0</span>]*<span class="number">2</span> <span class="keyword">for</span> _ in <span class="keyword">range</span>(<span class="built_in">len</span>(prics))]</span><br><span class="line"><span class="keyword">for</span> i in <span class="keyword">range</span>(<span class="built_in">len</span>(prices)):</span><br><span class="line">    dp[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    dp[i][<span class="number">1</span>] = -float(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> i==<span class="number">0</span>:</span><br><span class="line">    dp[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    dp[i][<span class="number">1</span>] = -prics[<span class="number">0</span>] - 手续费</span><br><span class="line"># <span class="keyword">for</span>一下</span><br><span class="line">dp[i][k][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][k][<span class="number">0</span>], dp[i<span class="number">-1</span>][k][<span class="number">1</span>] + prices[i])</span><br><span class="line">dp[i][k][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][k][<span class="number">1</span>], dp[i<span class="number">-1</span>][k<span class="number">-1</span>][<span class="number">0</span>] - prices[i] - 手续费) </span><br></pre></td></tr></table></figure>
<p>i表示第几天，k表示最多交易次数，0表示手里没股票了，1表示手里还有股票。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://labuladong.github.io/algo/di-er-zhan-a01c6/yong-dong--63ceb/yi-ge-fang-3b01b/">https://labuladong.github.io/algo/di-er-zhan-a01c6/yong-dong--63ceb/yi-ge-fang-3b01b/</a></p>
</blockquote>
<h2 id="买卖股票的最佳时机-121">买卖股票的最佳时机[121]</h2>
<p>题目：<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock/description/">https://leetcode.cn/problems/best-time-to-buy-and-sell-stock/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模板解法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices</span>):</span><br><span class="line">        <span class="comment"># dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) </span></span><br><span class="line">        <span class="comment"># k=1简化为</span></span><br><span class="line">        <span class="comment"># dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][1] = max(dp[i-1][1], -prices[i])</span></span><br><span class="line">        dp = [[<span class="number">0</span>]*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices))]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices)):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                dp[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">                dp[i][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            dp[i][<span class="number">0</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">0</span>], dp[i-<span class="number">1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">            dp[i][<span class="number">1</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">1</span>], -prices[i])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="built_in">len</span>(prices)-<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 更简单做法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices</span>):</span><br><span class="line">        mins = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">        profit = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> prices:</span><br><span class="line">            <span class="keyword">if</span> i &lt; mins:</span><br><span class="line">                mins = i</span><br><span class="line">            profit = <span class="built_in">max</span>(profit, i - mins)</span><br><span class="line">        <span class="keyword">return</span> profit</span><br></pre></td></tr></table></figure>
<h2 id="买卖股票的最佳时机II-122">买卖股票的最佳时机II[122]</h2>
<p>题目：<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-ii/description/">https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-ii/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模板做法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices</span>):</span><br><span class="line">        <span class="comment"># dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) </span></span><br><span class="line">        <span class="comment"># k=1简化为</span></span><br><span class="line">        <span class="comment"># dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][1] = max(dp[i-1][1], dp[i-1][0]-prices[i]) #改动点1</span></span><br><span class="line">        dp = [[<span class="number">0</span>]*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices))]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices)):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                dp[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">                dp[i][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            dp[i][<span class="number">0</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">0</span>], dp[i-<span class="number">1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">            dp[i][<span class="number">1</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">1</span>], dp[i-<span class="number">1</span>][<span class="number">0</span>]-prices[i])<span class="comment">#改动点2</span></span><br><span class="line">        <span class="keyword">return</span> dp[<span class="built_in">len</span>(prices)-<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 更简单做法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices</span>):</span><br><span class="line">        profit = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(prices)):</span><br><span class="line">            <span class="keyword">if</span> prices[i] - prices[i-<span class="number">1</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                profit += prices[i] - prices[i-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> profit</span><br></pre></td></tr></table></figure>
<h2 id="买卖股票的最佳时机含冷冻期-309">买卖股票的最佳时机含冷冻期[309]</h2>
<p>题目：<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-with-cooldown/">https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-with-cooldown/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模板做法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices</span>):</span><br><span class="line">        <span class="comment"># dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) </span></span><br><span class="line">        <span class="comment"># k=1简化为</span></span><br><span class="line">        <span class="comment"># dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][1] = max(dp[i-1][1], dp[i-1][0]-prices[i])</span></span><br><span class="line">        dp = [[<span class="number">0</span>]*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices))]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices)):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                dp[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">                dp[i][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            dp[i][<span class="number">0</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">0</span>], dp[i-<span class="number">1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">            dp[i][<span class="number">1</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">1</span>], dp[i-<span class="number">2</span>][<span class="number">0</span>]-prices[i]) <span class="comment"># 改动点</span></span><br><span class="line">        <span class="keyword">return</span> dp[<span class="built_in">len</span>(prices)-<span class="number">1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h2 id="买卖股票的最佳时机含手续费-714">买卖股票的最佳时机含手续费[714]</h2>
<p>题目：<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/description/">https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices, fee</span>):</span><br><span class="line">        <span class="comment"># dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) </span></span><br><span class="line">        <span class="comment"># k=1简化为</span></span><br><span class="line">        <span class="comment"># dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][1] = max(dp[i-1][1], dp[i-1][0]-prices[i])</span></span><br><span class="line">        dp = [[<span class="number">0</span>]*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices))]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices)):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                dp[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">                dp[i][<span class="number">1</span>] = -prices[<span class="number">0</span>] - fee <span class="comment"># 改动点</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            dp[i][<span class="number">0</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">0</span>], dp[i-<span class="number">1</span>][<span class="number">1</span>] + prices[i])</span><br><span class="line">            dp[i][<span class="number">1</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][<span class="number">1</span>], dp[i-<span class="number">1</span>][<span class="number">0</span>]-prices[i]-fee) <span class="comment"># 改动点</span></span><br><span class="line">        <span class="keyword">return</span> dp[<span class="built_in">len</span>(prices)-<span class="number">1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h2 id="买卖股票的最佳时机-III-123">买卖股票的最佳时机 III[123]</h2>
<p>题目：<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-iii/description/">https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-iii/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type prices: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k-1][0] - prices[i]) </span></span><br><span class="line">        <span class="comment"># k=1简化为</span></span><br><span class="line">        <span class="comment"># dp[i][0] = max(dp[i-1][0], dp[i-1][1] + prices[i])</span></span><br><span class="line">        <span class="comment"># dp[i][1] = max(dp[i-1][1], dp[i-1][0]-prices[i])</span></span><br><span class="line">        max_k = <span class="number">2</span></span><br><span class="line">        dp = [[[<span class="number">0</span>] * <span class="number">2</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_k + <span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices))]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prices)):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_k+<span class="number">1</span>): <span class="comment"># 改动点</span></span><br><span class="line">                <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                    dp[i][k][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">                    dp[i][k][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                dp[i][k][<span class="number">0</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][k][<span class="number">0</span>], dp[i-<span class="number">1</span>][k][<span class="number">1</span>] + prices[i])</span><br><span class="line">                dp[i][k][<span class="number">1</span>] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][k][<span class="number">1</span>], dp[i-<span class="number">1</span>][k-<span class="number">1</span>][<span class="number">0</span>]-prices[i])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="built_in">len</span>(prices)-<span class="number">1</span>][max_k][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h1>博弈问题</h1>
<p><a target="_blank" rel="noopener" href="https://labuladong.online/algo/dynamic-programming/game-theory/">https://labuladong.online/algo/dynamic-programming/game-theory/</a></p>
<h2 id="预测赢家-486">预测赢家[486]</h2>
<p>题目：<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/predict-the-winner/description/">https://leetcode.cn/problems/predict-the-winner/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predictTheWinner</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        dp = [[<span class="number">0</span>] * length <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(length)]</span><br><span class="line">        <span class="keyword">for</span> i, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            dp[i][i] = num</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,length):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length-l):</span><br><span class="line">                j = i + l</span><br><span class="line">                dp[i][j] = <span class="built_in">max</span>(nums[i] - dp[i + <span class="number">1</span>][j], nums[j] - dp[i][j - <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][length - <span class="number">1</span>] &gt;= <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h2 id="石子游戏-877">石子游戏[877]</h2>
<p>题目：<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/stone-game/description/">https://leetcode.cn/problems/stone-game/description/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">stoneGame</span>(<span class="params">self, piles: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        length = <span class="built_in">len</span>(piles)</span><br><span class="line">        dp = [[<span class="number">0</span>] * <span class="built_in">len</span>(piles) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(piles))]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(piles)):</span><br><span class="line">            dp[i][i] = piles[i]</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,length):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length-l):</span><br><span class="line">                j = i + l        </span><br><span class="line">                dp[i][j] = <span class="built_in">max</span>(piles[i] - dp[i + <span class="number">1</span>][j], piles[j] - dp[i][j - <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][length - <span class="number">1</span>] &gt; <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>和上面一样，严格意义上，不算双序列问题，所以这里不用dp=[[0]*(n+1) for i in range(n+1)]</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Tom</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tom</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
